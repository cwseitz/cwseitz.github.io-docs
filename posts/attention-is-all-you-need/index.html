<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="">
<meta name="description" content="Sequence to sequence learning One method developed by Sutskever et al in 2015 is a sequence to sequence (seq2seq) method for machine translation. As will all seq2seq translation tasks, we want to estimate distribution over the output sequence given the input sequence
\begin{eqnarray} P_{\Phi}(w_{1},..,w_{T_{in}}|w_{1},..,w_{T_{in}}) \end{eqnarray}
We usually estimate this probability distribution by using autoregression which is realized with a RNN
\begin{eqnarray} P_{\Phi}(w_{1},..,w_{T_{in}}|w_{1},..,w_{T_{in}}) = \Pi_{t=0}^{T} P_{\Phi}(w_{t}|\overset\leftarrow{h_{in}}[1,J],w_{0}&amp;hellip;w_{t-1}) \end{eqnarray}
Notice there is an unfamiliar term $\overset\leftarrow{h_{in}}[1,J]$ we are conditioning on." />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="/posts/attention-is-all-you-need/" />


    <title>
        
            Attention is all you need ::   — ~
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">



<link rel="stylesheet" href="/main.6b6843a15ca84918c558ab238eac679104e8cd7ec175d84678c0b83c87a4f007.css">






<meta itemprop="name" content="Attention is all you need">
<meta itemprop="description" content="Sequence to sequence learning One method developed by Sutskever et al in 2015 is a sequence to sequence (seq2seq) method for machine translation. As will all seq2seq translation tasks, we want to estimate distribution over the output sequence given the input sequence
\begin{eqnarray} P_{\Phi}(w_{1},..,w_{T_{in}}|w_{1},..,w_{T_{in}}) \end{eqnarray}
We usually estimate this probability distribution by using autoregression which is realized with a RNN
\begin{eqnarray} P_{\Phi}(w_{1},..,w_{T_{in}}|w_{1},..,w_{T_{in}}) = \Pi_{t=0}^{T} P_{\Phi}(w_{t}|\overset\leftarrow{h_{in}}[1,J],w_{0}&hellip;w_{t-1}) \end{eqnarray}
Notice there is an unfamiliar term $\overset\leftarrow{h_{in}}[1,J]$ we are conditioning on.">
<meta itemprop="datePublished" content="2020-11-17T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2020-11-17T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="983">
<meta itemprop="image" content=""/>



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content=""/>

<meta name="twitter:title" content="Attention is all you need"/>
<meta name="twitter:description" content="Sequence to sequence learning One method developed by Sutskever et al in 2015 is a sequence to sequence (seq2seq) method for machine translation. As will all seq2seq translation tasks, we want to estimate distribution over the output sequence given the input sequence
\begin{eqnarray} P_{\Phi}(w_{1},..,w_{T_{in}}|w_{1},..,w_{T_{in}}) \end{eqnarray}
We usually estimate this probability distribution by using autoregression which is realized with a RNN
\begin{eqnarray} P_{\Phi}(w_{1},..,w_{T_{in}}|w_{1},..,w_{T_{in}}) = \Pi_{t=0}^{T} P_{\Phi}(w_{t}|\overset\leftarrow{h_{in}}[1,J],w_{0}&hellip;w_{t-1}) \end{eqnarray}
Notice there is an unfamiliar term $\overset\leftarrow{h_{in}}[1,J]$ we are conditioning on."/>



    <meta property="og:title" content="Attention is all you need" />
<meta property="og:description" content="Sequence to sequence learning One method developed by Sutskever et al in 2015 is a sequence to sequence (seq2seq) method for machine translation. As will all seq2seq translation tasks, we want to estimate distribution over the output sequence given the input sequence
\begin{eqnarray} P_{\Phi}(w_{1},..,w_{T_{in}}|w_{1},..,w_{T_{in}}) \end{eqnarray}
We usually estimate this probability distribution by using autoregression which is realized with a RNN
\begin{eqnarray} P_{\Phi}(w_{1},..,w_{T_{in}}|w_{1},..,w_{T_{in}}) = \Pi_{t=0}^{T} P_{\Phi}(w_{t}|\overset\leftarrow{h_{in}}[1,J],w_{0}&hellip;w_{t-1}) \end{eqnarray}
Notice there is an unfamiliar term $\overset\leftarrow{h_{in}}[1,J]$ we are conditioning on." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/attention-is-all-you-need/" />
<meta property="og:image" content=""/>
<meta property="article:published_time" content="2020-11-17T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-11-17T00:00:00+00:00" />




    <meta property="article:section" content="deep-learning" />

    <meta property="article:section" content="neuroscience" />



    <meta property="article:published_time" content="2020-11-17 00:00:00 &#43;0000 UTC" />








    </head>

    <body class="dark-theme">
        <div class="container">
            <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<header class="header">
    <span class="header__inner">
        <a href="/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text">cd ~</span>
            <span class="logo__cursor" style=
                  "
                   background-color:#008080;
                   animation-duration:0.75s;">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="/posts">Drafts</a></li><li><a href="/mirage">Mirage</a></li><li><a href="/photos">Photography</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            

            <span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>
</span>
        </span>
    </span>
</header>


            <div class="content">
                
  <main class="post">

    <div class="post-info">
      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock">
          <circle cx="12" cy="12" r="10"></circle>
          <polyline points="12 6 12 12 16 14"></polyline>
        </svg>
        5 minutes

        
      </p>
    </div>

    <article>
      <h1 class="post-title">
        <a href="/posts/attention-is-all-you-need/">Attention is all you need</a>
      </h1>

      

      <div class="post-content">
        <h3 id="sequence-to-sequence-learning">Sequence to sequence learning</h3>
<p>One method developed by Sutskever et al in 2015 is a sequence to sequence (seq2seq) method for machine translation. As will all seq2seq translation tasks, we want to estimate distribution over the output sequence given the input sequence</p>
<p>\begin{eqnarray}
P_{\Phi}(w_{1},..,w_{T_{in}}|w_{1},..,w_{T_{in}})
\end{eqnarray}</p>
<p>We usually estimate this probability distribution by using autoregression which is realized with a RNN</p>
<p>\begin{eqnarray}
P_{\Phi}(w_{1},..,w_{T_{in}}|w_{1},..,w_{T_{in}}) = \Pi_{t=0}^{T} P_{\Phi}(w_{t}|\overset\leftarrow{h_{in}}[1,J],w_{0}&hellip;w_{t-1})
\end{eqnarray}</p>
<p>Notice there is an unfamiliar term $\overset\leftarrow{h_{in}}[1,J]$ we are conditioning on. This brings us to the **encoder**.</p>
<h4 id="the-encoder">The encoder</h4>
<p>Before we can compute the distribution above via autoregression, we encode the sentence into a <strong>thought-vector</strong>. The thought-vector is a vector of numbers meant to represent the meaning of a sequence of words. We assume that it is possible to encode the essence of a sentence - something that is invariant from language to language.</p>
<p>For this architecture, the thought vector representation is computed right-to-left or the last word comes first in the thought vector. As you can see in the equation above we typically represent such a thought vector as $\overset\leftarrow{h_{in}}[1,J]$ and it is the final hidden state in our encoding network.</p>
<h4 id="computing-probabilties">Computing probabilties</h4>
<p>Now that we have established what the thought-vector is and its role, we can move on to actually doing the autoregression. As mentioned above, the autoregression is implemented as an RNN from which we read out probabilities $ P_{\Phi}(w_{t}|\overset\leftarrow{h_{in}}[1,J],w_{0}&hellip;w_{t-1})$ according to</p>
<p>\begin{equation*}
\DeclareMathOperator*{\softmax}{softmax}
P(w_{t_{out}}|\overset\leftarrow{h_{in}}[1,J], w_{1},..,w_{t_{out}-1}) = \underset{w_{t_{out}}}{\softmax} e[w_{t_{out}},J]h_{out}[t_{out}-1,J]
\end{equation*}</p>
<p>where $e$ is the encoding for a particular word and $h$ is the hidden state at that point in the network.</p>
<h4 id="the-decoder">The decoder</h4>
<p>For each element of the sequence, we now have a probability distribution over the possible output words. The output sequence we get depends on how we utilize these probabilities for decoding. One way is called the <strong>greedy-decoder</strong> which is an autoregression conditioned on the thought-vector that selects each word $w_{t}$ in the translation that has the maximum probability.</p>
<p>\begin{eqnarray}
\DeclareMathOperator*{\argmax}{argmax}
w_{t} = \underset{w_{t}}{\argmax}P(w_{t}|\overset\leftarrow{h_{in}}[1,J], w_{1},..,w_{t-1})
\end{eqnarray}</p>
<p>This is distinct from a decoder that would find the sentence with a <em>globally</em> maximum probability.</p>
<h3 id="learning-to-align-and-translate">Learning to align and translate</h3>
<p>In another method, the model above is modified such that the output sequences are <em>aligned</em> with input sequences by a form of <strong>attention</strong>. In the context of alignment, the attention computes the strength of the connection between a word in the output $w_{t_{out}}$ and a word in the input $w_{t_{in}}$. This is especially useful because order is not always preserved from language to language.</p>
<h4 id="the-encoder-1">The encoder</h4>
<p>The model bares similarity in its structure to the one presented above except that the encoder is changed to a bi-directional RNN. In a bi-directional RNN, we run the network left-to-right and right-to-left and concatenate the hidden vectors produced at every $t$ to produce $\overset\leftrightarrow{h_{in}}[t_{in},J]$.</p>
<p>Then, we begin the autoregression by concatenating $\overset\rightarrow{h_{in}}[T_{in},J/2]$ and $\overset\leftarrow{h_{in}}[0,J/2]$ to produce $\overset\rightarrow{h_{in}}[t_{in},J]$ which is fed into the left-to-right RNN similar to the previous model</p>
<h4 id="the-decoder-1">The decoder</h4>
<p>The decoder is where the main differences are because we need a way to implement an attention mechanism that facilitates alignment of the input and output sequence. But first, I will write the general procedure executed by the decoding RNN</p>
<p>\begin{eqnarray}
\DeclareMathOperator*{\RNN}{RNN}
\overset\rightarrow{h_{out}}[t_{out},J] = \RNN(\overset\rightarrow{h_{out}}[t_{out},J], e[w_{t_{out}},I], \color{red}{\hat{h}_{in}[t_{out},J]})
\end{eqnarray}</p>
<p>where $\hat{h}<em>{in}[t</em>{out},J]$ is added to provide information for the alignment. To get that information, we construct the following $\alpha$</p>
<p>\begin{eqnarray}
\DeclareMathOperator*{\softmax}{softmax}
\alpha[t_{in},t_{out}] = \underset{w_{t_{in}}}{\softmax} e[w_{t_{out}},J]\overset\leftrightarrow{h_{in}}[t_{in},J]
\end{eqnarray}</p>
<p>In words, we compute the softmax of the inner product of the embedding of the word we most recently produced $w_{t_{out}}$ and the encoding at all input positions. In effect, this gives us a probability distribution over input positions for each word that we produce at $t_{out}$. Basically that tells us where to look in the input when generating $w_{t_{out}}$.</p>
<p>We then multiply each slice $t_{out}$ of the association matrix with the entire matrix $\overset\leftrightarrow{h_{in}}[T_{in},J]$ from the encoder step to give us a new matrix $\hat{h}_{in}[T_{out},J]$. This is done for every $t_{out}$ giving us the relevant parts of the encoder to produce  $w_{t_{out}}$.</p>
<p>\begin{eqnarray}
\color{red}{\hat{h}<em>{in}[t</em>{out},J]} = \alpha[t_{out},T_{in}]\overset\leftrightarrow{h_{in}}[T_{in},J]
\end{eqnarray}</p>
<h3 id="attention-is-all-you-need">Attention is all you need</h3>
<p>In the paper <em>Attention is all you need</em> published by researchers at Google Brain in 2017, a new architecture called the <strong>transformer</strong> was developed based on this idea of attention. This architecture outperforms LSTMs, recurrent networks, and gated recurrent networks. The primary pitfall of these methods is that they rely on sequential computation lacking any form of a parallelization. On the other hand, the transformer completely scraps the recurrence, relying on attention alone, and allowing for parallelization and improved computation time.</p>
<h3 id="transformer-heads--self-attention">Transformer heads &amp; self-attention</h3>
<p>For each position $t$ or position in the sentence, we construct an <strong>attention</strong> between that position and the other positions in the sentence. You can think of this as a weighted graph between the positions in the sentence. We store this information in the tensor $\alpha[k,t_{1},t_{2}]$ - which contains the weight of attention from words $t_{1}$ to $t_{2}$ with **head label** $k$. The head is just a label that describes their relationship.</p>
<h3 id="the-encoder-2">The encoder</h3>
<p>Each layer in the transformer has the same shape $L[T,J]$ which contains a time index which could be a particular word in a sentence and for each word we have the vector $L[t,J]$. It is pretty efficient on a modern machine because the transfomer can compute layer $L_{l+1}[T,J]$ from $L_{l}[T,J]$ in $O(\ln(TI))$ time by using a tree of additions.</p>
<h3 id="the-decoder-2">The decoder</h3>
<p>Each layer in the transformer has the same shape $L[T,J]$ which contains a time index which could be a particular word in a sentence and for each word we have the vector $L[t,J]$. It is pretty efficient on a modern machine because the transfomer can compute layer $L_{l+1}[T,J]$ from $L_{l}[T,J]$ in $O(\ln(TI))$ time by using a tree of additions.</p>
<h3 id="query-key-attention">Query-key attention</h3>
<p>With query-key attention, for each of our words $t$ and each of its heads $k$, we compute</p>
<p>\begin{eqnarray}
\DeclareMathOperator*{\Query}{Query}
\DeclareMathOperator*{\Key}{Key}
\Query_{l+1}[k,t,i] &amp;=&amp; W_{\mathcal{l}+1}^{Q}[k,i,J]L_{l}[t,J] \<br>
\Key_{l+1}[k,t,i] &amp;=&amp; W_{l+1}^{K}[k,i,J]L_{l}[t,J] \<br>
\end{eqnarray}</p>

      </div>
    </article>

    <hr />

    <div class="post-info">
      
      
    <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-folder meta-icon"><path d="M22 19a2 2 0 0 1-2 2H4a2 2 0 0 1-2-2V5a2 2 0 0 1 2-2h5l2 3h9a2 2 0 0 1 2 2z"></path></svg>

        <span class="tag"><a href="categories/deep-learning/">deep-learning</a></span>
        <span class="tag"><a href="categories/neuroscience/">neuroscience</a></span>
        
    </p>


      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text">
          <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path>
          <polyline points="14 2 14 8 20 8"></polyline>
          <line x1="16" y1="13" x2="8" y2="13"></line>
          <line x1="16" y1="17" x2="8" y2="17"></line>
          <polyline points="10 9 9 9 8 9"></polyline>
        </svg>
        983 Words
      </p>

      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar">
          <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect>
          <line x1="16" y1="2" x2="16" y2="6"></line>
          <line x1="8" y1="2" x2="8" y2="6"></line>
          <line x1="3" y1="10" x2="21" y2="10"></line>
        </svg>
        
          2020-11-16 18:00
        

         
          
        
      </p>
    </div>
      <hr />
      <div class="sharing-buttons">
        
<a class="resp-sharing-button__link" href="https://facebook.com/sharer/sharer.php?u=%2fposts%2fattention-is-all-you-need%2f" target="_blank" rel="noopener" aria-label="" title="Share on facebook">
  <div class="resp-sharing-button resp-sharing-button--facebook resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M18 2h-3a5 5 0 0 0-5 5v3H7v4h3v8h4v-8h3l1-4h-4V7a1 1 0 0 1 1-1h3z"></path></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="https://twitter.com/intent/tweet/?url=%2fposts%2fattention-is-all-you-need%2f" target="_blank" rel="noopener" aria-label="" title="Share on twitter">
  <div class="resp-sharing-button resp-sharing-button--twitter resp-sharing-button--small">
      <div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"></path></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="https://www.tumblr.com/widgets/share/tool?posttype=link&amp;title=Attention%20is%20all%20you%20need&amp;caption=Attention%20is%20all%20you%20need&amp;canonicalUrl=%2fposts%2fattention-is-all-you-need%2f" target="_blank" rel="noopener" aria-label="" title="Share on tumblr">
  <div class="resp-sharing-button resp-sharing-button--tumblr resp-sharing-button--small">
    <div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" stroke="none" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M14.563 24c-5.093 0-7.031-3.756-7.031-6.411V9.747H5.116V6.648c3.63-1.313 4.512-4.596 4.71-6.469C9.84.051 9.941 0 9.999 0h3.517v6.114h4.801v3.633h-4.82v7.47c.016 1.001.375 2.371 2.207 2.371h.09c.631-.02 1.486-.205 1.936-.419l1.156 3.425c-.436.636-2.4 1.374-4.156 1.404h-.178l.011.002z"/></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="mailto:?subject=Attention%20is%20all%20you%20need&amp;body=%2fposts%2fattention-is-all-you-need%2f" target="_self" rel="noopener" aria-label="" title="Share via email">
  <div class="resp-sharing-button resp-sharing-button--email resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"></path><polyline points="22,6 12,13 2,6"></polyline></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="https://pinterest.com/pin/create/button/?url=%2fposts%2fattention-is-all-you-need%2f&amp;media=%2fposts%2fattention-is-all-you-need%2f;description=Attention%20is%20all%20you%20need" target="_blank" rel="noopener" aria-label="" title="Share on pinterest">
  <div class="resp-sharing-button resp-sharing-button--pinterest resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" stroke="none"><path d="M12.017 0C5.396 0 .029 5.367.029 11.987c0 5.079 3.158 9.417 7.618 11.162-.105-.949-.199-2.403.041-3.439.219-.937 1.406-5.957 1.406-5.957s-.359-.72-.359-1.781c0-1.663.967-2.911 2.168-2.911 1.024 0 1.518.769 1.518 1.688 0 1.029-.653 2.567-.992 3.992-.285 1.193.6 2.165 1.775 2.165 2.128 0 3.768-2.245 3.768-5.487 0-2.861-2.063-4.869-5.008-4.869-3.41 0-5.409 2.562-5.409 5.199 0 1.033.394 2.143.889 2.741.099.12.112.225.085.345-.09.375-.293 1.199-.334 1.363-.053.225-.172.271-.401.165-1.495-.69-2.433-2.878-2.433-4.646 0-3.776 2.748-7.252 7.92-7.252 4.158 0 7.392 2.967 7.392 6.923 0 4.135-2.607 7.462-6.233 7.462-1.214 0-2.354-.629-2.758-1.379l-.749 2.848c-.269 1.045-1.004 2.352-1.498 3.146 1.123.345 2.306.535 3.55.535 6.607 0 11.985-5.365 11.985-11.987C23.97 5.39 18.592.026 11.985.026L12.017 0z"/></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=%2fposts%2fattention-is-all-you-need%2f&amp;title=Attention%20is%20all%20you%20need&amp;summary=Attention%20is%20all%20you%20need&amp;source=%2fposts%2fattention-is-all-you-need%2f" target="_blank" rel="noopener" aria-label="" title="Share on linkedin">
  <div class="resp-sharing-button resp-sharing-button--linkedin resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect x="2" y="9" width="4" height="12"></rect><circle cx="4" cy="4" r="2"></circle></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="https://reddit.com/submit/?url=%2fposts%2fattention-is-all-you-need%2f&amp;resubmit=true&amp;title=Attention%20is%20all%20you%20need" target="_blank" rel="noopener" aria-label="" title="Share on reddit">
  <div class="resp-sharing-button resp-sharing-button--reddit resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" stroke="none"><path d="M12 0A12 12 0 0 0 0 12a12 12 0 0 0 12 12 12 12 0 0 0 12-12A12 12 0 0 0 12 0zm5.01 4.744c.688 0 1.25.561 1.25 1.249a1.25 1.25 0 0 1-2.498.056l-2.597-.547-.8 3.747c1.824.07 3.48.632 4.674 1.488.308-.309.73-.491 1.207-.491.968 0 1.754.786 1.754 1.754 0 .716-.435 1.333-1.01 1.614a3.111 3.111 0 0 1 .042.52c0 2.694-3.13 4.87-7.004 4.87-3.874 0-7.004-2.176-7.004-4.87 0-.183.015-.366.043-.534A1.748 1.748 0 0 1 4.028 12c0-.968.786-1.754 1.754-1.754.463 0 .898.196 1.207.49 1.207-.883 2.878-1.43 4.744-1.487l.885-4.182a.342.342 0 0 1 .14-.197.35.35 0 0 1 .238-.042l2.906.617a1.214 1.214 0 0 1 1.108-.701zM9.25 12C8.561 12 8 12.562 8 13.25c0 .687.561 1.248 1.25 1.248.687 0 1.248-.561 1.248-1.249 0-.688-.561-1.249-1.249-1.249zm5.5 0c-.687 0-1.248.561-1.248 1.25 0 .687.561 1.248 1.249 1.248.688 0 1.249-.561 1.249-1.249 0-.687-.562-1.249-1.25-1.249zm-5.466 3.99a.327.327 0 0 0-.231.094.33.33 0 0 0 0 .463c.842.842 2.484.913 2.961.913.477 0 2.105-.056 2.961-.913a.361.361 0 0 0 .029-.463.33.33 0 0 0-.464 0c-.547.533-1.684.73-2.512.73-.828 0-1.979-.196-2.512-.73a.326.326 0 0 0-.232-.095z"/></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="https://www.xing.com/app/user?op=share;url=%2fposts%2fattention-is-all-you-need%2f;title=Attention%20is%20all%20you%20need" target="_blank" rel="noopener" aria-label="" title="Share on xing">
  <div class="resp-sharing-button resp-sharing-button--xing resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" stroke="none"><path d="M18.188 0c-.517 0-.741.325-.927.66 0 0-7.455 13.224-7.702 13.657.015.024 4.919 9.023 4.919 9.023.17.308.436.66.967.66h3.454c.211 0 .375-.078.463-.22.089-.151.089-.346-.009-.536l-4.879-8.916c-.004-.006-.004-.016 0-.022L22.139.756c.095-.191.097-.387.006-.535C22.056.078 21.894 0 21.686 0h-3.498zM3.648 4.74c-.211 0-.385.074-.473.216-.09.149-.078.339.02.531l2.34 4.05c.004.01.004.016 0 .021L1.86 16.051c-.099.188-.093.381 0 .529.085.142.239.234.45.234h3.461c.518 0 .766-.348.945-.667l3.734-6.609-2.378-4.155c-.172-.315-.434-.659-.962-.659H3.648v.016z"/></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="whatsapp://send?text=Attention%20is%20all%20you%20need%20%2fposts%2fattention-is-all-you-need%2f" target="_blank" rel="noopener" aria-label="" title="Share on whatsapp">
  <div class="resp-sharing-button resp-sharing-button--whatsapp resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" stroke="none" stroke-width="1" stroke-linecap="round" stroke-linejoin="round"><path d="M17.472 14.382c-.297-.149-1.758-.867-2.03-.967-.273-.099-.471-.148-.67.15-.197.297-.767.966-.94 1.164-.173.199-.347.223-.644.075-.297-.15-1.255-.463-2.39-1.475-.883-.788-1.48-1.761-1.653-2.059-.173-.297-.018-.458.13-.606.134-.133.298-.347.446-.52.149-.174.198-.298.298-.497.099-.198.05-.371-.025-.52-.075-.149-.669-1.612-.916-2.207-.242-.579-.487-.5-.669-.51-.173-.008-.371-.01-.57-.01-.198 0-.52.074-.792.372-.272.297-1.04 1.016-1.04 2.479 0 1.462 1.065 2.875 1.213 3.074.149.198 2.096 3.2 5.077 4.487.709.306 1.262.489 1.694.625.712.227 1.36.195 1.871.118.571-.085 1.758-.719 2.006-1.413.248-.694.248-1.289.173-1.413-.074-.124-.272-.198-.57-.347m-5.421 7.403h-.004a9.87 9.87 0 01-5.031-1.378l-.361-.214-3.741.982.998-3.648-.235-.374a9.86 9.86 0 01-1.51-5.26c.001-5.45 4.436-9.884 9.888-9.884 2.64 0 5.122 1.03 6.988 2.898a9.825 9.825 0 012.893 6.994c-.003 5.45-4.437 9.884-9.885 9.884m8.413-18.297A11.815 11.815 0 0012.05 0C5.495 0 .16 5.335.157 11.892c0 2.096.547 4.142 1.588 5.945L.057 24l6.305-1.654a11.882 11.882 0 005.683 1.448h.005c6.554 0 11.89-5.335 11.893-11.893a11.821 11.821 0 00-3.48-8.413Z"/></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="https://news.ycombinator.com/submitlink?u=%2fposts%2fattention-is-all-you-need%2f&amp;t=Attention%20is%20all%20you%20need" target="_blank" rel="noopener" aria-label="" title="Share on hacker news">
  <div class="resp-sharing-button resp-sharing-button--hackernews resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
			<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" stroke="none"><path d="M0 24V0h24v24H0zM6.951 5.896l4.112 7.708v5.064h1.583v-4.972l4.148-7.799h-1.749l-2.457 4.875c-.372.745-.688 1.434-.688 1.434s-.297-.708-.651-1.434L8.831 5.896h-1.88z"/></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="https://telegram.me/share/url?text=Attention%20is%20all%20you%20need&amp;url=%2fposts%2fattention-is-all-you-need%2f" target="_blank" rel="noopener" aria-label="" title="Share on telegram">
  <div class="resp-sharing-button resp-sharing-button--telegram resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="22" y1="2" x2="11" y2="13"></line><polygon points="22 2 15 22 11 13 2 9 22 2"></polygon></svg>
    </div>
  </div>
</a>

      </div>

    
      <div class="pagination">
        <div class="pagination__title">
          <span class="pagination__title-h">Read other posts</span>
          <hr />
        </div>

        <div class="pagination__buttons">
          
            <span class="button previous">
              <a href="/posts/analog-impulse-response/">
                <span class="button__icon">←</span>
                <span class="button__text">Analog impulse response</span>
              </a>
            </span>
          

          
            <span class="button next">
              <a href="/posts/deep-learning-backprop/">
                <span class="button__text">Backpropagation</span>
                <span class="button__icon">→</span>
              </a>
            </span>
          
        </div>
      </div>
    


    

  </main>

            </div>

            
                <footer class="footer">
    <div class="footer__inner">
        <div class="footer__content">
            <span>&copy; 2020</span>
            
            
                <span><a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC BY-NC 4.0</a></span>
            <span><a href="posts/index.xml" target="_blank" title="rss"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 20 20" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss"><path d="M4 11a9 9 0 0 1 9 9"></path><path d="M4 4a16 16 0 0 1 16 16"></path><circle cx="5" cy="19" r="1"></circle></svg></a></span>
        </div>
    </div>
    <div class="footer__inner">
        <div class="footer__content">
            <span>Powered by The Ring</span>
            <span><a href="https://github.com/cwseitz">Clayton Seitz</a></span>
          </div>
    </div>
</footer>

            
        </div>

        




<script type="text/javascript" src="/bundle.min.dc716e9092c9820b77f96da294d0120aeeb189b5bcea9752309ebea27fd53bbe6b13cffb2aca8ecf32525647ceb7001f76091de4199ac5a3caa432c070247f5b.js" integrity="sha512-3HFukJLJggt3&#43;W2ilNASCu6xibW86pdSMJ6&#43;on/VO75rE8/7KsqOzzJSVkfOtwAfdgkd5BmaxaPKpDLAcCR/Ww=="></script>



    </body>
</html>
