<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title></title>
<link rel="stylesheet" href="../../assets/main.css">
</head>
<body>
<div class="container"><header class="header">
                        <span class="header__inner">
                                <div class="logo">
  <span class="logo__mark">></span>
  <span class="logo__text">cd ~</span>
  <span class="logo__cursor"
  style="background-color:#ccf9f4;animation-duration:0.75s;">
  </span>
</div>

                                    <span class="header__right">
                                        <nav class="menu">
  <ul class="menu__inner">
    <li><a href="/">Home</a></li>
    <li><a href="/posts/">Drafts</a></li>
  </ul>
</nav>

                                    </span>
                        </span>
                        </header><div style="margin: 0 auto; width: 850px;
                border: 1px solid black; padding: 50px;
                margin-top: 100px; text-align: left;
                font-size: 14px">
                <hr />
<p>layout: post
title: "Recurrent neural networks"
date: 2020-11-17
categories: [deep-learning]</p>
<hr />
<h3>Time as Depth</h3>
<p>Recurrent neural networks differ from typical feedforward networks (like CNNs) in that a single set of parameters $A$ is used repeatedly rather than multiple sets of parameters stacked in space as in a MLP. This particularly useful for applications like language modeling where each word in a sentence comes at a particular time.</p>
<p>Below it can be seen that the same transformation $A$ is used in sequence and that transformation can be itself an MLP or whatever architecture is necessary. At each time-step, $A$ takes in an input $x_{t}$ and the output of the previous transformation $h_{t-1}$ which is referred to as the <em>hidden state</em>.</p>
<p><img src="../../images/rnn-model.png" width="300"/></p>
<p>This sort of architecture has proven to be useful in for data that has temporal order like language modeling. Each hidden state $h_{t}$ is a function of all of the previous states which is precisely what you want when building an <strong>autoregressive model</strong>.</p>
<p>\begin{eqnarray}
P_{\Phi}(w_{t}|w_{0},..,w_{t-1}) = \Pi_{t=0}^{T} P_{\Phi}(w_{t}|w_{0}...w_{t-1})
\end{eqnarray}</p>
<p>Although I haven't defined exactly how we compute $h$, I would like to motivate the use of RNNs by writing down how the realize the equation above in a model we can actually build</p>
<p>\begin{equation<em>}
\DeclareMathOperator</em>{\softmax}{softmax}
P_{\Phi}(w_{t}|w_{0},..,w_{t-1}) = \underset{w_{t}}{\softmax} e[w_{t},I]h[t-1,I]
\end{equation*}</p>
<p>where the vector $e[w_{t},I]$  is the <em>embedding</em> of the word $w$ and the hidden state of the previous operation $h[t-1,I]$ to predict a probability distribution for the word $w_{t}$. In practice, the transformation $A$ is composed of two-input linear threshold units where the hidden state $h$ at a time $t$ is computed as</p>
<p>\begin{equation<em>}
h[b,t,j] = \sigma(W^{h,h}[j,I]h[b,t-1, j] + W^{x,h}[j,K]x[b,t,K] - B[j])
\end{equation</em>}</p>
<p>You can see that it is a sum of linear transformations of $h$ and $x$ each with their own distinct matrices $W$ compared to a threshold.</p>
<h3>The Gated RNN</h3>
<p>One very useful feature of RNNs is that they can pass information across time and have a somewhat primitive form of <strong>memory</strong>. The kind of memory an RNN has might be described as a data-dependent data flow. By this we mean that architecture retains the important parts of the data and can filter the rest. One implementation of that if the gated RNN (GRNN).</p>
<p>\begin{equation<em>}
h[b,t,j] = G[b,t,j]\odot h[b,t-1, j] + (1-G[b,t,j])\odot R[b,t,j]
\end{equation</em>}</p>
<p>This gate determines whether to remember a value or to replace it in a data-dependent way. $R[b,t,j]$ is the replacement value while $G[b,t,j]$ is the gate which are both computed using a two-input linear threshold unit</p>
<p>\begin{eqnarray}
R[b,t,j] &amp;=&amp; \tanh(W^{h,R}[j,I]h[b,t-1, j] + W^{x,R}[j,K]x[b,t,K] - B^{R}[j]) \
G[b,t,j] &amp;=&amp; \sigma(W^{h,G}[j,I]h[b,t-1, j] + W^{x,G}[j,K]x[b,t,K] - B^{G}[j]) \
\end{eqnarray}</p>
<h3>Bidirectional RNNs</h3>
<p>Another useful architecture especially for machine translation is the bidirectional RNN. In this setup, you compute the hidden states in both directions and concatenate the results to make predictions</p>
<pre><code class="code">
</code></pre>
                </div>
                 <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
                        <script type="text/javascript" id="MathJax-script" async
                        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
                        <script type="text/javascript" src="assets/main.js"></script></div></body>
</html>
