{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:50% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:50% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error rate vs. Loss\n",
    "\n",
    "I need to make an important distinct before proceeding. The loss function, which we typically take to be the cross-entropy loss, is fundamentally distinct from the error-rate. The error-rate is the fraction of testing, validation, or testing samples that we correctly predict. This is an important metric for assessing the performance of a model and also things such as overfitting.\n",
    "\n",
    "### Regularization\n",
    "\n",
    "A major downfall of deep networks is that they often fail to generalize. Models can be trained to perform quite well on a batch of test data but their performance dwindles when using the same model to predict a validation or test sample. This is often called overfitting. We would like a better way to train our models so that we can bring the validation error rate closer to the testing error rate i.e. to prevent overfitting. One way to achieve this is through *regularization* which defines the search for an optimum model $\\Phi^{*}$ such that not all models have equal probability. This is made possible by first making the following observations\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\DeclareMathOperator*{\\argmin}{argmin}\n",
    "\\DeclareMathOperator*{\\argmax}{argmax}\n",
    "\\Phi^* & = & \\argmax_\\Phi\\;p(\\Phi | (x_1,y_1),\\ldots,(x_n,y_n)) \\\\\n",
    "\\\\\n",
    " & = & \\argmax_\\Phi\\;p(\\Phi,\\;(x_1,y_1),\\ldots,(x_n,y_n)) \\\\\n",
    " \\\\\n",
    "  & = & \\argmax_\\Phi\\;p(\\Phi)P((x_1,y_1),\\ldots,(x_n,y_n)\\;|\\;\\Phi) \\\\\n",
    " \\\\\n",
    " & = & \\argmax_\\Phi \\; p(\\Phi)\\;\\prod_i \\mathcal{Pop}(x_i)P_\\Phi(y_i|x_i) \\\\\n",
    " \\\\\n",
    "  & = & \\argmax_\\Phi \\; p(\\Phi)\\;\\prod_i P_\\Phi(y_i|x_i)\n",
    " \\end{eqnarray*}\n",
    "\n",
    "where we refer to $p(\\Phi)$ as the model-prior probability and constraints on that probability define the following different forms of regularization.\n",
    "\n",
    "### L1 Regularization\n",
    "\n",
    "L1 regularization gets its name from the fact that we use the L1 norm of the parameter vector $\\Phi$ to define our model-prior. For L1 regularization, we define the prior to be Poisson and plug it into the last expression we derived above\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\DeclareMathOperator*{\\argmin}{argmin}\n",
    "\\DeclareMathOperator*{\\argmax}{argmax}\n",
    "p(\\Phi) & \\propto & e^{-\\lambda ||\\Phi||_1} \\;\\;\\;\\;\\;\\;\\;\\;||\\Phi||_1 = \\sum_i |\\Phi_i| \\\\\n",
    "\\\\\n",
    "\\Phi^* & = & \\argmax_\\Phi \\; \\;\\;p(\\Phi) \\prod_i P_\\Phi(y_i|x_i) \\\\\n",
    "\\\\\n",
    "\\Phi^* & = & \\argmin_\\Phi \\; \\;\\;\\left(\\sum_i\\; -\\ln P_\\Phi(y_i|x_i)\\right) \\;+ \\; \\;\\lambda||\\Phi||_1 \\\\\n",
    "\\\\\n",
    "\\Phi^* & = & \\argmin_\\Phi \\; \\;\\;\\hat{\\cal L}(\\Phi) \\;+ \\; \\;\\frac{\\lambda}{N_{\\mathrm{Train}}}||\\Phi||_1\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Notice that we take a negative log and divided by $N$ to transform it into something that looks like an average cross-entropy loss over the training set $\\hat{\\mathcal{L}}$ with an extra term to give us our new objective which is optimized as follows \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\Phi_i & = & \\Phi_i - \\eta \\left(\\hat{g}_i + \\frac{\\lambda}{N_{\\mathrm{Train}}}\\;\\mathrm{sign}(\\Phi_i)\\right)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "For such an objective, $\\Phi^*_i = 0 $ when $|g_i| <  \\lambda/N_{\\mathrm{Train}}$ and $g_i = -(\\lambda/N_{\\mathrm{Train}}) \\mathrm{sign}(\\Phi_i)$ otherwise.\n",
    "\n",
    "### L2 Regularization\n",
    "\n",
    "For L2 regularization, which is often called shrinkage, we define the model-prior to be Gaussian and use the L2 norm of the parameter vector. The argument is very similar to above and it can be shown that the objective and update equation under a Gaussian prior is \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\DeclareMathOperator*{\\argmin}{argmin}\n",
    "\\DeclareMathOperator*{\\argmax}{argmax}\n",
    "\\Phi^* & = & \\argmin_\\Phi \\; \\;\\;\\hat{\\cal L}(\\Phi) \\;+ \\; \\;\\frac{1}{2N\\sigma^2} ||\\Phi||^2\n",
    "\\end{eqnarray*}\n",
    "\n",
    "$$\\Phi_{i+1} = \\Phi_i - \\eta\\hat{g}_i  - \\frac{\\eta}{N\\sigma^2}\\Phi$$\n",
    "\n",
    "Now we can see why this is called shrinkage\n",
    "\n",
    "$${ \\Phi_{i+1}} = \\Phi_i - \\eta\\hat{g} - \\frac{\\eta}{N\\sigma^2}\\Phi_i\\;\\;\\;\\; = {\\Phi_i - \\eta\\hat{g} - \\gamma\\Phi_i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "nikola": {
   "category": "neuro",
   "date": "2020-11-09 22:33:08 UTC-05:00",
   "description": "",
   "link": "",
   "slug": "regularization",
   "tags": "",
   "title": "Regularization",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
