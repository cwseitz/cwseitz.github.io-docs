\documentclass[12pt]{article}
\usepackage{amsmath} % AMS Math Package
\usepackage{amsthm} % Theorem Formatting
\usepackage{amssymb}    % Math symbols such as \mathbb
\usepackage{graphicx} % Allows for eps images
\usepackage[dvips,letterpaper,margin=1in,bottom=0.7in]{geometry}
\usepackage{tensor}
\usepackage{amsmath}
\usepackage{siunitx}

\newtheorem{p}{Problem}[section]
\usepackage{cancel}
\newtheorem*{lem}{Lemma}
\theoremstyle{definition}
\newtheorem*{dfn}{Definition}
 \newenvironment{s}{%\small%
        \begin{trivlist} \item \textbf{Solution}. }{%
            \hspace*{\fill} $\blacksquare$\end{trivlist}}%


\begin{document}

{\noindent\Huge\bf  \\[0.5\baselineskip] {\fontfamily{cmr}\selectfont  Homework 2}         }\\[2\baselineskip] % Title
{ {\bf \fontfamily{cmr}\selectfont Biophysics of Biomolecules}\\ {\textit{\fontfamily{cmr}\selectfont     April 13th, 2021}}}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    {\large \textsc{Clayton Seitz}
\\[1.4\baselineskip] 

\begin{p} Two teams A and B play a series of up to 5 games, in which the team to win 3 games wins the series.   Let X be a random variable which is a sequence of letters corresponding to the winners of each of the games played - possible values for X then include AAA, ABBAB etc. Let Y be the number of games played (the teams play till the series winner is decided).  Calculate $H(X), H(Y), H(X|Y), H(Y|X)$ and $I(X;Y)$. Assume both teams are equally likely to win each game independent of any previous games.
\end{p}

\begin{s} 
The series can be 3, 4, or 5 games long. There are a total of $2^{5}=32$ possible 5-bit strings but we need to truncate strings after one team has won. Starting with 32 leaves, any nodes below strings 000 or 111 in the binary tree should be deleted. This trims $2^{5}\cdot \frac{2}{2^{3}}=8$ leaves down to 2. Also, there are 6 ways the series can end after 4 games, so we trim $2^{5}(\frac{6}{2^{4}})=12$ leaves to 6. We have removed a total of 12 leaves leaving us with 20 possible leaves (series).
 
\begin{equation*}
H(X) = \sum_{X} P(X)\frac{1}{\log P(X)} = \log 20 \approx 4.3 \si{bits}
\end{equation*}

where $\log x = \log_{2} x$. There are 2 ways the series can end after 3 games, 6 ways it can end after 4 games, and 12 ways it can end after 5 games. If $Y$ is the number of games played, 

\begin{equation*}
H(Y) = \sum_{Y} P(Y)\frac{1}{\log P(Y)} = \frac{1}{10}\log 10 + \frac{3}{10}\log\frac{10}{3} + \frac{6}{10}\log \frac{5}{3} \approx  1.3 \si{bits}
\end{equation*}

The conditional entropy $H(X|Y)$ is given by

\begin{equation*}
H(X|Y) = \sum_{y} P(y)H(X|Y=y) = \frac{1}{10}\cdot 1 + \frac{3}{10}\cdot \log 6 + \frac{6}{10}\cdot \log 12 \approx 3 \si{bits}
\end{equation*}

We see that conditioning on $Y$ reduced the entropy in $X$. The conditional entropy $H(Y|X)$ is

\begin{equation*}
H(Y|X) = \sum_{x} P(x)H(Y|X=x) =  0 \si{bits}
\end{equation*}

since if we are given the series, it is certain how many games were played. Finally the mutual information $I(X;Y)$ is

\begin{equation*}
I(X;Y) = H(X) - H(X|Y) = 1.3\si{bits}
\end{equation*}
\end{s}


\begin{p}
Lost in transmission
\end{p}

\begin{s}
\begin{eqnarray*}
I(X_{n};X_{1}) &=& H(x_{n}) - H(x_{n}|x_{1})\\
&=& \sum_{y}P(x_{1}=y)\sum_{x}P(x_{n}=x|x_{1}=y)\log \frac{1}{P(x_{n}=x|x_{n}=y)}\\
\end{eqnarray*}

where $x,y \rightarrow 0,1$. We can compute $P(x_{n}=x_{1})$ by noticing that this will only occur if there is an even number of bit flips. If $\epsilon$ is the probability of a flip which we can define as a "success" and each trial is independent, then $P(x_{n}=x_{1})$ is a sum over the binomial distribution $B(n,k)$ for even values of $k$. Define $P(x_{n}=x_{1})$ as

\begin{eqnarray*}
\Omega &=& 1-\frac{1}{2}[1+(1-2\epsilon)^{n}]
\end{eqnarray*}

From which we can compute the entropy

\begin{eqnarray*}
H(x_{n}|x_{1}) &=& \Omega \log \frac{1}{\Omega} + (1-\Omega)\log\frac{1}{1-\Omega}\\
&=& H_{b}(\Omega)
\end{eqnarray*}

and because $H(x_{n})=1$, the mutual information is given by

\begin{eqnarray*}
I(X_{n};X_{1}) &=& 1 - \Omega \log \frac{1}{\Omega} + (1-\Omega)\log\frac{1}{1-\Omega}\\
\end{eqnarray*}

As a sanity check, we can show that for $n=1$ this takes the form of a binary symmetric channel and when $\epsilon = 0$ or $\epsilon = 1$ we have $I(x_{1};x_{n})=1$.


\end{s}

\begin{p}
Prove the following basic identities about the quantities we have studied so far.
\end{p}

\begin{s}
\begin{eqnarray}
D(P||Q) &=& \sum P(x)\log \frac{P(x)}{Q(x)}\\
&=& \sum P(x)\log |\chi | P(x)\\
&=& \sum P(x) \log P(x) + \sum P(x)\log |\chi |\\
&=& \log |\chi| - H(x)
\end{eqnarray}

\begin{eqnarray}
I(X;Y) &=& H(X) - H(X|Y)\\
&=& \sum P(x)\log \frac{1}{P(x)} + \sum P(x,y)\log \frac{P(x,y)}{P(y)} \\
&=& \sum P(x,y) \log \frac{P(x,y)}{P(x)P(y)}\\
&=& D(P(x,y)||P(x)P(y))
\end{eqnarray}


\end{s}

\begin{p}
Expand our definition for mutual information between three random variables $X,Y,Z$.
\end{p}

\begin{s}
\begin{eqnarray*}
I(X;Y;Z) &=& I(X;Y) - I(X;Y|Z)\\
&=& [H(X) + H(Y) - H(XY)]- [H(XZ)+H(YZ)-H(XYZ)-H(Z)] \\
&=& H(X) + H(Y) + H(Z) - H(XY) - H(XZ) - H(YZ) + H(XYZ)
\end{eqnarray*}
\end{s}

We need to design $X,Y,Z$ such that $I(X;Y|Z) > I(X;Y)$. Lets consider a scenario where $I(X;Z) = I(Y;Z)=0$ but together $Y,Z$ carry information about $X$.

\[(X,Y,Z)= \begin{cases} 
      011 & w.p \; 1/4 \\
      000 & w.p \; 1/4 \\
      101 & w.p \; 1/4 \\
      110 & w.p \; 1/4 \\
   \end{cases}
\]

It is necessary to show that $I(X;Y|Z) > 0$. Indeed,

\begin{eqnarray*}
I(X;Y|Z) &=& \mathbf{E}_{z}[I(X|Z=z; Y|Z=z)]\\
&=& \frac{1}{2}I(X|Z=0; Y|Z=0) + \frac{1}{2}I(X|Z=1; Y|Z=1)\\
&=& \frac{1}{2}\log 2 + \frac{1}{2} \log 2 = 1
\end{eqnarray*}

which means that $I(X;Y;Z) < 0$.

\begin{p}
Prove that $\mathbf{E}_{x}||P(Y|X=x) - P(Y)||_{1} \leq \sqrt{2\ln 2 \;I(X;Y)}$
\end{p}

\begin{s}
We can use  Pinsker's inequality

\begin{equation*}
||P(Y|X)-P(Y)||_{1} \leq \sqrt{2\ln 2 \; D_{KL}(P(Y|X)||P(Y))}
\end{equation*}

\begin{eqnarray*}
\mathbf{E}_{x}||P(Y|X=x) - P(Y)||_{1} &\leq & \mathbf{E}_{x}\sqrt{2\ln 2 \;D_{KL}(P(Y|X=x)||P(Y))}\\
&\leq & \sqrt{2\ln 2 \cdot \mathbf{E}_{x}\; D_{KL}(P(Y|X=x)||P(Y))}\\
&=& \sqrt{2\ln 2 \;I(X;Y)}
\end{eqnarray*}

\end{s}

\begin{p}
Prove that 

\begin{equation*}
\sum (\frac{\sqrt{5}-1}{2})^{e_{i}} \leq 1
\end{equation*}

\end{p}

\begin{s}
Let the sum of energies of all possible codes in a tree of depth $l_{max}$ be $E$. We need to show that the fraction of $E$ we delete when we select a code of energy $e_{i}$ is equal to $(\frac{\sqrt{5}-1}{2})^{e_{i}}$.

\end{s}


\end{document}