\documentclass[12pt]{article}
\usepackage{amsmath} % AMS Math Package
\usepackage{amsthm} % Theorem Formatting
\usepackage{amssymb}    % Math symbols such as \mathbb
\usepackage{graphicx} % Allows for eps images
\usepackage[dvips,letterpaper,margin=1in,bottom=0.7in]{geometry}
\usepackage{tensor}
\usepackage{amsmath}
\usepackage{siunitx}

\newtheorem{p}{Problem}[section]
\usepackage{cancel}
\newtheorem*{lem}{Lemma}
\theoremstyle{definition}
\newtheorem*{dfn}{Definition}
 \newenvironment{s}{%\small%
        \begin{trivlist} \item \textbf{Solution}. }{%
            \hspace*{\fill} $\blacksquare$\end{trivlist}}%


\begin{document}

{\noindent\Huge\bf  \\[0.5\baselineskip] {\fontfamily{cmr}\selectfont  Problem Set I}         }\\[2\baselineskip] % Title
{ {\bf \fontfamily{cmr}\selectfont Information and Coding Theory}\\ {\textit{\fontfamily{cmr}\selectfont     January 29, 2021}}}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    {\large \textsc{Clayton Seitz}
\\[1.4\baselineskip] 

\begin{p} Two teams A and B play a series of up to 5 games, in which the team to win 3 games wins the series.   Let X be a random variable which is a sequence of letters corresponding to the winners of each of the games played - possible values for X then include AAA, ABBAB etc. Let Y be the number of games played (the teams play till the series winner is decided).  Calculate $H(X), H(Y), H(X|Y), H(Y|X)$ and $I(X;Y)$. Assume both teams are equally likely to win each game independent of any previous games.
\end{p}

\begin{s} 
The series can be 3, 4, or 5 games long. There are a total of $2^{5}=32$ possible 5-bit strings but we need to truncate strings after one team has won. For example, any nodes below strings 000 or 111 in the binary tree should be deleted. This removes a total of  $2^{5}(\frac{2}{2^{3}})=8 $ possible leaves. Also, there are 8 ways the series can end after 4 games, so we subtract $2^{5}(\frac{8}{2^{4}})=16$ more leaves. However, two of these were already subtracted before so we have a total of 10 possible series. Assuming each is equally likely,
 
\begin{equation*}
H(X) = \sum_{X} P(X)\frac{1}{\log P(X)} = \log 10 \approx 3.3 \si{bits}
\end{equation*}

where $\log x = \log_{2} x$. There are 2 ways the series can end after 3 games or after 5 games and there is 6 ways it can end after 4 games. Therefore, 

\begin{equation*}
H(Y) = \sum_{Y} P(Y)\frac{1}{\log P(Y)} = \frac{2}{5}\log 5 + \frac{3}{5}\log\frac{5}{3} \approx 1.4 \si{bits}
\end{equation*}

The conditional entropy $H(X|Y)$ is given by

\begin{equation*}
H(X|Y) = \sum_{y} P(y)H(X|Y=y) = \frac{1}{5}\cdot 1 + \frac{3}{5}\cdot \log 6 + \frac{1}{5}\cdot 1 \approx 2 \si{bits}
\end{equation*}

The conditional entropy $H(Y|X)$ is given by

\begin{equation*}
H(Y|X) = \sum_{x} P(x)H(Y|X=x) =  0 \si{bits}
\end{equation*}

The mutual information $I(X;Y)$ is given by

\begin{equation*}
I(X;Y) = H(X) - H(X|Y) = 1.3\si{bits}
\end{equation*}
\end{s}


\begin{p}
Lost in transmission
\end{p}

\begin{s}
\begin{eqnarray*}
I(X_{n};X_{1}) &=& H(x_{n}) - H(x_{n}|x_{1})\\
&=& \sum_{y}P(x_{1}=y)\sum_{x}P(x_{n}=x|x_{1}=y)\log \frac{1}{P(x_{n}=x|x_{n}=y)}\\
\end{eqnarray*}

where $x,y \rightarrow 0,1$. We can compute $P(x_{n}=x_{1})$ by noticing that this will only occur if there is an even number of bit flips. If $\epsilon$ is the probability of a flip which we can define as a "success" and each trial is independent, then $P(x_{n}=x_{1})$ is a sum over the binomial distribution $B(n,k)$ for even values of $k$. Define $P(x_{n}=x_{1})$ as

\begin{eqnarray*}
\Omega &=& 1-\frac{1}{2}[1+(1-2\epsilon)^{n}]
\end{eqnarray*}

Therefore ,

\begin{eqnarray*}
H(x_{n}|x_{1}) &=& \Omega \log \frac{1}{\Omega} + (1-\Omega)\log\frac{1}{1-\Omega}\\
&=& H_{b}(\Omega)
\end{eqnarray*}

and because $H(x_{n})=1$, the mutual information is given by

\begin{eqnarray*}
I(X_{n};X_{1}) &=& 1 - \Omega \log \frac{1}{\Omega} + (1-\Omega)\log\frac{1}{1-\Omega}\\
\end{eqnarray*}

As a sanity check, we can show that for $n=1$ this takes the form of a binary symmetric channel and when $\epsilon = 0$ or $\epsilon = 1$ we have $I(x_{1};x_{n})=1$.


\end{s}

\begin{p}
Prove the following basic identities about the quantities we have studied so far.
\end{p}

\begin{s}
\begin{eqnarray}
D(P||Q) &=& \sum P(x)\log \frac{P(x)}{Q(x)}\\
&=& \sum P(x)\log |\chi | P(x)\\
&=& \sum P(x) \log P(x) + \sum P(x)\log |\chi |\\
&=& \log |\chi| - H(x)
\end{eqnarray}

\begin{eqnarray}
I(X;Y) &=& H(X) - H(X|Y)\\
&=& \sum P(x)\log \frac{1}{P(x)} + \sum P(x,y)\log \frac{P(x,y)}{P(y)} \\
&=& \sum P(x,y) \log \frac{P(x,y)}{P(x)P(y)}\\
&=& D(P(x,y)||P(x)P(y))
\end{eqnarray}


\end{s}

\begin{p}
Expand our definition for mutual information between three random variables $X,Y,Z$.
\end{p}

\begin{s}
\begin{eqnarray*}
I(X;Y;Z) &=& I(X;Y) - I(X;Y|Z)\\
&=& [H(X) + H(Y) - H(XY)]- [H(XZ)+H(YZ)-H(XYZ)-H(Z)] \\
&=& H(X) + H(Y) + H(Z) - H(XY) - H(XZ) - H(YZ) + H(XYZ)
\end{eqnarray*}
\end{s}

We need to design $X,Y,Z$ such that $I(X;Y|Z) > I(X;Y)$. Lets consider a scenario where $I(X;Z) = I(Y;Z)=0$ but together $Y,Z$ carry information about $X$.

\[(X,Y,Z)= \begin{cases} 
      011 & w.p \; 1/4 \\
      000 & w.p \; 1/4 \\
      101 & w.p \; 1/4 \\
      110 & w.p \; 1/4 \\
   \end{cases}
\]

It is necessary to show that $I(X;Y|Z) > 0$. Indeed,

\begin{eqnarray*}
I(X;Y|Z) &=& \mathbf{E}_{z}[I(X|Z=z; Y|Z=z)]\\
&=& \frac{1}{2}I(X|Z=0; Y|Z=0) + \frac{1}{2}I(X|Z=1; Y|Z=1)\\
&=& \frac{1}{2}\log 2 + \frac{1}{2} \log 2 = 1
\end{eqnarray*}

which means that $I(X;Y;Z) < 0$.

\begin{p}
Prove that $\mathbf{E}_{x}||P(Y|X=x) - P(Y)||_{1} \leq \sqrt{2\ln 2 \;I(X;Y)}$
\end{p}

\begin{s}
We begin with Pinsker's inequality

\begin{equation*}
||P(Y|X)-P(Y)||_{1} \leq \sqrt{2\ln 2 \; D_{KL}(P(Y|X)||P(Y))}
\end{equation*}

\begin{eqnarray*}
\mathbf{E}_{x}||P(Y|X=x) - P(Y)||_{1} &\leq & \mathbf{E}_{x}\sqrt{2\ln 2 \;D_{KL}(P(Y|X=x)||P(Y))}\\
&\leq & \sqrt{2\ln 2 \cdot \mathbf{E}_{x}\; D_{KL}(P(Y|X=x)||P(Y))}\\
&=& \sqrt{2\ln 2 \;I(X;Y)}
\end{eqnarray*}

\end{s}




\end{document}