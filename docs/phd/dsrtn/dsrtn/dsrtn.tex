% ---- ETD Document Class and Useful Packages ---- %
\documentclass{ucetd}
\usepackage{subfigure,epsfig,amsfonts}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[toc,page]{appendix}
\usepackage[labelfont=bf]{caption}
\usepackage{rotating}
\usepackage[dvipsnames]{xcolor}
\usepackage{url}
\usepackage{bm}
\usepackage{bbm}

%% Use these commands to set biographic information for the title page:
\title{Visualizing nucleosome cluster dynamics with dense single molecule localization microscopy}
\author{Clayton W. Seitz}
\department{Department of Physics}
\division{Physical Sciences}
\degree{Doctor of Philosophy}
\date{Spring 20XX}

%% Use these commands to set a dedication and epigraph text

\epigraph{Epigraph}



\begin{document}
%% Basic setup commands
% If you don't want a title page comment out the next line and uncomment the line after it:
\maketitle
%\omittitle

% These lines can be commented out to disable the copyright/dedication/epigraph pages
\makecopyright
%\makededication


%% Make the various tables of contents
\tableofcontents
%\listoffigures
%\listoftables

%\acknowledgments
% Enter Acknowledgements here

\abstract



\clearpage

\mainmatter

\chapter{Introduction}

\chapter{Literature Review}

\chapter{Materials and Methods}

\section{Single molecule localization microscopy}


Single molecule localization microscopy (SMLM) is a type of super-resolution microscopy that allows the imaging of fluorescently-labeled molecules with high precision, well beyond the diffraction limit of light. SMLM techniques such as direct stochastic optical reconstruction microscopy (dSTORM), photoactivated localization microscopy (PALM), and related methods rely on the precise localization of single molecules by resolving them in time rather than space. By combining the precise localization of many individual molecules, SMLM can generate images with resolution down to a few nanometers. SMLM has been used in a variety of applications, including the imaging of subcellular structures such as synapses, mitochondria, and cytoskeletal elements, as well as the study of protein-protein interactions, molecular dynamics, and other biological processes at the nanoscale level.

Despite its successs and gaining popularity, the basic principle of SMLM is one of its primary limitations: the need for sparse activation leads to long acquisition times and expensive autofocusing equipment to actively correct for sample drift. This results in low throughput, poor time resolution when imaging dynamic processes, low labeling densities and a reduced choice of fluorophores. In addition, the need for sparse activation requires laborious optimization of dSTORM buffers containing oxygen scavenging systems and/or oxygen purging techniques. In response to these problems, a host software tools for SMLM have emerged, which permit the acquisition of emitters at higher densities. (Speiser 2021). In the multi-emitter setting, PSFs are no longer well-separated but may overlap, adding additional 1uncertainty into the localization process. Existing algorithms have explicitly modeled the point spread function as a mixture of single molecule PSFs or have utilized deep learning-based tools to estimate the parameters of each PSF embedded in the mixture.  

Due to overlap, conventional detection strategies may undercount the emitters in a local neighborhood in some frames, localization uncertainty can increase for overlapping emitters, and some localizations may be missed entirely. These complications make conventional detection strategies inappropriate for reconstruction of super-resolution images from time-series. Perhaps most importantly, overlapping emitters can result in additional localization uncertainty, rendering discernment between the arrival of a new particle and a poorly localized existing one difficult. This issue poses a major bottleneck to super-resolution imaging acqusitions. Additional uncertainty can be partially alleviated by using pairwise or higher-order temporal correlations within a pixel neighborhood to deconvolve individual emitters. A similar idea is employed in super-resolution optical fluctuation imaging (SOFI) - a post-processing technique that uses image cumulants to deconvolve emitters. 


\subsection{Statistics of sCMOS cameras}

We are now prepared to write the shot-noise limited signal, which is a vector with units of phototelectrons

\begin{equation}
\vec{S} = \left[\mathrm{Poisson}(\mu_{1}), \mathrm{Poisson}(\mu_{2}), ..., \mathrm{Poisson}(\mu_{N})\right]
\end{equation}


However this noise model is incomplete, because detectors often suffer from dark noise, which may refer to readout noise or dark current, and contributes to a nonzero signal even in the absence of incident light. Dark current is due to statistical fluctuations in the photoelectron count due to thermal fluctuations. Readout noise is introduced by the amplifier circuit during the coversion of photoelectron charge to a voltage. Here, we use the Hamamatsu ORCA v3 CMOS camera, which is air cooled to -10C and has very low dark current - around 0.06 electrons/pixel/second - and can therefore be safely ignored for exposure times on the order of milliseconds. Readout noise has been often neglected in localization algorithms because its presence in EMCCD cameras is small enough that it can be ignored within the tolerances of the localization precision. In the case of sCMOS cameras, however, the readout noise of each pixel is significantly higher and, in addition, every pixel has its own noise and gain characteristic sometimes with dramatic pixel-to-pixel variations.

It is important to note that we cannot measure the contribution by readout noise before amplification and therefore it must be expressed in units of $\mathrm{ADU}$. This is in contrast to $\vec{S}$, which can be expressed in units of photoelectrons, because these statistical fluctuations can be predicted to be Poisson by quantum mechanics. Furthermore, the number of photoelectrons $S_{k}$ is  multiplied by a gain factor $g_{k}$ which has units of $[\mathrm{ADU}/e^{-}]$, which generally must be measured for each pixel. Here, we will always assume that readout noise per pixel $\xi_{k}$ is Gaussian with some pixel-specific offset $o_{k}$ and variance $\sigma_{k}^{2}$. We will also assume that gain factors and pixel noise characteristics are constants and do not scale with the signal level $S_{k}$. Therefore our measurement, in units of ADU, is: 

\begin{equation}
\vec{H} = \vec{S} + \vec{\xi}
\end{equation}

What we are after is the joint distribution $P(\vec{H})$. A fundamental result in probability theory is that the distribution of $H_{k}$ is the convolution of the distributions of $S_{k}$ and $\xi_{k}$,

\begin{align}
P(H_{k}|\theta) &= P(S_{k})\circledast P(\xi_{k})\\
&= A\sum_{q=0}^{\infty} \frac{1}{q!}e^{-\mu_{k}}\mu_{k}^{q}\frac{1}{\sqrt{2\pi}\sigma_{k}}e^{-\frac{(H_{k}-g_{k}q-o_{k})}{2\sigma_{k}^{2}}}
\end{align}

where $P(\xi_{k}) = \mathcal{N}(o_{k},\sigma_{k}^{2})$ and $P(S_{k}) = \mathrm{Poisson}(g_{k}\mu_{k})$. In practice, this expression is difficult to work with, so we look for an approximation. Notice that 

\begin{align*}
\xi_{k} - o_{k} + \sigma_{k}^{2} \sim \mathcal{N}(\sigma_{k}^{2},\sigma_{k}^{2}) \approx \mathrm{Poisson}(\sigma_{k}^{2})
\end{align*}

Since $H_{k} = S_{k} + \xi_{k}$, we transform $H_{k}' = H_{k} - o_{k} + \sigma_{k}^{2}$, which is distributed according to 

\begin{align*}
H_{k}' \sim \mathrm{Poisson}(\mu_{k}')
\end{align*}

where $\mu_{k}' = g_{k}\mu_{k} + \sigma_{k}^{2}$. This result can be seen from the fact the the convolution of two Poisson distributions is also Poisson.

\subsection{Integrated isotropic Gaussian point spread function}

Due to diffraction, any point emitter, such as a single fluorescent molecule, will be registered as a diffraction limited spot. It is common to describe the point spread function as a two-dimensional isotropic Gaussian:

\begin{equation*}
\mathrm{G}(x,y) = \frac{1}{2\pi\sigma^{2}}e^{-\frac{(x-x_{0})^{2}+(y-y_{0})^{2}}{2\sigma^{2}}}
\end{equation*}

Modern cameras used in light microscopy, such as scientific complementary metal oxide semiconductor (sCMOS) cameras, are powered by the photoelectric effect. Electrons within each pixel, called photoelectrons, absorb enough energy from incoming photons to be promoted to the conduction band to give electrical current which can be detected. Integration of photoelectrons during the exposure time results in a monochrome image captured by a camera. The image of a single point particle, such as a fluorescent molecule, can be thought of as two-dimensional histogram of photon arrivals and a discretized form of the classical intensity profile $\mathrm{G}(x,y)$. The value at a pixel approaches an integral of this density over the pixel:

\begin{equation}
\mu_{k} = i_{0}\lambda_{k} = i_{0}\int_{\mathrm{pixel}} G(x,y)dxdy
\end{equation}

Let $(x_{k},y_{k})$ be the center of pixel $k$. If a fluorescent molecule is located at $(x_{0},y_{0})$, the probability of a photon arriving at pixel $k$ per unit time reads

\begin{equation*}
\lambda_{k} = \int_{x_{k}-\frac{1}{2}}^{x_{k}+\frac{1}{2}}G(x-x_{0})dx \int_{y_{k}-\frac{1}{2}}^{y_{k}+\frac{1}{2}} G(y-y_{0})dy
\end{equation*}

\vspace{0.2in}
where $i_{0} = g_{k}\eta N_{0}\Delta$. The parameter $\eta$ is the quantum efficiency and $\Delta$ is the exposure time. $N_{0}$ represents the number of photons emitted per unit time, which may be itself a Poisson random variable; however, this is inconsequential since it is a fixed value for a single image of a fluorescent molecule. We can then express the Gaussian integrals over a pixel by making use of the following property of the error function

\begin{equation*}
\frac{1}{\sqrt{2\pi}\sigma}\int_{a}^{b} e^{\frac{-(x-\mu)^{2}}{2\sigma^{2}}} = \frac{1}{2}\left(\mathrm{erf}\left(\frac{b-\mu}{\sqrt{2}\sigma}\right) -\mathrm{erf}\left(\frac{a-\mu}{\sqrt{2}\sigma}\right)\right)
\end{equation*}

This gives a convenient expression for the fraction of photons which arrive at a pixel $k$

\begin{align*}
\lambda_{k}(x) &= \frac{1}{2}\left(\mathrm{erf}\left(\frac{x_{k}+\frac{1}{2}-x_{0}}{\sqrt{2}\sigma}\right) -\mathrm{erf}\left(\frac{x_{k}-\frac{1}{2}-x_{0}}{\sqrt{2}\sigma}\right)\right)\\
\lambda_{k}(y) &= \frac{1}{2}\left(\mathrm{erf}\left(\frac{y_{k}+\frac{1}{2}-y_{0}}{\sqrt{2}\sigma}\right) -\mathrm{erf}\left(\frac{y_{k}-\frac{1}{2}-y_{0}}{\sqrt{2}\sigma}\right)\right)
\end{align*}

\subsection{Integrated astigmatic Gaussian point spread function}

In 2D SMLM simulations, a 2D PSF  model  with  a z-dependent  isotropic  width $\sigma$ can  be  used. For 3D, we could use that the isotropic Gaussian point spread function has a FWHM $\sigma$ which is dependent on the axial coordinate. However, it can be shown that the error around the focus is very large and negative and positive defocus cannot be distinguished given the symmetric dependence in $z$.  Therefore, for 3D SMLM, a cost-effective approach is to introduce astigmatism into the detection path using a weak ($f\approx 10\mathrm{m}$) cylindrical lens. This gives an anisotropic Gaussian point spread function which is elongated perpendicular to the optical axis, depending on the axial ($z$) position of the fluorescent emitter. A fairly simple model for $\sigma_{x}(z_{0})$ and $\sigma_{y}(z_{0})$ upon defocus of a fluorescent molecule might be

\begin{equation*}
\sigma_{x}(z_{0}) = \sigma_{0} + \alpha(z_{0}+z_{min})^{2}\;\;\;\; \sigma_{y}(z_{0}) = \sigma_{0} + \beta(z_{0}-z_{min})^{2}
\end{equation*}

with the following continuous density over the pixel array

\begin{equation}
\mathrm{G}(x,y) = \frac{1}{2\pi\sigma_{x}(z)\sigma_{y}(z)}e^{-\frac{(x-x_{0})^{2}}{2\sigma_{x}(z_{0})^{2}}+\frac{(y-y_{0})^{2}}{2\sigma_{y}(z_{0})^{2}}}
\end{equation}


\subsection{Localization microscopy as frequentist inference}

According to our image formation, molecules really do have an exact location in space. In pratice, this is only an approximation since molecules diffuse at physiological temperatures, and our exposure time would need to tend to zero for this to be exactly true. If we suppose that we can collect a sufficient amount of photons in a short enough time, the physical nature of the system suggests a frequentist inference procedure for localization. A natural choice is maximum likelihood estimation. 

\begin{equation*}
\theta_{\mathrm{MLE}} = \underset{\theta}{\mathrm{argmax}}\prod_{k}P(H_{k}|\theta)= \underset{\theta}{\mathrm{argmin}}-\sum_{k}\log P(H_{k}|\theta)
\end{equation*}

Under the Poisson approximation, the model negative log-likelihood is

\begin{align}
\ell(\vec{H}|\theta) &= -\log \prod_{k} \frac{e^{-\left(\mu_{k}'\right)}\left(\mu_{k}'\right)^{n_{k}}}{n_{k}!}\\
&= \sum_{k}  \log n_{k}! + \mu_{k}' - n_{k}\log\left(\mu_{k}'\right)
\end{align}

When the log-likelihood is easy to compute, as it is under this approximation, we may ask: how much information does the data actually carry about parameters of our model? If the likelihood of the dataset is roughly constant for any parameter set we choose, we might expect our model cannot explain our observations. In other words, the data does not appear to carry much information about the parameters. After all, our posterior is being shaped in part by this likelihood. On the other hand, if $\ell$ has a number of bumps or inflection points, then we expect that maybe some parameter sets make our observed data more likely. The ``bumpiness" of the likelihood surface is called the Fisher information - a fundamental metric in information geometry. 

In frequentist statistics, the Fisher information matrix $I(\theta)$ can be directly related to the curvature of the KL-Divergence over the parameter space

\begin{align*}
\nabla^2_{\theta'} D_{KL}[\ell(H|\theta) \parallel \ell(H|\theta')] 
&= - \nabla_{\theta'} \int \ell(H|\theta) \nabla_{\theta'}  \log \ell(H|\theta') \, dH \\ 
&= - \int \ell(H|\theta) \nabla^2_{\theta'}  \log \ell(H|\theta') \, dH \\
&= - \mathbb{E}_{\theta}[\nabla^2_{\theta'} \log \ell(H|\theta')] \\
&= I(\theta)
\end{align*}


We often call the Hessian matrix the \emph{score}. The Fisher information is the result of averaging the score over the parameter space. 

When calculating localization uncertainties in this setting, we must assume that the likelihood function is well-approximated by a quadratic form around the MLE (i.e., the asymptotic normality of MLE), which may not hold in general. In order to calculate uncertainties in the molecular coordinates, in can be convenient to switch to the Bayesian point of view, where we know have to write the full posterior distribution, specifying a prior.

\begin{align}
P(\theta|\vec{H}) = \frac{P(\vec{H}|\theta)P(\theta)}{\int P(\vec{H}|\theta)P(\theta) d\theta}
\end{align}


\section{Markovian photoswitching dynamics}

The number of molecules within the diffraction limit is $K\left(\frac{\lambda}{2\mathrm{NA}}\right)$. If $\alpha$ is the \emph{detection probability}, then $\alpha K\left(\frac{\lambda}{2\mathrm{NA}}\right)$ are detected, on average. We want to minimize

\begin{equation*}
\mathcal{L} = \alpha K\left(\frac{\lambda}{2\mathrm{NA}}\right) + \gamma\left(\Delta_{\mathrm{SR}}+\frac{2N}{\log(1-\alpha)}\right)^{2}
\end{equation*}

The second term contains $\frac{2N}{\log(1-\alpha)}$, which is the minimum number of frames needed to detect 99 percent of N molecules (which can be obtained from the geometric distribution). If we assume a two-state generator, then

\begin{equation*}
P(t) = P(0)e^{Gt}
\end{equation*}

and $G\pi = 0$ gives $\pi = (\alpha,\beta) = \frac{1}{k}\left(k_{12},k_{21}\right)$ where $k=k_{12}+k_{21}$.


The true photo-switching behavior of the fluorophore is a continuous time stochastic phenomenon. However, an experimenter can only ever observe a discretized manifestation of this
by imaging the fluorophore in a sequence of frames. These frames are regarded as a set of
sequential exposures of the fluorophore and the observed discrete time signal indicates whether
the fluorophore has been observed in a particular frame. It is the continuous time process on
which we wish to draw inference based on the observed discrete-time process indicating whether
the fluorophore was observed in a frame. In this section we first present the continuous time
Markov model of the true (hidden) photo-switching behavior, and then derive the observed
discrete time signal, together with key results on its statistical properties.


We model the photoswitching behavior of a fluorophore as a continuous time Markov process. It is a stochastic process, which satisfies the Markov property

\begin{equation*}
P(X_{t}|X_{t-1}, X_{t-2}, ..., X_{t-N}) = P(X_{t}|X_{t-1})
\end{equation*}

We consider a general model for $X(t)$ that can accommodate the numerous mechanisms of photo-switching utilized in standard SMLM approaches such as (F)PALM and (d)STORM. Specifically, this model consists of a photon emitting (On) state $1$, $m+1$ non photon emitting (dark/temporary off) states $0_0$, $0_1$, $\ldots$, $0_m$, where $m \in \mathbb{Z}{\geq 0}$, and a photobleached (absorbing/permanently off) state $2$. In order to accommodate for the $m = 0$ case when we have a single dark state, we use the notational convention that state $0_0 \equiv 0$. The model, allows for transitions from state $1$ to the multiple dark states (from a photochemical perspective, these can include triplet, redox and quenched states). These dark states are typically accessed via the first dark state $0$ (reached as a result of inter-system crossing of the excited $S1$ electron to the triplet $T1$ state). Further dark states $0_{i+1}$, $i = 0, \ldots, m-1$, are accessible by previous dark states $0_i$ (by, for example, the successive additions of electrons forming radical anions (Van de Linde et al., 2010)). We allow the On state $1$ to be accessible by any dark state and we consider the most general model in which the absorption state $2$ is accessible from any combination of other states (Vogelsang et al., 2010; Van de Linde and Sauer, 2014; Ha and Tinnefeld, 2012).

To capture $P(X_{t}|X_{t-1})$ for all possible pairs $X_{t}$ and $X_{t-1}$, we define a square generator matrix $G\in \mathcal{R}^{N\times N}$ where $N$ denotes the number of states. As such, the elements of $G$ represent the probability of a transition from a state $\omega_{j}$ to $\omega_{i}$ in an infinitesimal time interval

\begin{equation*}
G_{ij} = \mathrm{Pr}\left(X(t+dt)=\omega_{i}, | \;X(t)=\omega_{j}\right)
\end{equation*}

Let the state space for the process $X(t)$ be $\Omega = \{0_{0},0_{1},0_{2},1,2\}$. The generator matrix for such a process reads

\begin{equation*}
G = 
\begin{pmatrix}
\lambda_{00} & \lambda_{0 0_{1}} & 0 & \lambda_{01} & \mu_{0}\\
0 & \lambda_{0_{1}0_{1}} & \lambda_{0_{1}0_{2}} & \lambda_{0_{1}1} & \mu_{1}\\
0 & 0 & \lambda_{0_{2}0_{2}} & \lambda_{0_{2}1} & \mu_{2}\\
\lambda_{10} & 0 & 0 & \lambda_{11} & \mu_{0}\\
0 & 0 & 0 & 0 & 0
\end{pmatrix}
\end{equation*}

Of considerable practical interest is using the matrix $G$ to determine the probability the system is in a particular state at a time $t$. This can be achieved by solving the master equation:

\begin{equation*}
\frac{\partial P(\omega_{i})}{\partial t} = \sum_{j}G_{ji}P(\omega_{j},t) - G_{ij}P(\omega_{i},t)
\end{equation*}

Our notational convention is the $G_{ij}$ is a transition $i\rightarrow j$ while $G_{ji}$ is $j\rightarrow i$. The above equation and its solution is very powerful in this context and in the broader context of non-equilibrium dynamics, so I will devote a short section to its derivation. The reader can safely skip this section if time is scarce.


\subsection{Solving the master equation}

The master equation is a first order differential equation and its solution is straightforward when the dimensionality of the state space is small. The solution is found easily by massaging the master equation into something that has a simple exponential solution. Define a matrix $W$ s.t. $W_{ij} = T_{ij}$ and $W_{ii} = -\sum_{j\neq i}G_{ij}$. This operator acts on $P(\omega)$ and gives a vector of probability currents $\dot{P}(\omega,t) = \mathcal{J}(\bm{\omega}) = W P(\bm{\omega})$. 

\begin{equation*}
P(\bm{\omega}, t) = \exp(W P(\omega))
\end{equation*}

The matrix $W$ for the 4-state system presented before reads

\begin{equation*}
W = 
\begin{pmatrix}
-\sigma_{0} & \lambda_{0 0_{1}} & 0 & \lambda_{01} & \mu_{0}\\
0 & -\sigma_{0_{1}} & \lambda_{0_{1}0_{2}} & \lambda_{0_{1}1} & \mu_{1}\\
0 & 0 & -\sigma_{0_{2}} & \lambda_{0_{2}1} & \mu_{2}\\
\lambda_{10} & 0 & 0 & -\sigma_{1} & \mu_{0}\\
0 & 0 & 0 & 0 & 0
\end{pmatrix}
\end{equation*}

where we have introduced shorthands $\sigma = -\sum_{j\neq i}G_{ij}$.

\subsection{Modeling photoswitching dynamics}

The imaging procedure requires taking a series of successive frames. Frame $n$ is formed by taking an exposure over the time interval $[n\Delta, (n+1)\Delta)$, where $n\in\mathbb{Z}{\geq0}$. The constant $\Delta$ corresponds to the exposure time for a single frame, also known as the frame length. We define the discrete time observed process ${Y_n: n \in \mathbb{Z}{\geq0}}$, with state space $S_Y={0,1}$, as $Y_n=1$ if the fluorophore (characterized by ${X(t)}$) is observed in frame $n$ and equal to $0$ otherwise. For the fluorophore to be observed in the time interval $[n\Delta, (n+1)\Delta)$, it must be in the On state $1$ for a minimum time of $\delta\in[0,\Delta)$. The value of $\delta$ is unknown and is a result of background noise and the imaging system's limited sensitivity. We note that if ${X(t)}$ exhibits multiple jumps to state $1$ within a frame, then a sufficient condition for observing the fluorophore is that the total time spent in the On state exceeds $\delta$. The $\delta=0$ case is the idealistic scenario of a noiseless system and perfect sensitivity such that the fluorophore is detected if it enters the On state for any non-zero amount of time during the exposure time $\Delta$.

\begin{equation*}
Y_{n} = \mathbbm{1}_{[\delta,\Delta)}\left(\int_{n\Delta}^{(n+1)\Delta} \mathbbm{1}_{1} (X(t)) dt\right)
\end{equation*}

Further, the distribution on $\Delta t$ will depend on the hidden state of the system before integration begins.

\begin{equation*}
\Delta t = \int_{n\Delta}^{(n+1)\Delta} \mathbbm{1}_{1} (X(t)) dt \;\;\;\;\;
\Delta t \sim P(\Delta t | X_{t} = i) 
\end{equation*}

where $P(\Delta t | X_{t} = i) $ will be an exponential distribution with parameters dependent on the starting state. Following (Patel 2019), we write down the joint probability of a transition from state $i$ to state $j$ during an interval $\Delta$ and the observation $Y_{n}=\ell$. 

\begin{align*}
b_{ij,\Delta}^{\ell} &= \mathbb{P}(Y_{n}=\ell,X(\Delta)=j|X(0)=i)\\
&= \sum_{k}q_{ij}(k,\Delta)\xi_{ij}(\ell,k,\Delta)
\end{align*}

where we have made the following definitions

\begin{align*}
q_{ij}(k,\Delta) &= \mathbb{P}(N(\Delta)=k,X(\Delta)=j|X(0)=i)\\
\xi_{ij}(\ell,k,\Delta)  &= \mathbb{P}(Y_{n}=\ell|N(\Delta)=k,X(\Delta)=j,X(0)=i)
\end{align*}

We focus first on $q_{ij}(k,\Delta)$, which represents the probability distribution over $k$ transitions of type $ij$ occuring in the interval $\Delta$. This is analagous to considering the time-evolution of the probability mass over states in our solution of the master equation. However, here we instead find the time-evolution of the probability mass over the number of transitions $k$ of a certain type $q_{ij}$, and then integrate out the variable $k$. As an example, the distribution over $k$ for transition of type $q_{i0}$ (from $i\rightarrow 0$), is determined by the distribution over the number of $i\rightarrow 1$ transitions and the rate of a $1\rightarrow 0$ transition over a finite time interval $\Delta t$

\begin{align*}
q_{i0}(k,t+\Delta t) = (1-\sigma_{0}\Delta t)q_{i0}(k,t) + \lambda_{10}q_{i1}(k,t)\Delta t + \mathcal{O}(\Delta t)
\end{align*}

The negative contribution represents the distribution over the number of transitions $i\rightarrow 0$ that have already occured by time $t$ and will leave the $0$ state in the interval $\Delta t$. The full coupled system of differential equations reads


\section{A deep learning framework for super-resolution microscopy}

\section{Bayesian clustering of chromatin nanodomains}

\subsection{A Brief Note on Bayesian Nonparametrics}

In parametric modeling, it is assumed that data can be represented by models using a fixed, finite number
of parameters. Examples of parametric models include clusters of K Gaussians and polynomial regression
models. In many problems, determining the number of parameters a priori is difficult; for example, selecting
the number of clusters in a cluster model, the number of segments in an image segmentation problem, the
number of chains in a hidden Markov model, or the number of topics in a topic modelling problem before
the data is seen can be problematic.

In nonparametric modeling, the number of parameters is not fixed, and often grows with the sample size.
Kernel density estimation is an example of a nonparametric model. In Bayesian nonparametrics, the number
of parameters is itself considered to be a random variable. One example is to do clustering with k-means (or
mixture of Gassuians) while the number of clusters k is unknown. Bayesian inference addresses this problem
by treating k itself as a random variable. A prior is defined over an infinite dimensional model space, and
inference is done to select the number of parameters. Such models have infinite capacity, in that they include
an infinite number of parameters a priori; however, given finite data, only a finite set of these parameters
will be used. Unused parameters will be integrated out.


\subsection{Variational Bayes}

A complete description of the generative model in the Bayesian framework includes the prior distribution $P(\theta)$ which describes the spatial distribution and temporal dynamics of fluorophores, and the likelihood $P(\vec{H}|\theta)$ - a distribution of images generated by the microscope for a given configuration of fluorophores. According to Bayes rule, the posterior on $\theta$ can be constructed as

\begin{align}
P(\theta|\vec{H}) = \frac{P(\vec{H}|\theta)P(\theta)}{\int P(\vec{H}|\theta)P(\theta) d\theta}
\end{align}

The posterior $P(\theta|\vec{H})$ is commonly approximated using Markov Chain Monte Carlo (MCMC) methods. MCMC is asymptotically exact, but can be slow and hyperparameters can be difficult to tune. On the other hand, variational inference (VI) attempts to approximate the true posterior distribution $p(\theta|x)$ with a variational distribution $q(\theta)$, which is often a model distribution which can be easily sampled from e.g., a multivariate Gaussian. One method to fit the variational distribution is to minimize the KL-divergence between the variational distribution and the true posterior:

\begin{align*}
D_{\mathrm{KL}}(q(\theta)||p(\theta|x)) &= \underset{{\theta \sim q(\theta)}}{\mathbb{E}}\left(\log\frac{q(\theta)}{p(\theta|x)}\right)\\
&=  \underset{{\theta \sim q(\theta)}}{\mathbb{E}}\left(\log\frac{q(\theta)p(x)}{p(\theta,x)}\right)\\
&=  \log p(x) + \underset{{\theta \sim q(\theta)}}{\mathbb{E}}\left(\log\frac{q(\theta)}{p(x|\theta)p(\theta)}\right)\\
&= \log p(x) + \underset{{\theta \sim q(\theta)}}{\mathbb{E}}\left(\log q(\theta) - \log p(x|\theta) - \log p(\theta) \right)\\
\end{align*}

We can treat $\log p(x)$ as a constant here, so the KL-divergence is minimized by minimizing this expectation. Equivalently, we can maximize its negative, which is often called the evidence lower bound (ELBO).

\begin{align}
\mathrm{ELBO} = - \underset{{\theta \sim q(\theta)}}{\mathbb{E}}\left(\log q(\theta) - \log p(x|\theta) - \log p(\theta) \right)
\end{align}

Let $\theta = (\theta_{1},...,\theta_{K})$ and $\theta_{k} = (x_{k},y_{k})$ be the pointillist coordinates

\begin{align*}
\nabla_{\phi}\mathrm{ELBO} &= - \nabla_{\phi}\left(\underset{{\theta \sim q_{\phi}(\theta)}}{\mathbb{E}}\left(\log q_{\phi}(\theta) - \log p(x|\theta)\right)\right)\\
&= \nabla_{\phi}\int_{\theta}q_{\phi}(\theta)\log p(x|\theta)d\theta-\nabla_{\phi}\int_{\theta}q_{\phi}(\theta)\log q_{\phi}(\theta)d\theta\\
&= \int_{\theta}\nabla_{\phi}q_{\phi}(\theta)\log p(x|\theta)d\theta-\nabla_{\phi}H(q_{\phi}(\theta))
\end{align*}

where $H(q_{\phi}(\theta))$ is the differential entropy of the variational posterior. If the variational posterior is simple, like a multivariate Gaussian, gradients of the differential entropy can be computed analytically. However, the first term is intractable and often must be estimated using Monte Carlo methods

\begin{equation*}
\int_{\theta}\nabla_{\phi}q_{\phi}(\theta)\log p(x|\theta)d\theta \approx \underset{{\theta \sim q_{\phi}(\theta)}}{\mathbb{E}}\nabla_{\phi}q_{\phi}(\theta)\log p(x|\theta)
\end{equation*}


\chapter{Results}

\chapter{Discussion}

\begin{appendices}

We approximate the log-likelihood function to second order in terms of the Jacobian and Hessian matrices. 

\begin{equation*}
\ell(\theta + \Delta\theta) \approx \ell(\theta) + \nabla\ell^{T}\Delta\theta + \frac{1}{2}\theta^{T}H\theta
\end{equation*}

and 

\begin{equation*}
\nabla\ell(\theta + \Delta\theta) = \nabla\ell(\theta) + H\Delta\theta = 0
\end{equation*}

and so $\Delta\theta = -H^{-1}\nabla\ell(\theta)$. We will make use of both of these in the following sections, so we derive their analytical form now. We will derive the gradients for the integrated astigmatic Gaussian, since it is the more general case. As before, define $i_{0} = g_{k}\gamma\Delta t N_{0}$ such that $\mu_{k}' = i_{0}\lambda_{k}$

\begin{equation*}
J_{x_{0}} = \beta_{k}\lambda_{y}\frac{\partial \lambda_{x}}{\partial x_{0}} \;\; J_{y_{0}} = \beta_{k}\lambda_{x}\frac{\partial \lambda_{y}}{\partial y_{0}}\;\;\; J_{z_{0}}  = \frac{\partial \mu_{k}'}{\partial \sigma_{x}}\frac{\partial \sigma_{x}}{\partial z_{0}} + \frac{\partial \mu_{k}'}{\partial \sigma_{y}}\frac{\partial \sigma_{y}}{\partial z_{0}}
\end{equation*}

\begin{align*}
J_{x_{0}} &= \beta_{k}\lambda_{y}\frac{\partial \lambda_{x}}{\partial x_{0}} \\
&= \frac{\beta_{k}\lambda_{y}}{2}\frac{\partial}{\partial x_{0}}\left(\mathrm{erf}\left(\frac{x_{k}+\frac{1}{2}-x_{0}}{\sqrt{2}\sigma_{x}}\right) -\mathrm{erf}\left(\frac{x_{k}-\frac{1}{2}-x_{0}}{\sqrt{2}\sigma_{x}}\right)\right)\\
&= \frac{\beta_{k}\lambda_{y}}{\sqrt{2\pi}\sigma_{x}}\left(\mathrm{exp}\left(\frac{(x_{k}-\frac{1}{2}-x_{0})^{2}}{2\sigma_{x}^{2}}\right) -\mathrm{exp}\left(\frac{(x_{k}+\frac{1}{2}-x_{0})^{2}}{2\sigma_{x}^{2}}\right)\right)
\end{align*}

\begin{align*}
J_{y_{0}} &= \beta_{k}\lambda_{x}\frac{\partial \lambda_{y}}{\partial y_{0}} \\
&= \frac{\beta_{k}\lambda_{x}}{2}\frac{\partial}{\partial y_{0}}\left(\mathrm{erf}\left(\frac{y_{k}+\frac{1}{2}-y_{0}}{\sqrt{2}\sigma_{y}}\right) -\mathrm{erf}\left(\frac{y_{k}-\frac{1}{2}-y_{0}}{\sqrt{2}\sigma_{y}}\right)\right)\\
&= \frac{\beta_{k}\lambda_{x}}{\sqrt{2\pi}\sigma_{y}}\left(\mathrm{exp}\left(\frac{(y_{k}-\frac{1}{2}-y_{0})^{2}}{2\sigma_{y}^{2}}\right) -\mathrm{exp}\left(\frac{(y_{k}+\frac{1}{2}-y_{0})^{2}}{2\sigma_{y}^{2}}\right)\right)
\end{align*}

\begin{align*}
J_{\sigma_{x}} &= \beta_{k}\lambda_{y}\frac{\partial \lambda_{x}}{\partial \sigma_{x}} \\
&= \frac{\beta_{k}\lambda_{y}}{2}\frac{\partial}{\partial \sigma_{x}}\left(\mathrm{erf}\left(\frac{x_{k}+\frac{1}{2}-x_{0}}{\sqrt{2}\sigma_{x}}\right) -\mathrm{erf}\left(\frac{x_{k}-\frac{1}{2}-x_{0}}{\sqrt{2}\sigma_{x}}\right)\right)\\
&= \frac{\beta_{k}\lambda_{y}}{\sqrt{2\pi}}\left(\frac{\left(x-x_{0}-\frac{1}{2}\right) e^{-\frac{\left(x-x_{0}-\frac{1}{2}\right)^2}{2 \sigma_{x} ^2}}}{\sigma_{x} ^2}-\frac{ \left(x-x_{0}+\frac{1}{2}\right) e^{-\frac{\left(x-x_{0}+\frac{1}{2}\right)^2}{2 \sigma_{x} ^2}}}{\sigma_{x} ^2}\right)
\end{align*}

\begin{align*}
J_{\sigma_{y}} &= \beta_{k}\lambda_{x}\frac{\partial \lambda_{y}}{\partial \sigma_{y}} \\
&= \frac{\beta_{k}\lambda_{x}}{2}\frac{\partial}{\partial \sigma_{y}}\left(\mathrm{erf}\left(\frac{y_{k}+\frac{1}{2}-y_{0}}{\sqrt{2}\sigma_{y}}\right) -\mathrm{erf}\left(\frac{y_{k}-\frac{1}{2}-y_{0}}{\sqrt{2}\sigma_{y}}\right)\right)\\
&= \frac{\beta_{k}\lambda_{x}}{\sqrt{2\pi}}\left(\frac{\left(y-y_{0}-\frac{1}{2}\right) e^{-\frac{\left(y-y_{0}-\frac{1}{2}\right)^2}{2 \sigma_{y} ^2}}}{\sigma_{y} ^2}-\frac{ \left(y-y_{0}+\frac{1}{2}\right) e^{-\frac{\left(y-y_{0}+\frac{1}{2}\right)^2}{2 \sigma_{y} ^2}}}{\sigma_{y} ^2}\right)
\end{align*}

Luckily, computing the Hessian matrix for (2.9) is tractable, and is actually quite simple when one takes advantage of the chain rule for Hessian matrices. Looking at (2.9), the likelihood is a hierarchical function that maps a vector space $\Theta$ to a vector space $\Lambda$ to a scalar value. Formally, we define $T: \Theta \rightarrow \Lambda$ and $W: \Lambda \rightarrow \mathbb{R}$. The parameter vector $(x_{0},y_{0},z_{0}, \sigma_{0}, N_{0})\in \Theta$, the Poisson rate vector $\vec{\lambda} \in \Lambda$ and $\ell \in \mathbb{R}$. Note that we choose to optimize $\sigma_{x}$ and $\sigma_{y}$ directly and compute $z_{0}$ to simplify the computation of the Hessian. To get the Hessian, we need the chain-rule for Hessian matrices, which can be quickly computed in terms of the jacobian and hessian of $T$ and $W$.


\begin{equation*}
H_{\ell} = J_{\mu}^{T} H_{\ell} J_{\mu} + (J_{\ell}\otimes I_{n})H_{\mu}
\end{equation*}

where we have used $J_{\mu}$ to represent the jacobian of $T$ and $J_{\ell}$ for the jacobian of $W$. Similar notation is used for the corresponding Hessian matrices. 
In the 3D case, the Hessian matrix is not directly separable since $\mu \propto \lambda_{x}(x_{0},\sigma_{0},\sigma_{x})\lambda_{y}(y_{0},\sigma_{0},\sigma_{y})$. To see this, an abstract representation of the Hessian reads 

\subsection{A Newton-Raphson method for maximum likelhood estimation}

Newton-Raphson can be advantageous compared to gradient-descent by taking advantage of second-order derivatives when they are available. However, computing second-order derivatives can be cumbersome, and often intractable in a number of machine-learning contexts.  



\subsection{Brief derivation of the master equation}

Each row of the generator matrix $G_{i}$ represents the a conditional probability distribution on the future state given the present state $P(\omega | X(t) = \omega_{j})$:

\begin{equation*}
\sum_{j}G_{ij} = \sum_{j} P(X(t+dt) = \omega_{j} | X(t) = \omega_{i}) = 1
\end{equation*}

The generator is not necessarily symmetric. Also, one should note that the columns $G_{j}$ \emph{do not} define a probability distribution and do not necessarily sum to unity. The columns of $G$ have no meaning in this context, since we have defined the rows to represent a probability of the future given the present. The first order dynamics for a particular state $\omega_{i}$ is given by


\begin{equation*}
P(\omega_{i},t+dt) = P(\omega_{i},t) + \mathcal{J}_{i}dt
\end{equation*}

where the probability current $\mathcal{J}_{i}$ must be the net probability flux into the state $\omega_{i}$

\begin{equation*}
\mathcal{J}_{i} = \sum_{i}G_{ij}P(\omega_{j},t) - \sum_{j}G_{ij}P(\omega_{i},t)\\
\end{equation*}

The first is a sum on a column (in) and the second a sum on a row (out). This can be simplified further by noticing that the normalization condition implies

\begin{equation*}
G_{ij} = 1 - \sum_{j}G_{ij}(1-\delta_{ij}) = 1 - \sum_{j}G_{ij} + \sum_{j}G_{ij}\delta_{ij}
\end{equation*}

Inserting this into the probability current gives

\begin{align*}
\mathcal{J}_{i} &= \sum_{i}G_{ij}P(\omega_{j},t) - \sum_{j}G_{ij}P(\omega_{i},t)\\
&= \sum_{i}\left(1 - \sum_{j}G_{ij} + \sum_{j}G_{ij}\delta_{ij}\right)P(\omega_{j},t) - \sum_{j}G_{ij}P(\omega_{i},t)\\
&= |\Omega| - |\Omega| + \sum_{i}\sum_{j}G_{ij}P(\omega_{j},t)\delta_{ij} - \sum_{j}G_{ij}P(\omega_{i},t)\\
&= \sum_{j}G_{ji}P(\omega_{j},t) - G_{ij}P(\omega_{i},t)
\end{align*}

which is RHS of the master equation. Notice that the Kronecker delta effectively just swaps the index. 

\subsection{Fisher information for 2D integrated gaussian}

For the 2D integrated gaussian point spread function, the Hessian only contains separable second order derivatives, so the Fisher information matrix takes on a convenient form

\begin{equation}
I_{ij}(\theta) = \underset{\theta}{\mathbb{E}}\left(\frac{\partial \ell}{\partial\theta_{i}}\frac{\partial\ell}{\partial\theta_{j}}\right) 
\end{equation}

For an arbitrary parameter then we have

\begin{align*}
\frac{\partial \ell}{\partial \theta_{i}} &= \frac{\partial}{\partial \theta_{i}} \sum_{k}  x_{k}\log x_{k} + \mu_{k}' - x_{k}\log\left(\mu_{k}'\right)\\
&= \sum_{k} \frac{\partial \mu_{k}'}{\partial\theta_{i}} \left(\frac{\mu_{k}'-x_{k}}{\mu_{k}'}\right)
\end{align*}

\begin{equation*}
I_{ij}(\theta) = \underset{\theta}{\mathbb{E}}\left(\sum_{k}\frac{\partial \mu_{k}'}{\partial\theta_{i}}\frac{\partial \mu_{k}'}{\partial\theta_{j}} \left(\frac{\mu_{k}'-x_{k}}{\mu_{k}'}\right)^{2}\right) = \sum_{k}\frac{1}{\mu_{k}'}\frac{\partial \mu_{k}'}{\partial\theta_{i}}\frac{\partial \mu_{k}'}{\partial\theta_{j}}
\end{equation*}

To compute the bound, it turns out all we need is the jacobian $\frac{\partial \mu_{k}'}{\partial\theta_{j}} $.

\section{The Fokker-Planck Equation}

\subsection{Kramers-Moyal Expansion}

Given many instantiations of a stochastic variable $x$, we can construct a normalize histogram oxer all observations as a function of time $P(x,t)$. However, in order to systematically explore the relationship between the parameterization of the process and $P(x,t)$ we require an expression for $\dot{P}(x,t)$. If we make a fundamental assumption that the evolution of $P(x,t)$ follows a Markov process i.e. its evolution has the memoryless property, then we can write

\begin{equation}
P(x', t) = \int T(x', t | x, t-\tau)P(x, t-\tau)dx
\end{equation} 

which is known at the Chapman-Kolmogorov equation. The factor $T(x', t | x, t-\tau)$ is known as the \emph{transition operator} in a Markov process and determines the evolution of $P(x,t)$ in time. We proceed by writing $T(x', t | x, t-\tau)$ in a form referred to as the Kramers-Moyal expansion

\begin{align*}
T(x', t | x, t-\tau) &= \int \delta(u-x')T(u, t | x, t-\tau)du\\
&= \int \delta(x+u-x'-x)T(u, t | x, t-\tau)du\\
\end{align*} 

If we use the Taylor expansion of the $\delta$-function 

\begin{equation*}
\delta(x+u-x'-x) = \sum_{n=0}^{\infty} \frac{(u-x)^{n}}{n!}\left(-\frac{\partial}{\partial x}\right)^{n}\delta(x-x')
\end{equation*}

Inserting this into the result from above, pulling out terms independent of $u$ and swapping the order of the sum and integration gixes

\begin{align}
T(x', t | x, t-\tau) &= \sum_{n=0}^{\infty} \frac{1}{n!}\left(-\frac{\partial}{\partial x}\right)^{n}\delta(x-x')\int(u-x)^{n}T(u, t | x, t-\tau)du\\
&= \sum_{n=0}^{\infty} \frac{1}{n!}\left(-\frac{\partial}{\partial x}\right)^{n}\delta(x-x')M_{n}(x,t)
\end{align} 

noticing that $M_{n}(x,t) = \int(u-x)^{n}T(u, t | x, t-\tau)du$ is just the $n$th moment of the transition operator $T$. Plugging (2.6) back in to (2.4) gixes 

\begin{align}
P(x, t) &= \int \left(1 + \sum_{n=1}^{\infty} \frac{1}{n!}\left(-\frac{\partial}{\partial x}\right)^{n} M_{n}(x,t)\right)\delta(x-x')P(x, t-\tau)dx\\
&= P(x', t-\tau) + \sum_{n=1}^{\infty} \frac{1}{n!}\left(-\frac{\partial}{\partial x}\right)^{n} \left[M_{n}(x,t)P(x,t)\right]
\end{align} 

Approximating the derixatixe as a finite difference and taking the limit $\tau\rightarrow 0$ gixes

\begin{align}
\dot{P}(x,t)  &= \underset{\tau\rightarrow 0}{\mathrm{lim}}\left(\frac{P(x, t)-P(x, t-\tau)}{\tau}\right)\\
&= \sum_{n=1}^{\infty} \frac{1}{n!}\left(-\frac{\partial}{\partial x}\right)^{n} \left[M_{n}(x,t)P(x,t)\right]
\end{align} 

which is formally known as the Kramers-Moyal (KM) expansion. The Fokker-Planck equation is a special case of (2.10) where we neglect terms $n>2$ in the \emph{diffusion approximation}.


Consider the following Ito stochastic differential equation 

\begin{align*}
d\vec{x} = F(\vec{x},t) + G(\vec{x},t)dW
\end{align*}

The SDE gixen aboxe corresponds to the Kramers-Moyal expansion (KME) of a transition density $T(x',t'|x,t)$ see (Risken 1989) for a full derixation.

\begin{align}
\frac{\partial P(x,t)}{\partial t}  &= \sum_{n=1}^{\infty} \frac{1}{n!}\left(-\frac{\partial}{\partial x}\right)^{n} \left[M_{n}(x,t)P(x,t)\right]
\end{align}

where $M_{n}$ is the $n$th moment of the transition density. In the diffusion approximation, the KME becomes the Fokker-Planck equation (FPE) (Risken 1989). For the sake of demonstration, consider the univariate case with random variable $x$ and the form of $T(x',t'|x,t)$ is a Gaussian with mean $\mu(t)$ and variance $\sigma^{2}(t)$. In this scenario, the FPE applies because $M_{n} = 0$ for all $n > 2$. Given that the drift $M_{1}(x,t) = \mu(t)$ and the diffusion $M_{2}(x,t) = \sigma^{2}(t)$, the FPE reads

\begin{align}
\frac{\partial P(x,t)}{\partial t}  &= \left(-\frac{\partial}{\partial x}M^{(1)}(t) + \frac{1}{2}\frac{\partial^{2}}{\partial x^{2}}M^{(2)}(t)\right)P(x,t)
\end{align}

We can additionally define the term in parentheses as a differential operator acting on $P(x,t)$

\begin{align}
\hat{\mathcal{L}}_{FP} = \left(-\frac{\partial}{\partial x}M^{(1)}(t) + \frac{1}{2}\frac{\partial^{2}}{\partial x^{2}}M^{(2)}(t)\right)
\end{align}

It is common to additionally define the probability current $J(x,t)$ as 

\begin{align}
J(x,t)  &= \left(M^{(1)}(t) - \frac{1}{2}\frac{\partial}{\partial x}M^{(2)}(t)\right)P(x,t)
\end{align}

This definition provides some useful intuition. The value of $J(x,t)$ is the net probability flux into the interval between $x$ and $x+dx$ at at time $t$. This also allows us to write the FPE as a continuity equation

\begin{align}
\frac{\partial P(x,t)}{\partial t} = -\frac{\partial J(x,t)}{\partial x}
\end{align}

\subsection{Solving the FPE: Heat (Diffusion) Equation}

The well-known heat equation (it has several names: diffusion equation, heat equation, Brownian motion, Wiener process) is a special case of the FPE where $M^{(1)}(t) = 0$ and $M^{(2)}(t) = \sigma^{2} = \mathrm{const}$. 

\begin{align}
\frac{\partial P(x,t)}{\partial t}  &= D\frac{\partial^{2}P(x,t)}{\partial x^{2}}
\end{align}

with $D = \sigma^{2}/2$. We would like to solve the above equation, but it is a PDE which usually require some tricks to solve e.g., integral transforms. Generally a transform can reduce a differential equation to a simpler form, like an ODE. Upon Fourier tranformation, spatial derivatives turn into factors of $ik$. That is, 

\begin{align*}
\frac{\partial}{\partial x} \psi(x,t) \rightarrow ik\tilde{\psi}(k,t)\;\;\;\;\;\; \frac{\partial^{2}}{\partial x^{2}} \psi(x,t) \rightarrow -k^{2}\tilde{\psi}(k,t)
\end{align*}


\subsubsection{Fourier Transform of the Heat Equation}

Recall the general definition of a Fourier pair

\begin{align*}
\tilde{\psi}(k) &= \mathcal{F}[\psi] = \int_{-\infty}^{\infty} \psi(x) e^{-2\pi ikx}dx\\
\psi(x) &= \mathcal{F}^{-1}[\tilde{\psi}] = \int_{-\infty}^{\infty} \tilde{\psi}(k) e^{2\pi ikx}dk
\end{align*}


Let's see the Fourier transformation of Eq. (6)

\begin{align}
\frac{\partial}{\partial t} \int_{-\infty}^{\infty}  P(x,t) e^{-2\pi ikx}dx = D\int_{-\infty}^{\infty} \frac{\partial^{2}P(x,t)}{\partial x^{2}} e^{-2\pi ikx}dx
\end{align}

As mentioned above, $\mathcal{F}[\partial_{x}\psi] = ik \mathcal{F}[\psi]$ and $\mathcal{F}[\partial_{x}^{2}\psi] = -k^{2} \mathcal{F}[\psi]$ which allows us to write the heat equation as a first order equation

\begin{align}
\frac{\partial \tilde{P}(k,t)}{\partial t}  &= -Dk^{2}\tilde{P}(k,t)
\end{align}

which suggests the solution $\tilde{p}_{0}(k) \exp\left(-D k^{2}t\right)$, which is Gaussian in $k$-space. Let's say our initial condition satisfies $\tilde{P}(x,t_{0}) = \delta (x-x_{0})$ which in the Fourier domain is $P(k,t_{0}) = \exp (-ikx_{0})$. The inverse transform is

\begin{align}
\int_{-\infty}^{\infty} \tilde{p}_{0}(k) \exp\left(ikx-D k^{2}t\right)dk &= \int_{-\infty}^{\infty} \exp\left(ik(x-x_{0})-D k^{2}t\right)dk
\end{align}

which we can rewrite as

\begin{align*}
\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} \exp\left(-\left(D k^{2}t - ik(x-x_{0}\right)\right)dk
= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} \exp\left(-Dt\left(k^{2} - \frac{ik(x-x_{0})}{Dt}\right)\right)dk
\end{align*}

Now we would like to complete the square in the exponential, since we know how to do Gaussian integrals. This can be done as follows:

\begin{align*}
\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} \exp\left(-Dt\left(k^{2} - \frac{ik(x-x_{0})}{Dt} + \frac{(x-x_{0})^{2}}{4D^{2}t^{2}} - \frac{(x-x_{0})^{2}}{4D^{2}t^{2}}\right)\right)dk
\end{align*}

We are then left to simplify,

\begin{align*}
\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} \exp\left(-Dt\left(k-\frac{i(x-x_{0})}{2Dt}\right)^{2}\right)dk &= \frac{1}{\sqrt{2\pi}}\exp\left(- \frac{(x-x_{0})^{2}}{4Dt}\right)\int_{-\infty}^{\infty} \exp\left(-Dtk'^{2}\right)dk'\\
&= \frac{1}{\sqrt{2Dt}}\exp\left(- \frac{(x-x_{0})^{2}}{4Dt}\right)
\end{align*}

which is a Gaussian distribution will time-dependent variance $\sigma=4Dt$, given originally by Einstein in his famous paper on Brownian motion in 1905. 

\subsection{Solving the FPE: Ornstein-Uhlenbeck}

The Ornstein-Uhlenbeck process is another special case of the FPE where $M^{(1)}(t) = -\gamma$ and $M^{(2)}(t) = \sigma^{2} = \mathrm{const}$. It is a stationary Gaussâ€“Markov process, which means that it is a Gaussian process, a Markov process, and is temporally homogeneous. The Ito SDE for this process reads

\begin{align}
dx = -\gamma xdt + \sigma dW
\end{align}

which of course has a corresponding Fokker-Planck equation

\begin{align}
\frac{\partial P(x,t)}{\partial t} = -\gamma\frac{\partial}{\partial x} x P(x,t) + D\frac{\partial^{2}P(x,t)}{\partial x^{2}}
\end{align}

In this form, the solution is slightly complicated by the presence of the first order spatial derivative. However, we can still find a solution via a Fourier transform:

\begin{align}
\frac{\partial \tilde{P}(k,t)}{\partial t}  =  -\gamma k \frac{\partial \tilde{P}(k,t)}{\partial k} -k^{2}D \tilde{P}(k,t)
\end{align}

Notice that this is a partial differential equation with the general form 

\begin{align}
a(\tilde{P},k,t)\partial_{k}\tilde{P} + b(\tilde{P},k,t)\partial_{t}\tilde{P} - c(\tilde{P},k,t) = 0
\end{align}

Therefore can solve the above equation using the method of characteristics. As a brief review, suppose we know a solution surface $\tilde{P}$. A vector normal to this surface has the form $\vec{u} = \langle \partial_{k}\tilde{P}, \partial_{t}\tilde{P}, -1 \rangle$. If this vector is normal to the surface, then the vector field 

\begin{align}
\vec{v} = \langle a(\tilde{P},k,t),b(\tilde{P},k,t),c(\tilde{P},k,t) \rangle
\end{align}

is tangent to the surface at every point. In other words, we would like to find a surface $\tilde{P}(k,t)$ for which the vector field above lies in the tangent plane to $\tilde{P}(k,t)$ and therefore $\vec{u}\cdot \vec{v} = 0$. The task that remains then is to find a $\tilde{P}(k,t)$ s.t. the vector $\vec{u}$ is orthogonal to $\vec{v}$. Now, if we construct a curve $\mathcal{C}$ which is an integral curve of $\vec{v}$, then this curve lies on the solution surface $\tilde{P}(k,t)$. Such a curve satisfies the ODEs

\begin{align*}
\frac{dk}{ds} &= \gamma k\\
\frac{dt}{ds} &= 1\\
\frac{d\tilde{P}}{ds} &= -k^{2}D\tilde{P}
\end{align*}

since the vector field given by the Fokker-Planck equation we have is $\vec{v} = \langle \gamma k, 1, -k^{2}D\rangle$. Clearly $t=s$ and $k = k_{0}\exp(\gamma t)$ and thus

\begin{align}
\frac{d\tilde{P}}{dt} &= -k^{2}D\tilde{P}\\
&= -Dk_{0}^{2}\exp(2\gamma t)\tilde{P}
\end{align}

and we have the solution in the Fourier domain

\begin{align}
\tilde{P}(k,t) &= \tilde{P}(k,0)\exp\left(-\frac{Dk_{0}^{2}}{2\gamma}(\exp(2\gamma t)-1)\right)\\
&= \exp\left(-ik_{0} x_{0} -\frac{Dk_{0}^{2}}{2\gamma}(\exp(2\gamma t)-1)\right)\\
&= \exp\left(-i k e^{-\gamma t} x_{0} -\frac{Dk^{2}}{2\gamma}(1-\exp(-2\gamma t))\right)
\end{align}

Let $\mu(t) = x_{0}\exp(-\gamma t)$ and $\sigma^{2}(t) = \frac{D}{\gamma}(1-e^{-2\gamma t})$

\begin{align}
\tilde{P}(k,t) &= \exp\left(-i k \mu(t) -\frac{k^{2}}{2}\sigma^{2}(t)\right)
\end{align}

Taking the inverse Fourier transform of this equation gives 


\begin{align}
P(x,t) &= \frac{1}{\sqrt{2\sigma^{2}(t)}}\exp\left(-\frac{(x-\mu(t))^{2}}{2\sigma^{2}(t)}\right)
\end{align}

\subsection{The Multivariate Case}

If we now generalize the above equation to a case where we are faced with many variables $\bm{x} = (x_{1},x_{2},...,x_{n})$. The continuity equation becomes 

\begin{align}
\frac{\partial P(\vec{x},t)}{\partial t} = -\vec{\nabla} \cdot J(\vec{x},t)
\end{align}

where the multivariate probability current now has the interpretation of the net flux into or out of a volume $dx^{n}$ centered around $\bm{x}$. If we consider each dimension, 

\begin{align}
J(x_{i},t)  &= \left(M_{i}^{(1)}(t) - \sum_{j}\frac{\partial}{\partial x_{j}}M_{ij}^{(2)}(t) \right)P(\vec{x},t)
\end{align}

The full Fokker-Planck equation then reads

\begin{align}
\frac{\partial P(\vec{x},t)}{\partial t}  &= \vec{\nabla} \cdot J(\vec{x},t)\\
&= \sum_{i=1}^{N}\left(-\frac{\partial}{\partial x_{i}}M_{i}^{(1)}(t) + \sum_{j=1}^{N} \frac{\partial^{2}}{\partial x_{i}\partial x_{j}}M_{ij}^{(2)}(t)\right)P(\vec{x},t)
\end{align}

It proves quite useful in this form because we can see that the Fokker-Planck equation represents a differentiation operator acting on the distribution $P(\vec{x},t)$

\begin{align}
\hat{\mathcal{L}}_{FP} = \sum_{i=1}^{N}\left(-\frac{\partial}{\partial x_{i}}M_{i}^{(1)}(t) + \sum_{j=1}^{N} \frac{\partial^{2}}{\partial x_{i}\partial x_{j}}M_{ij}^{(2)}(t)\right)
\end{align}

\subsection{Ornstein-Uhlenbeck Process}

If the transition density is Gaussian then the density is fully specified by the first two moments $M^{(1)}(t) = \vec{\mu}(t)$ and $M^{(2)}(\vec{x},t) = \Sigma(t)$. The moments can also be functions of $\vec{x}$. Both of these possibilities are evident in the Ornstein-Uhlenbeck (OU) process. Let the drift vector be a linear function of the state $\vec{x}$ and the diffusion matrix the square of the Gaussian covariances

\begin{align*}
M^{(1)}(t) = \Gamma \vec{x}\;\;\;\;\;M^{(2)}(t) = 2D
\end{align*}

with $D = \Sigma\Sigma^{T}$ and $\Gamma$ are assumed to be independent of time (non-volatile).


The Fourier transform of (34) is fairly simple, since the FT is linear. We will switch to matrix notation to drop the summations

\begin{align}
\frac{\partial \tilde{P}(\vec{x},t)}{\partial t}= -\bm{\Gamma}\bm{k}\frac{\partial}{\partial \bm{k}} \tilde{P}(\vec{k},t) + \bm{\mathrm{D}}\bm{k}\bm{k}^{T}\tilde{P}(\vec{k},t)
\end{align}

This is in a very similar form to the single variable case (18). We then make the ansatz in analogy with (27)

\begin{align}
\tilde{P}(\bm{k},t) &= \exp\left(-i \bm{k}^{T} \bm{\mu}(t) -\frac{\bm{k}\bm{k}^{T}}{2}\bm{\Sigma}(t)\right)
\end{align}

Plugging this into (34) we have

\begin{align}
\frac{\partial \tilde{P}(\vec{x},t)}{\partial t}+ \bm{\Gamma}\bm{k}\frac{\partial}{\partial \bm{k}} \tilde{P}(\vec{k},t) + \bm{\mathrm{D}}\bm{k}\bm{k}^{T}\tilde{P}(\vec{k},t)\\
= -i\bm{k}\dot{\bm\mu}(t) - \frac{1}{2}\bm{k}\bm{k}^{T}\dot{\bm\Sigma}-i\bm\Gamma \bm\mu^{T} - \bm\Gamma
\end{align}

\end{appendices}





\end{document}


