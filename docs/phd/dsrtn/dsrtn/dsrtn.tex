% ---- ETD Document Class and Useful Packages ---- %
\documentclass{ucetd}
\usepackage{subfigure,epsfig,amsfonts}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[toc,page]{appendix}
\usepackage[labelfont=bf]{caption}
\usepackage{rotating}
\usepackage[dvipsnames]{xcolor}
\usepackage{url}
\usepackage{bm}

%% Use these commands to set biographic information for the title page:
\title{Phase transitions of DNA-protein condensates during the immune response}
\author{Clayton W. Seitz}
\department{Department of Physics}
\division{Physical Sciences}
\degree{Doctor of Philosophy}
\date{Spring 20XX}

%% Use these commands to set a dedication and epigraph text

\epigraph{Epigraph}



\begin{document}
%% Basic setup commands
% If you don't want a title page comment out the next line and uncomment the line after it:
\maketitle
%\omittitle

% These lines can be commented out to disable the copyright/dedication/epigraph pages
\makecopyright
%\makededication


%% Make the various tables of contents
\tableofcontents
%\listoffigures
%\listoftables

%\acknowledgments
% Enter Acknowledgements here

\abstract

Eukaryotic transcription is episodic, consisting of a series of transcriptional bursts, Bursty transcriptional dynamics are well-exemplified by the transient expression of pro-inflammatory guanylate binding proteins (GBPs) - a group interferon-inducible GTPases that restrict the replication of intracellular pathogens [XXX]. Classical models of gene regulation explain transcriptional bursts by invoking stochastic binding and unbinding of transcription factors, RNA polymerase and mediator proteins at enhancer or promoter sequences. However, more recent studies have pointed towards a more cooperative picture of transcriptional control where phase-separated aggregates of DNA, RNA, and proteins form higher-order structures to control gene expression. For example, both chromatin immunoprecipitation and super resolution imaging have captured the phase separation of super-enhancer-binding proteins MED1 and BRD4 in transcriptional condensates at the \textit{Essrb} genomic locus [XXX]. Furthermore, fluorescence microscopy techniques have colocalized MED1 and BRD4 with the GBP gene cluster alongside a reduction in the degree of disorder of 3D chromatin structure in murine macrophages after infection with \textit{Mycobacterium tuberculosis}. Taken together, these results suggest that phase separation may play a role in the reorganization of chromatin structure during trancriptional control of innate immune response genes [XXX]. Here, we hypothesize that phase separation reduces the entropy of chromatin structure in order to induce bursty gene expression. Using single molecule localization microscopy (SMLM) to obtain super-resolution images of the H2B protein, we intend to demonstrate simultaneous (i) loss of disorder in chromatin structure (ii) formation of transcriptional condensates containing MED1 and BRD4 and (iii) non-Poissonian gene expression. The following sections discuss recent the biological evidence in more detail and summarize the single molecule microscopy techniques and biophysical models we employ to study the interactions between transcriptional condensates and the chromatin scaffold.

\clearpage

\mainmatter

\chapter{Single molecule localization microscopy}

\section{Gaussian point spread functions in single molecule localization microscopy}

Most detectors used for imaging have many elements (pixels) so that we can record an image projected onto the detector by a system of lenses. In fluorescence imaging, this is usually a relay consisting of an objective lens and a tube lens to focus the image onto the camera. Due to diffraction, any point emitter, such as a single fluorescent molecule, will be registered as a diffraction limited spot. The profile of that spot is often described as a Gaussian point spread function

\begin{equation}
\mathrm{G}(x,y) = \frac{1}{2\pi\sigma^{2}}e^{-\frac{(x-x_{0})^{2}+(y-y_{0})^{2}}{2\sigma^{2}}} + B_0
\end{equation}


Modern cameras used in light microscopy, such as scientific complementary metal oxide semiconductor (sCMOS) cameras, are powered by the photoelectric effect. Electrons within each pixel, called photoelectrons, absorb enough energy from incoming photons to be promoted to the conduction band to give electrical current which can be detected. Integration of photoelectrons during the exposure time results in a monochrome image captured by a camera. The image of a single point particle, such as a fluorescent molecule, can be thought of as two-dimensional histogram of photon arrivals and a discretized form of the classical intensity profile $\mathrm{G}(x,y)$. The value at a pixel approaches an integral of this density over the pixel:

\begin{equation}
\mu_{k} = I_{0}\lambda_{k} + B_{0} = I_{0}\int_{\mathrm{pixel}} G(x,y)dxdy + B_{0}
\end{equation}

where $I_{0} = \eta N_{0}\Delta t$. The parameter $\eta$ is the quantum efficiency and $\Delta t$ is the exposure time. $N_{0}$ represents the number of photons emitted per unit time, which may be itself a Poisson random variable; however, this is inconsequential since it is a fixed value for a single image of a fluorescent molecule. The value of the integral $\lambda_{k}$ at a pixel $k$ defines the fraction of $I_{0}$ photons observed at that pixel during the camera exposure. Since the 2D Gaussian in (1.1) has cylindrical symmetry, it is separable, and we can write

\begin{align*}
\lambda_{k} &= \frac{1}{2\pi\sigma^{2}}\left(\int_{x_{k}}^{x_{k+1}}e^{-\frac{(x-x_{0})^{2}}{2\sigma^{2}}}dx\right)\left(\int_{y_{k}}^{y_{k+1}}e^{-\frac{(y-y_{0})^{2}}{2\sigma^{2}}}dy\right)
\end{align*}


We can then express the Gaussian integrals over a pixel by making use of the following property of the error function

\begin{equation*}
\frac{1}{\sqrt{2\pi}\sigma}\int_{a}^{b} e^{\frac{-(x-\mu)^{2}}{2\sigma^{2}}} = \frac{1}{2}\left(\mathrm{erf}\left(\frac{b-\mu}{\sqrt{2}\sigma}\right) -\mathrm{erf}\left(\frac{a-\mu}{\sqrt{2}\sigma}\right)\right)
\end{equation*}

Now, suppose the particle is known to be located at $(x_{0},y_{0})$ and some square pixel $k$ is centered on coordinates $(x,y)$ and has a width $a$. Then $\lambda_{k}$ at this pixel is

\begin{equation}
\lambda_{k}(x,y) = \lambda_{x}(x)\lambda_{y}(y)
\end{equation}

where

\begin{align*}
\lambda_{x}(x) &= \frac{1}{2}\left(\mathrm{erf}\left(\frac{x+a/2-x_{0}}{\sqrt{2}\sigma}\right) -\mathrm{erf}\left(\frac{x-a/2-x_{0}}{\sqrt{2}\sigma}\right)\right)\\
\lambda_{y}(y) &= \frac{1}{2}\left(\mathrm{erf}\left(\frac{y+a/2-y_{0}}{\sqrt{2}\sigma}\right) -\mathrm{erf}\left(\frac{y-y/2-y_{0}}{\sqrt{2}\sigma}\right)\right)
\end{align*}

We are now prepared to write the shot-noise limited signal, which is a vector with units of phototelectrons

\begin{equation}
\vec{S} = \left[\mathrm{Poisson}(\lambda_{1}), \mathrm{Poisson}(\lambda_{2}), ..., \mathrm{Poisson}(\lambda_{N})\right]
\end{equation}


However this noise model is incomplete, because detectors often suffer from dark noise, which may refer to readout noise or dark current, and contributes to a nonzero signal even in the absence of incident light. Dark current is due to statistical fluctuations in the photoelectron count due to thermal fluctuations. Readout noise is introduced by the amplifier circuit during the coversion of photoelectron charge to a voltage. Here, we use the Hamamatsu ORCA v3 CMOS camera, which is air cooled to -10C and has very low dark current - around 0.06 electrons/pixel/second - and can therefore be safely ignored for exposure times on the order of milliseconds. Readout noise has been often neglected in localization algorithms because its presence in EMCCD cameras is small enough that it can be ignored within the tolerances of the localization precision. In the case of sCMOS cameras, however, the readout noise of each pixel is significantly higher and, in addition, every pixel has its own noise and gain characteristic sometimes with dramatic pixel-to-pixel variations.

On the other hand, readout noise is not negligible, and must represented as a vector-valued random variable. It is important to note that we cannot measure the contribution by readout noise before amplification and therefore it must be expressed in units of $\mathrm{ADU}$. This is in contrast to $\vec{S}$, which can be expressed in units of photoelectrons, because these statistical fluctuations can be predicted to be Poisson by quantum mechanics. The number of photoelectrons $S_{k}$ is  multiplied by a gain factor $g_{k}$ which has units of $[\mathrm{ADU}/e^{-}]$, which generally must be measured for each pixel. Here, we will always assume that readout noise per pixel $\xi_{k}$ is Gaussian with some pixel-specific offset $o_{k}$ and variance $\sigma_{k}^{2}$. 

We will now define a vector which represents the image we measure, in units of ADU: 

\begin{equation}
\vec{H} = \vec{S} + \vec{\xi}
\end{equation}

What we are after is the joint distribution $P(\vec{H})$. A fundamental result in probability theory is that the distribution of $H_{k}$ is the convolution of the distributions of $S_{k}$ and $\xi_{k}$,

\begin{align*}
P(H_{k}|\theta) &= P(S_{k})\circledast P(xi_{k})\\
&= A\sum_{q=0}^{\infty} \frac{1}{q!}e^{-\mu_{k}}\mu_{k}^{q}\frac{1}{\sqrt{2\pi}\sigma_{k}}e^{-\frac{(H_{k}-g_{k}q-o_{k})}{2\sigma_{k}^{2}}}
\end{align*}

In practice, this expression is difficult to work with, so we look for an analytical approximation. In the following analysis we use a Poisson approximation of (1.6) - insert Komogonov distance argument from nanoscopy paper. In the Poisson approximation, the model log-likelihood is surprisingly simple

\begin{align*}
\ell(\vec{H}) &= \log \prod_{k} \frac{e^{-\mu_{k}}\mu_{k}^{n}}{n!}\\
&= \sum_{k} n_{k}\log\mu_{k} - \log n_{k}! - \mu_{k}\\
&= \sum_{k} n_{k}\log\left(I_{0}\lambda_{k} + B_{0}\right) - \log n_{k}! - I_{0}\lambda_{k} - B_{0}
\end{align*}

Note that

\begin{align*}
\frac{\partial\ell}{\partial\lambda_{k}} &= \frac{n_{k}I_{0}}{I_{0}\lambda_{k} + B_{0}} - I_{0}\\
\frac{\partial^{2}\ell}{\partial\lambda_{k}^{2}} &= -\frac{n_{k}I_{0}^2}{(I_{0}\lambda_{k} + B_{0})^2}
\end{align*}

\subsection{The Cramer-Rao lower bound}

A general task in Bayesian inference is to determine $\theta$ from the data under the model $\mathcal{M}_{\theta}$. We may then ask - does the log-likelihood $\ell$ vary as we vary the parameters? If the likelihood is flat, all parameter sets are equally likely and the data does not appear to carry much information about the parameters. Moreover, if $\ell$ has a number of bumps or inflection points, then we expect that maybe some parameter sets are more likely that others. The ``bumpiness" of the likelihood surface is called the Fisher information - a fundmantal concept in information geometry. The Fisher information matrix $I(\theta)$ can be directly related to the curvature of the KL-Divergence over the parameter space

\begin{align*}
\nabla^2_{\theta'} D_{KL}[\ell(H|\theta) \parallel \ell(H|\theta')] 
&= - \nabla_{\theta'} \int \ell(H|\theta) \nabla_{\theta'}  \log \ell(H|\theta') \, dH \\ 
&= - \int \ell(H|\theta) \nabla^2_{\theta'}  \log \ell(H|\theta') \, dH \\
&= - \mathbb{E}_{\theta}[\nabla^2_{\theta'} \log \ell(H|\theta')] \\
&= I(\theta)
\end{align*}


We often call the Hessian matrix the \emph{score}. The Fisher information is the result of averaging the score over the parameter space. To be clear, the score is a function of the \emph{data}, not the parameters. It is a measure of sensitivity of the likelihood to changes in the parameters. Of course, the Fisher information matrix also depends on the parameterization chosen. Looking at (1.4), the likelihood is a hierarchical function that maps a vector space $\Theta$ to a vector space $\Lambda$ to a scalar value. Formally, we define $T: \Theta \rightarrow \Lambda$ and $W: \Lambda \rightarrow \mathbb{R}$. The parameter vector $(x_{0},y_{0},\sigma, N_{0})\in \Theta$, the Poisson rate vector $\vec{\lambda} \in \Lambda$ and $\ell \in \mathbb{R}$. To get the Hessian, we need the chain-rule for Hessian matrices.


\begin{align*}
\hat{H}_{(\ell,\theta)} = \hat{J}_{(\lambda,\theta)}^{T} \hat{H}_{(\ell,\lambda)} \hat{J}_{(\lambda,\theta)} + (J_{(\ell,\lambda)}\otimes I_{n})\hat{H}_{(\lambda,\theta)}
\end{align*}

In the second term of the equation, we have a Kronecker product between the Jacobian matrix of the likelihood with respect to the parameters of the hierarchical model ($J_{(L,\lambda)}$) and the $n\times n$ identity matrix ($I_n$), denoted as $J_{(L,\lambda)}\otimes I_n$. This Kronecker product gives a diagonal matrix where the elements along the diagonal are the elements of the vector $J_{(L,\lambda)}$. This provides a concise way of summarizing the operation:

\begin{align*}
((J_{(L,\lambda)}\otimes I_{n})\hat{H}_{(\lambda,\theta)})_{ij} = \sum_{k}\hat{J}_{(L,\lambda)}^{ij}\hat{H}_{(\lambda,\theta)}^{ijk}
\end{align*}

Note that we sum over the last index to get a 3 x 3 matrix, which is not directly obvious in the compact notation.


\section{Photoswitching dynamics of JF646}

The central assumption underlying a Markov process, is the memoryless property

\begin{equation*}
P(X_{t}|X_{t-1}, X_{t-2}, ..., X_{t-N}) = P(X_{t}|X_{t-1})
\end{equation*}

A single Markov chain is the set of states $\bm{X} = \{X_{1},X_{2},...,X_{N}\}$. Such a set can be generated provided that $P(X_{t}|X_{t-1})$ is known. To capture $P(X_{t}|X_{t-1})$ for all possible pairs $X_{t}$ and $X_{t-1}$, we define a square transition matrix $T\in \mathcal{R}^{N\times N}$ where $N = |\Omega|$. As such, the elements of $T$ represent the probability of a transition from a state $\omega_{j}$ to $\omega_{i}$ in a unit time

\begin{equation*}
T_{ij} = \mathrm{Pr}\left(X_{t}=\omega_{i}, | \;X_{t-1}=\omega_{j}\right)
\end{equation*}

Under these definitions, the row $T_{i}$ represents the present time, and is a conditional  probability distribution $P(\omega | X_{t-1} = \omega_{j})$ which requires that

\begin{equation*}
\sum_{j}T_{ij} = \sum_{j} P(X_{t} = \omega_{j} | X_{t-1} = \omega_{i}) = 1
\end{equation*}

The matrix $T$ is not necessarily symmetric $T_{ij} \neq T_{ji}$. One should note that the columns $T_{j}$ \emph{do not} define a probability distribution $P(X_{t} = \omega_{i} | X_{t-1} = \omega_{j})$ and therefore do not necessarily sum to unity. The probability $P(X_{t} = \omega_{i} | X_{t-1} = \omega_{j})$ has no meaning in this context, since we have defined the rows to represent a probability of the future given the present. We simply sample $X_{t} \sim P(X_{t} = \omega_{j} | X_{t-1} = \omega_{i})$, assign $i=j$, and repeat. It directly follows from the fundamental rules of probability, the first order dynamics for a \textbf{particular} state $\omega_{i}$: $P(\omega_{i},t)$ is given by

\begin{equation}
P(\omega_{i},t+dt) = P(\omega_{i},t) + \mathcal{J}_{i}dt
\end{equation}

The net probability current $\mathcal{J}_{i}$ must be 

\begin{equation*}
\mathcal{J}_{i} = \sum_{i}T_{ij}P(\omega_{j},t) - \sum_{j}T_{ij}P(\omega_{i},t)\\
\end{equation*}

The first is a sum on a column and the second a sum on a row. This can be simplified further by noticing that the normalization condition implies

\begin{align*}
T_{ij} &= 1 - \sum_{j}T_{ij}(1-\delta_{ij})\\
&= 1 - \sum_{j}T_{ij} + \sum_{j}T_{ij}\delta_{ij}
\end{align*}


\begin{align*}
\mathcal{J}_{i} &= \sum_{i}T_{ij}P(\omega_{j},t) - \sum_{j}T_{ij}P(\omega_{i},t)\\
&= \sum_{i}\left(1 - \sum_{j}T_{ij} + \sum_{j}T_{ij}\delta_{ij}\right)P(\omega_{j},t) - \sum_{j}T_{ij}P(\omega_{i},t)\\
&= |\Omega| - |\Omega| + \sum_{i}\sum_{j}T_{ij}P(\omega_{j},t)\delta_{ij} - \sum_{j}T_{ij}P(\omega_{i},t)\\
&= \sum_{j}T_{ji}P(\omega_{j},t) - T_{ij}P(\omega_{i},t)\\
\end{align*}

Notice that the Kronecker delta effectively just swaps the index. Taking the limit of (1.1), we arrive at the \textbf{master equation}


\begin{equation*}
\frac{\partial P(\omega_{i})}{\partial t} = \sum_{j}T_{ji}P(\omega_{j},t) - T_{ij}P(\omega_{i},t)
\end{equation*}

It is common to then define an operator $\bm{W}$ s.t. $W_{ij} = T_{ij}$ and $W_{ii} = -\sum_{j}T_{ij}$ 

\begin{align*}
\frac{dP(\omega_{i})}{dt} = \sum_{j}W_{ij}P(\omega_{j}) \rightarrow \frac{dP(\bm{\omega})}{dt} = \mathcal{J}(\bm{\omega}) = \mathbf{W}P(\bm{\omega})
\end{align*}

This operator form has a solution in terms of a matrix exponential

\begin{equation*}
P(\bm{\omega}, t) = \exp(\mathcal{J}(\bm{\omega}))
\end{equation*}

This matrix exponential is intractable for large $|\Omega|$. However, in the Finite State Projection algorithm, it is possible to truncate the state space $\Omega \rightarrow \tilde{\Omega}$ and obtain good estimates $\tilde{P}(\bm{\omega}, t)$ with some certificate of accuracy.

\subsection{Lifetime of the ON and OFF states}

Alongside the solution to the master equation $P_{\mathrm{ON}}(t)$ and $P_{\mathrm{OFF}}(t)$, we are often interested in lifetime of the ON and OFF states and how these lifetimes change according to experimental parameters. In fact, the distribution over the ON and OFF state lifetimes gives a convenient frequentist mechanism for estimating the rate constants of a photoswitching model. To see this, consider the two-state model where $\alpha_{12}(t) = k_{12}(t)dt$ and $\alpha_{21}(t) = k_{21}(t)dt$ are the propensity of transitions from on to off and off to on, respectively. Suppose the system begins in the ON state, 

\begin{align*}
P(\mathrm{ON} \geq t \; | \;\mathrm{ON}) &= \underset{n\rightarrow\infty}{\mathrm{lim}}\left(1-k_{12}dt\right)^{n}\\
&= \underset{n\rightarrow\infty}{\mathrm{lim}}\left(1-\frac{k_{12}t}{n}\right)^{n}\\
&= e^{-k_{12}t}
\end{align*}

This is the cumulative density function for the lifetime of the ON state. Notice that we are given that the system is in the ON state at $t=0$, and all we want to know is the lifetime of that state, so we need not consider the transition rates back to the ON state: $k_{j1}$. The probability density of the lifetime of the ON state is: 

\begin{align*}
f_{\mathrm{ON}}(t) = -\frac{dP(\mathrm{ON} \geq t)}{dt} = k_{12}e^{-k_{12}t}
\end{align*}

Therefore one possible way of inferring the rate constant $k_{12}$ is by fitting the observed lifetime distribution with an exponential distribution with $k_{12}$ as a free parameter. However, for the OFF state, it has been previously shown that a single rate constant may not fit the data accurately and the introduction of multiple rate constants representing multiple dark states are necessary. Common photoswitching models include three OFF states: the triplet state, dark state, and long-lived dark state and rate constants between these two states.

\begin{align*}
P(\mathrm{OFF} \geq t\; | \; \mathrm{OFF}) = P_{\mathrm{T}}(t) + P_{\mathrm{D}}(t) + P_{\mathrm{LLD}}(t)
\end{align*}

where we have the system of equations for transitions within the OFF state

\begin{align*}
\frac{dP_{\mathrm{T}}(t)}{dt} &= -k_{21}P_{\mathrm{T}}(t) - k_{23}P_{\mathrm{T}}(t)\\
\frac{dP_{\mathrm{D}}(t)}{dt} &= P_{\mathrm{T}}(t)k_{23}-k_{31}P_{\mathrm{D}}(t) - k_{34}P_{\mathrm{D}}(t)\\
\frac{dP_{\mathrm{LLD}}(t)}{dt} &= k_{34}P_{\mathrm{D}}(t) - k_{41}P_{\mathrm{LLD}}(t)
\end{align*}

This system can be solved analytically by Laplace transformation. Similar to the ON state, 

\begin{align*}
f_{\mathrm{OFF}}(t) = -\frac{dP(\mathrm{OFF} \geq t)}{dt} = \sum_{i}a_{i}\lambda_{i}e^{-\lambda_{i}t}
\end{align*}



\chapter{Inferring DNA structure from super-resolution microscopy images}



\begin{appendices}


\subsection{ODE model for the ensemble average}

We can define the following system of ODEs for a autorepressive gene circuit

\begin{align}
\frac{\mathrm{d}m}{\mathrm{d}t} &= \frac{\beta_{m}}{1 + (p/k)^n} - \gamma_m m,\\[1em]
\frac{\mathrm{d}r}{\mathrm{d}t} &= \beta_{r} m - \gamma_r r,\\[1em]
\frac{\mathrm{d}p}{\mathrm{d}t} &= \beta_{p} r - \gamma_{p} p,
\end{align}

We can greatly reduce the number of parameters by nondimensionalizing the system. To that end, we define a characteristic time scale $t_{0}$ and characteristic "length" scales for each variable: $m_{0}, r_{0}, p_{0}$. The choice of these characteristic scale will become apparent after writing the nondimensionalized ODEs out for a general case. The derivatives transform as

\begin{equation*}
\frac{dm}{dt} \rightarrow \frac{m_{0}}{t_{0}}\frac{dm'}{d\tau}, \;\;\; \frac{dr}{dt} \rightarrow \frac{r_{0}}{t_{0}}\frac{dr'}{d\tau},  \;\;\; \frac{dp}{dt} \rightarrow \frac{p_{0}}{t_{0}}\frac{dp'}{d\tau} 
\end{equation*}

Our general system of nondimensionalized ODEs reads:


\begin{align*}
\frac{\mathrm{d}m'}{\mathrm{d}\tau} &= \frac{t_0\beta_{m}}{m_0(1 + (p_{0}p'/k)^n)} - \gamma_m t_{0}m'\\
\frac{\mathrm{d}r'}{\mathrm{d}\tau} &= \frac{\beta_{r} t_{0}m_{0} m'}{r_0} - \gamma_r t_{0}r'\\
\frac{\mathrm{d}p'}{\mathrm{d}\tau} &= \frac{\beta_{p} t_{0}r_{0} r'}{p_{0}} - \gamma_{p} t_{0} p'
\end{align*}

Define the characteristic scales as: $t_{0} = 1/\gamma_{m}$, $m_{0}=\gamma_{m}^{2}k/\beta_{r}\beta_{p}$, $r_{0} = \gamma_{m}k/\beta_{p}$, and $p_{0} = k$. Making these substitutions, we have

\begin{align*}
\frac{\mathrm{d}m'}{\mathrm{d}\tau} &= \frac{\beta}{(1 + (p')^n)} - m'\\
\frac{\mathrm{d}r'}{\mathrm{d}\tau} &= m' - \frac{\gamma_{r}}{\gamma_{m}}r'\\
\frac{\mathrm{d}p'}{\mathrm{d}\tau} &= r' - \gamma p'
\end{align*}

\subsection{Telegraph model of gene expression}

We will begin by writing the the system of ODEs describing the dynamics of the first moment

\begin{align}
\frac{\mathrm{d}m}{\mathrm{d}t} &= \beta_{m}(1-m) - \gamma_m m\\
\frac{\mathrm{d}r}{\mathrm{d}t} &= \beta_{r} m - \gamma_r r\\
\end{align}

This system has 4 parameters, making it difficult to directly visualize interesting relationships between parameterization and dynamics. Therefore, we will nondimensionalize this system

\begin{align}
\frac{\mathrm{d}m'}{\mathrm{d}t} &= \frac{t_0}{m_0}\beta_{m} - \frac{t_0}{m_0}\beta_{m}m_{0}m' - \frac{t_0}{m_0}\gamma_m m_{0}m'\\
\frac{\mathrm{d}r'}{\mathrm{d}t} &= \frac{t_0}{m_0}\beta_{r} m_{0}m' - \frac{t_0}{m_0}\gamma_r r_{0}r'\\
\end{align}

Let $t_{0} = 1/\gamma_{m}$, $m_{0} = \beta_{m}/\gamma_{m}$, $r_{0} = \beta_{m}\beta_{r}/\gamma_{m}^{2}$, $\gamma = \gamma_{r}/\gamma_{m}$, $\beta = \beta_{m}/\gamma_{m}$, and we have

\begin{align}
\frac{\mathrm{d}m'}{\mathrm{d}t} &= 1 - \beta m' - m'\\
\frac{\mathrm{d}r'}{\mathrm{d}t} &= m' - \gamma r'\\
\end{align}


\subsection{Variational Bayes}

Variational inference attempts to approximate the true posterior distribution $p(\theta|x)$ with a variational distribution $q(\theta)$, which we assume to be Gaussian. We try to minimize the KL-divergence between the variational distribution and the true posterior:

\begin{align*}
D_{\mathrm{KL}}(q(\theta)||p(\theta|x)) &= \underset{{\theta \sim q(\theta)}}{\mathbb{E}}\left(\log\frac{q(\theta)}{p(\theta|x)}\right)\\
&=  \underset{{\theta \sim q(\theta)}}{\mathbb{E}}\left(\log\frac{q(\theta)p(x)}{p(\theta,x)}\right)\\
&=  \log p(x) + \underset{{\theta \sim q(\theta)}}{\mathbb{E}}\left(\log\frac{q(\theta)}{p(x|\theta)p(\theta)}\right)\\
&= \log p(x) + \underset{{\theta \sim q(\theta)}}{\mathbb{E}}\left(\log q(\theta) - \log p(x|\theta) - \log p(\theta) \right)\\
&= \log p(x) + H(q) -\underset{{\theta \sim q(\theta)}}{\mathbb{E}}\left(\log p(x|\theta) + \log p(\theta) \right)
\end{align*}

Clearly the KL-divergence is minimized by minimizing this expectation. It is common to define the evdience lower bound (ELBO).

\begin{align*}
\ell(x,\theta) = - \underset{{\theta \sim q(\theta)}}{\mathbb{E}}\left(\log q(\theta) - \log p(x|\theta) - \log p(\theta) \right)
\end{align*}

It is name such because

\begin{equation*}
\log p(x) = D_{\mathrm{KL}}(q(\theta)||p(\theta|x)) + \ell(x,\theta) \geq \ell(x,\theta)
\end{equation*}

The ELBO can be minimized when the variational distribution is easy to sample from, for example a multivariate normal distribution and when the likelihood is tractable. The gradients of the ELBO

\begin{align*}
\nabla_{\Phi}\ell(x,\theta) = - \underset{{\theta \sim q(\theta)}}{\mathbb{E}}\left(\log q(\theta) - \log p(x|\theta) - \log p(\theta) \right)
\end{align*}


\subsection{Kramers-Moyal Expansion}

Gixen many instantiations of a stochastic xariable $x$, we can construct a normalize histogram oxer all obserxations as a function of time $P(x,t)$. Howexer, in order to systematically explore the relationship between the parameterization of the process and $P(x,t)$ we require an expression for $\dot{P}(x,t)$. If we make a fundamental assumption that the exolution of $P(x,t)$ follows a Markox process i.e. its exolution has the memoryless property, then we can write

\begin{equation}
P(x', t) = \int T(x', t | x, t-\tau)P(x, t-\tau)dx
\end{equation} 

which is known at the Chapman-Kolmogorox equation. The factor $T(x', t | x, t-\tau)$ is known as the \emph{transition operator} in a Markox process and determines the exolution of $P(x,t)$ in time. We proceed by writing $T(x', t | x, t-\tau)$ in a form referred to as the Kramers-Moyal expansion

\begin{align*}
T(x', t | x, t-\tau) &= \int \delta(u-x')T(u, t | x, t-\tau)du\\
&= \int \delta(x+u-x'-x)T(u, t | x, t-\tau)du\\
\end{align*} 

If we use the Taylor expansion of the $\delta$-function 

\begin{equation*}
\delta(x+u-x'-x) = \sum_{n=0}^{\infty} \frac{(u-x)^{n}}{n!}\left(-\frac{\partial}{\partial x}\right)^{n}\delta(x-x')
\end{equation*}

Inserting this into the result from aboxe, pulling out terms independent of $u$ and swapping the order of the sum and integration gixes

\begin{align}
T(x', t | x, t-\tau) &= \sum_{n=0}^{\infty} \frac{1}{n!}\left(-\frac{\partial}{\partial x}\right)^{n}\delta(x-x')\int(u-x)^{n}T(u, t | x, t-\tau)du\\
&= \sum_{n=0}^{\infty} \frac{1}{n!}\left(-\frac{\partial}{\partial x}\right)^{n}\delta(x-x')M_{n}(x,t)
\end{align} 

noticing that $M_{n}(x,t) = \int(u-x)^{n}T(u, t | x, t-\tau)du$ is just the $n$th moment of the transition operator $T$. Plugging (2.6) back in to (2.4) gixes 

\begin{align}
P(x, t) &= \int \left(1 + \sum_{n=1}^{\infty} \frac{1}{n!}\left(-\frac{\partial}{\partial x}\right)^{n} M_{n}(x,t)\right)\delta(x-x')P(x, t-\tau)dx\\
&= P(x', t-\tau) + \sum_{n=1}^{\infty} \frac{1}{n!}\left(-\frac{\partial}{\partial x}\right)^{n} \left[M_{n}(x,t)P(x,t)\right]
\end{align} 

Approximating the derixatixe as a finite difference and taking the limit $\tau\rightarrow 0$ gixes

\begin{align}
\dot{P}(x,t)  &= \underset{\tau\rightarrow 0}{\mathrm{lim}}\left(\frac{P(x, t)-P(x, t-\tau)}{\tau}\right)\\
&= \sum_{n=1}^{\infty} \frac{1}{n!}\left(-\frac{\partial}{\partial x}\right)^{n} \left[M_{n}(x,t)P(x,t)\right]
\end{align} 

which is formally known as the Kramers-Moyal (KM) expansion. The Fokker-Planck equation is a special case of (2.10) where we neglect terms $n>2$ in the \emph{diffusion approximation}.


Consider the following Ito stochastic differential equation 

\begin{align*}
d\vec{x} = F(\vec{x},t) + G(\vec{x},t)dW
\end{align*}

The SDE gixen aboxe corresponds to the Kramers-Moyal expansion (KME) of a transition density $T(x',t'|x,t)$ see (Risken 1989) for a full derixation.

\begin{align}
\frac{\partial P(x,t)}{\partial t}  &= \sum_{n=1}^{\infty} \frac{1}{n!}\left(-\frac{\partial}{\partial x}\right)^{n} \left[M_{n}(x,t)P(x,t)\right]
\end{align}

where $M_{n}$ is the $n$th moment of the transition density. In the diffusion approximation, the KME becomes the Fokker-Planck equation (FPE) (Risken 1989). For the sake of demonstration, consider the univariate case with random variable $x$ and the form of $T(x',t'|x,t)$ is a Gaussian with mean $\mu(t)$ and variance $\sigma^{2}(t)$. In this scenario, the FPE applies because $M_{n} = 0$ for all $n > 2$. Given that the drift $M_{1}(x,t) = \mu(t)$ and the diffusion $M_{2}(x,t) = \sigma^{2}(t)$, the FPE reads

\begin{align}
\frac{\partial P(x,t)}{\partial t}  &= \left(-\frac{\partial}{\partial x}M^{(1)}(t) + \frac{1}{2}\frac{\partial^{2}}{\partial x^{2}}M^{(2)}(t)\right)P(x,t)
\end{align}

We can additionally define the term in parentheses as a differential operator acting on $P(x,t)$

\begin{align}
\hat{\ell}_{FP} = \left(-\frac{\partial}{\partial x}M^{(1)}(t) + \frac{1}{2}\frac{\partial^{2}}{\partial x^{2}}M^{(2)}(t)\right)
\end{align}

It is common to additionally define the probability current $J(x,t)$ as 

\begin{align}
J(x,t)  &= \left(M^{(1)}(t) - \frac{1}{2}\frac{\partial}{\partial x}M^{(2)}(t)\right)P(x,t)
\end{align}

This definition provides some useful intuition. The value of $J(x,t)$ is the net probability flux into the interval between $x$ and $x+dx$ at at time $t$. This also allows us to write the FPE as a continuity equation

\begin{align}
\frac{\partial P(x,t)}{\partial t} = -\frac{\partial J(x,t)}{\partial x}
\end{align}
\end{appendices}



\section{Poisson processes}

The Poisson process can be derived very quickly by noticing that it is simply the continuous-time limit of the Binomial distribution.

\begin{align*}
B(m;t) &= {n\choose m}\lambda^{m}(1-\lambda)^{n-m}\\
\end{align*}

Since $\lambda$ is the fraction of successes, the expected number of successes is $\mu = n\lambda$

\begin{align*}
B(m;t) &=  {n\choose m}\left(\frac{\mu}{n}\right)^{m}\left(1-\frac{\mu}{n}\right)^{n-m}\\
&= {n\choose m}\left(\frac{\mu}{n}\right)^{m}\left(1-\frac{\mu}{n}\right)^{n}\left(1-\frac{\mu}{n}\right)^{-m}
\end{align*}

\begin{align*}
B(m;t) &= \frac{n!}{m!(n-m)!}\left(\frac{\mu}{n}\right)^{m}\left(1-\frac{\mu}{n}\right)^{n}\left(1-\frac{\mu}{n}\right)^{-m}\\
&= \frac{n!}{(n-m)!}\left(\frac{1}{n}\right)^{m}\left(1-\frac{\mu}{n}\right)^{-m}\frac{\mu^{m}\left(1-\frac{\mu}{n}\right)^{n}}{m!}\\
\end{align*}

In the first term, we can take the first $m$ subterms of the numerator $n! = n(n-1)...(n-m)$ and, since $n>>m$, each term will cancel with one factor of $n$ from the term $1/n^{m}$. This leaves

\begin{align*}
B(m;t) &= \frac{(n-m)!}{(n-m)!}\left(1-\frac{\mu}{n}\right)^{-m}\frac{\mu^{m}\exp(-\mu)}{m!}\\\\
\end{align*}

We now take the continuous time limit i.e. $n\rightarrow\infty$ and, again, since $m << n$ we are left with 

\begin{align*}
\underset{n\rightarrow\infty}{\mathrm{lim}} \;\; B(m;t) &= \frac{\mu^{m}\exp(-\mu)}{m!}\\
\end{align*}

If an event can be detected will probability $\gamma$, the rate of the Poisson process will be reduced by that factor i.e., $\lambda' = \gamma\lambda$. Therefore, the mean and variance of the process becomes $\mu = \gamma\lambda\Delta t$

\end{document}


