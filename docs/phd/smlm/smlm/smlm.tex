% Latex template: mahmoud.s.fahmy@students.kasralainy.edu.eg
% For more details: https://www.sharelatex.com/learn/Beamer

%\documentclass{beamer}					% Document class
% Aspect ratio
\documentclass[aspectratio=169]{beamer}

\setbeamertemplate{footline}[text line]{%
  \parbox{\linewidth}{\vspace*{-8pt}Single molecule localization microscopy \hfill\insertshortauthor\hfill\insertpagenumber}}
\setbeamertemplate{navigation symbols}{}

\usepackage[english]{babel}				% Set language
\usepackage[utf8x]{inputenc}			% Set encoding

\mode<presentation>						% Set options
{
  \usetheme{default}					% Set theme
  \usecolortheme{default} 				% Set colors
  \usefonttheme{default}  				% Set font theme
  \setbeamertemplate{caption}[numbered]	% Set caption to be numbered
}

% Uncomment this to have the outline at the beginning of each section highlighted.
%\AtBeginSection[]
%{
%  \begin{frame}{Outline}
%    \tableofcontents[currentsection]
%  \end{frame}
\usepackage{graphicx}					% For including figures
\usepackage{booktabs}					% For table rules
\usepackage{hyperref}	
\usepackage{tikz-network}				% For cross-referencing
\usepackage[absolute,overlay]{textpos}
\usepackage{bm}
\usepackage[font=small,labelfont=bf]{caption}				% For cross-referencing

\title{Single molecule localization microscopy}	% Presentation title
\author{Clayton W. Seitz}								% Presentation author
\date{\today}									% Today's date	

\begin{document}

% Title page
% This page includes the informations defined earlier including title, author/s, affiliation/s and the date
\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Photoswitching as a Markov jump process}
\begin{equation*}
G_{ij} = \mathrm{Pr}\left(X(t+dt)=\omega_{i}, | \;X(t)=\omega_{j}\right)
\end{equation*}

Let the state space for the process $X(t)$ be $\Omega = \{0_{0},0_{1},0_{2},1,2\}$. The generator matrix for such a process reads

\begin{equation*}
G = 
\begin{pmatrix}
\lambda_{00} & \lambda_{0 0_{1}} & 0 & \lambda_{01} & \mu_{0}\\
0 & \lambda_{0_{1}0_{1}} & \lambda_{0_{1}0_{2}} & \lambda_{0_{1}1} & \mu_{1}\\
0 & 0 & \lambda_{0_{2}0_{2}} & \lambda_{0_{2}1} & \mu_{2}\\
\lambda_{10} & 0 & 0 & \lambda_{11} & \mu_{0}\\
0 & 0 & 0 & 0 & 0
\end{pmatrix}
\end{equation*}
\end{frame}

\begin{frame}{Photoswitching as a Markov jump process}
\begin{equation*}
\frac{\partial P(\omega_{i})}{\partial t} = \sum_{j}G_{ji}P(\omega_{j},t) - G_{ij}P(\omega_{i},t)
\end{equation*}
\begin{equation*}
P(\bm{\omega}, t) = \exp(W P(\omega))
\end{equation*}

The matrix $W$ for the 4-state system presented before reads

\begin{equation*}
W = 
\begin{pmatrix}
-\sigma_{0} & \lambda_{0 0_{1}} & 0 & \lambda_{01} & \mu_{0}\\
0 & -\sigma_{0_{1}} & \lambda_{0_{1}0_{2}} & \lambda_{0_{1}1} & \mu_{1}\\
0 & 0 & -\sigma_{0_{2}} & \lambda_{0_{2}1} & \mu_{2}\\
\lambda_{10} & 0 & 0 & -\sigma_{1} & \mu_{0}\\
0 & 0 & 0 & 0 & 0
\end{pmatrix}
\end{equation*}

\end{frame}


\begin{frame}{Integrated isotropic gaussian point spread function}

Let $G(x,y)$ be a normalized isotropic Gaussian density over the pixel array
\begin{equation}
\mathrm{G}(x,y) = \frac{1}{2\pi\sigma^{2}}e^{-\frac{(x-x_{0})^{2}+(y-y_{0})^{2}}{2\sigma^{2}}}
\end{equation}
Let's presume (correctly) that pixel gray levels are a Poisson r.v. with expected value
\begin{equation}
\mu_{k} = \eta\Delta t(N_{0}\lambda_{k} + B_{0}) 
\end{equation}
where $\Delta t$ is the camera exposure time and $N_{0}$ and $B_{0}$ are the fluorophore and background emission rates respectively. Eq (1) and (2) are related by
\begin{equation*}
\lambda_{k} = \int_{\mathrm{pixel}}G(x,y)dxdy
\end{equation*}

\end{frame}

\begin{frame}{Integrated anisotropic gaussian point spread function (astigmatism)}
Let $G(x,y)$ be a normalized anisotropic Gaussian density over the pixel array

\begin{equation}
\mathrm{G}(x,y) = \frac{1}{2\pi\sigma_{x}(z)\sigma_{y}(z)}e^{-\frac{(x-x_{0})^{2}}{2\sigma_{x}(z)^{2}}+\frac{(y-y_{0})^{2}}{2\sigma_{y}(z)^{2}}}
\end{equation}

A fairly simple model for $\sigma_{x}(z)$ and $\sigma_{y}(z)$ is

\begin{align*}
\sigma_{x}(z) &= \sigma_{0} + \alpha(z+z_{min})^{2}\\
\sigma_{y}(z) &= \sigma_{0} + \beta(z-z_{min})^{2}
\end{align*}

\end{frame}

\begin{frame}{How to compute $\lambda_{k}$ at each pixel}

We can replace this integral with error functions:

\begin{align*}
\lambda_{x}(x) &= \frac{1}{2}\left(\mathrm{erf}\left(\frac{x+a/2-x_{0}}{\sqrt{2}\sigma}\right) -\mathrm{erf}\left(\frac{x-a/2-x_{0}}{\sqrt{2}\sigma}\right)\right)\\
\lambda_{y}(y) &= \frac{1}{2}\left(\mathrm{erf}\left(\frac{y+a/2-y_{0}}{\sqrt{2}\sigma}\right) -\mathrm{erf}\left(\frac{y-y/2-y_{0}}{\sqrt{2}\sigma}\right)\right)
\end{align*}
For multiple emitters
\begin{equation*}
\lambda(x,y) = \sum_{n}\lambda_{n,x}(x)\lambda_{n,y}(y)
\end{equation*}
The \emph{true signal} is then
\begin{equation*}
\vec{S} = \left[\mathrm{Poisson}(\lambda_{1}), \mathrm{Poisson}(\lambda_{2}), ..., \mathrm{Poisson}(\lambda_{N})\right]
\end{equation*}

\end{frame}

\begin{frame}{Poisson approximation of pixel values}

However, due to readout noise, we measure

\begin{equation*}
\vec{H} = \vec{S} + \vec{\xi}
\end{equation*}
The distribution of $H_{k}$ is the convolution:
\begin{align*}
P(H_{k}|\theta) &= P(S_{k})\circledast P(\xi_{k})\\
&= A\sum_{q=0}^{\infty} \frac{1}{q!}e^{-\mu_{k}}\mu_{k}^{q}\frac{1}{\sqrt{2\pi}\sigma_{k}}e^{-\frac{(H_{k}-g_{k}q-o_{k})}{2\sigma_{k}^{2}}}
\end{align*}
where $P(\xi_{k}) = \mathcal{N}(o_{k},\sigma_{k}^{2})$ and $P(S_{k}) = \mathrm{Poisson}(g_{k}\mu_{k})$. In practice, this expression is difficult to work with, so we look for an approximation. Notice that 

\begin{align*}
\xi_{k} - o_{k} + \sigma_{k}^{2} \sim \mathcal{N}(\sigma_{k}^{2},\sigma_{k}^{2}) \approx \mathrm{Poisson}(\sigma_{k}^{2})
\end{align*}

\end{frame}

\begin{frame}{The model log likelihood and Hessian matrix}
Since $H_{k} = S_{k} + \xi_{k}$, we transform $H_{k}' = H_{k} - o_{k} + \sigma_{k}^{2}$, which is distributed according to 

\begin{align*}
H_{k}' \sim \mathrm{Poisson}(\mu_{k}')\;\;\;\mu_{k}' = g_{k}\mu_{k} + \sigma_{k}^{2}
\end{align*}
Since each Poisson r.v. is independent, the negative log likelihood reads
\begin{align*}
\ell(\vec{H}) &= -\log \prod_{k} \frac{e^{-\left(\mu_{k}'\right)}\left(\mu_{k}'\right)^{n_{k}}}{n_{k}!}\\
&= \sum_{k}  \log n_{k}! + \mu_{k}' - n_{k}\log\left(\mu_{k}'\right)\\
&\approx \sum_{k}  n_{k}\log n_{k} + \mu_{k}' - n_{k}\log\left(\mu_{k}'\right)
\end{align*}

\end{frame}

\begin{frame}{Localization with maximum likelihood estimation (MLE)}
A slew of gradient calculations
\end{frame}

\begin{frame}{Bayesian localization with stochastic gradient langevin dynamics}
$$dw = - \nabla L(w) dt + \epsilon \sqrt{\eta dt}, \quad \epsilon \sim \mathcal N(0, \sigma^2), \eta \propto dt$$
Our goal is to sample from the stationary distribution of the above SDE. To do that, we will use Stochastic Gradient Langevin Dynamics (SGLD), an algorithm commonly used to sample from the parameter's posterior -- this is unlike MLE, whose goal is to simply minimize the objective $L$, which can be seen as finding the modes of the parameter's posterior
\end{frame}



\end{document}