% Latex template: mahmoud.s.fahmy@students.kasralainy.edu.eg
% For more details: https://www.sharelatex.com/learn/Beamer

\documentclass[aspectratio=1610]{beamer}					% Document class

\setbeamertemplate{footline}[text line]{%
  \parbox{\linewidth}{\vspace*{-8pt}Bayesian image reconstruction\hfill\insertshortauthor\hfill\insertpagenumber}}
\setbeamertemplate{navigation symbols}{}

\usepackage[english]{babel}				% Set language
\usepackage[utf8x]{inputenc}			% Set encoding

\mode<presentation>						% Set options
{
  \usetheme{default}					% Set theme
  \usecolortheme{default} 				% Set colors
  \usefonttheme{default}  				% Set font theme
  \setbeamertemplate{caption}[numbered]	% Set caption to be numbered
}

% Uncomment this to have the outline at the beginning of each section highlighted.
%\AtBeginSection[]
%{
%  \begin{frame}{Outline}
%    \tableofcontents[currentsection]
%  \end{frame}
%}

\usepackage{graphicx}					% For including figures
\usepackage{booktabs}					% For table rules
\usepackage{hyperref}					% For cross-referencing

\title{Bayesian image reconstruction}	% Presentation title
\author{Clayton W. Seitz}								% Presentation author
\date{\today}									% Today's date	

\begin{document}

% Title page
% This page includes the informations defined earlier including title, author/s, affiliation/s and the date
\begin{frame}
  \titlepage
\end{frame}

% Outline
% This page includes the outline (Table of content) of the presentation. All sections and subsections will appear in the outline by default.
\begin{frame}{Outline}
  \tableofcontents
\end{frame}

% The following is the most frequently used slide types in beamer
% The slide structure is as follows:
%
%\begin{frame}{<slide-title>}
%	<content>
%\end{frame}


\begin{frame}{Photon statistics of CMOS cameras}
\begin{itemize}
\item Imaging noise consists of shot noise, thermal noise, and readout noise
\vspace{0.1in}
\item Shot noise is Poisson, thermal noise and readout noise are Gaussian
\end{itemize}
\vspace{0.2in}
For a CMOS pixel $n$, the true signal $S_{n}\; [\mathrm{ADU}]$ is a Poisson process with rate parameter $\lambda_{n}$

\begin{equation*}
S_{n} = \gamma g_{n}P_{n}(\lambda_{n})
\end{equation*}

where $\gamma\;\;[e^{-}/p]$ is the quantum efficiency and $g_{n}\;\; [\mathrm{ADU}/e^{-}]$ is the pixel's gain
\vspace{0.1in}

\begin{equation*}
P(S_{n}) = \frac{\exp\left({-\lambda_{n}}\right)\lambda_{n}^{p}}{p!}
\end{equation*}

\textcolor{blue}{But what is the distribution over the corrupted signal $P(\hat{S}_{n})$?}


\end{frame}


\begin{frame}{Photon statistics of CMOS cameras}

To find $P(\hat{S}_{n})$, we first evaluate the joint density $P(S_{n},\hat{S}_{n})$
\vspace{0.1in}
\begin{align*}
P(S_{n},\hat{S}_{n}) &= P(\hat{S}_{n}|S_{n}=s)P(S_{n}=s)\\
&= \frac{1}{Z}\exp\left(-\frac{(\hat{S}_{n}-g_{n}s-\mu_{n})^{2}}{\sigma_{n}^{2}}\right)\frac{\exp\left({-\lambda_{n}}\right)\lambda_{n}^{s}}{s!}
\end{align*}
\vspace{0.1in}

Marginalizing over $S_{n}$ gives the desired distribution over $\hat{S}_{n}$

\begin{equation*}
P(\hat{S}_{n}) = \frac{1}{Z}\sum_{s=0}^{\infty}\frac{\exp\left({-\lambda_{n}}\right)\lambda_{n}^{s}}{s!}\exp\left(-\frac{(\hat{S}_{n}-g_{n}s-\mu_{n})^{2}}{\sigma_{n}^{2}}\right)
\end{equation*}

\end{frame}

\begin{frame}{Bayesian parameter inference for CMOS photon statistics}

The parameters in our model $\theta = (\lambda_{n},g_{n},\mu_{n},\sigma^{2}_{n})$ are unknown apriori


\begin{equation*}
P(\theta|\hat{S}_{n}) \propto P(\hat{S}_{n}|\theta)P(\theta)
\end{equation*}

\vspace{0.2in}
We can just computed the likelihood $P(\hat{S}_{n}|\theta)$ on the last slide. Samples from the posterior can be found for example by MCMC or we could use MAP estimation\\
\vspace{0.2in}
Either of these approaches only make sense for stationary statistics, which means the physical locations and photophysics of the sample remain unchanged in time\\
\vspace{0.2in}
For example photostable fluorophores like quantum dots would be a good choice

\end{frame}

\begin{frame}{Fisher Information and the Cramer-Rao Bound}

Consider the general prescripton of maxmimum likelihood parameter estimation:

\begin{align*}
\mathcal{E}_{\mathrm{MLE}}: \theta^{*} = \underset{\theta}{\mathrm{argmax}}\; \ell(\mathcal{D}|\theta)
\end{align*}

where $\ell = \log\mathcal{L}$ is the log-likelihood function\\
\vspace{0.1in}
\textcolor{blue}{Question: can we derive a theoretical lower bound on our uncertainty in $\theta^{*}$ for an arbitrary estimator $\mathcal{E}$?}\\
\vspace{0.1in}
Start by defining the \emph{score} of $\ell$ with respect to $\theta$ as

\begin{align*}
\mathcal{S} = \underset{{x\sim p}}{\mathbb{E}}\left[\frac{\partial}{\partial\theta} \ell(x|\theta)\right]
\end{align*}

Since $x$ is a continuous random variable, we have to consider the average score

\end{frame}

\begin{frame}{Fisher Information for a single parameter}

The Fisher Information $I(\theta)$ is defined as the variance of the score

\begin{align*}
I(\theta) = \underset{{x\sim p}}{\mathbb{E}}\left[\frac{\partial}{\partial\theta} \left(\ell(x|\theta)\right)\right]^{2} = \underset{{x\sim p}}{\mathbb{E}}\left[\frac{\partial^{2}}{\partial\theta^{2}} \left(\ell(x|\theta)\right)\right]
\end{align*}

for $x\in\mathcal{D}$. The variance takes this from because it can be shown that $\mathcal{S}=0$\\
\vspace{0.1in}
Intuitively, if the likelihood is insensitive changes in $\theta$, then $\mathcal{D}$ does not provide very much information about $\theta$\\
\vspace{0.1in}
The Cramer-Rao Bound places a lower bound on the variance in our parameter estimate in iterms of $I(\theta)$ :

\begin{equation*}
\mathrm{Var}(\hat{\theta}) \geq \frac{1}{I(\theta)}
\end{equation*}

\end{frame}

\begin{frame}{Fisher Information for a multiple parameters}

When there are many parameters, the Fisher Information (second moment of the score) is a covariance matrix

\begin{align*}
I_{ij}(\theta) = \underset{{x\sim p}}{\mathbb{E}}\left[\frac{\partial}{\partial\theta_{i}} \left(\ell(x|\theta)\right)\frac{\partial}{\partial\theta_{j}} \left(\ell(x|\theta)\right)\right]
\end{align*}

We are going to consider the case where $\mathcal{L}$ is a multivariate Gaussian distribution:

\begin{align*}
\mathcal{L}(x|\mu,\Sigma) = \frac{1}{(2\pi)^{\frac{d}{2}}|\Sigma|^{\frac{1}{2}}}\exp\left(-\frac{1}{2}(x-\mu)\Sigma^{-1}(x-\mu)^{T}\right)
\end{align*}

We have $\theta=(\mu,\Sigma)$ so we need to compute $\frac{\partial \ell}{\partial \mu}$ and $\frac{\partial \ell}{\partial \Sigma}$


\end{frame}

\begin{frame}{Fisher Information for a multiple parameters}

Using our definition of the Fisher information:

\begin{align*}
\mathbf{I}(\theta) &= \underset{{x\sim p}}{\mathbb{E}}\begin{pmatrix}
\frac{\partial^{2}\ell}{\partial\mu^{2}} & \frac{\partial\ell}{\partial\Sigma}\frac{\partial\ell}{\partial\mu}\\
\frac{\partial\ell}{\partial\mu}\frac{\partial\ell}{\partial\Sigma} & \frac{\partial^{2}\ell}{\partial\Sigma^{2}}
\end{pmatrix}\\
&= \underset{{x\sim p}}{\mathbb{E}}\left[\mathbf{H}_{\ell}\;\ell(x|\mu,\Sigma)\right]
\end{align*}

where $\mathbf{H}_{\ell}$ is the Hessian of the log-likelihood. We note that $\mathbf{I}(\theta)$ is essentially the Hessian of the cross-entropy\\
\vspace{0.1in}
Thus we need to evaluate $\mathbf{H}_{\ell}$ explicitly


\end{frame}

\begin{frame}{Fisher Information for a multiple parameters}

The following derivatives can be shown using matrix calculus:

\begin{align*}
\frac{\partial\ell}{\partial\mu} &= \Sigma^{-1}(x-\mu)\\
\frac{\partial\ell}{\partial\Sigma} &= -\frac{1}{2}\left(\Sigma^{-1} - \Sigma^{-1}(x-\mu)(x-\mu)^{T}\Sigma^{-1}\right)
\end{align*}


\end{frame}

\section{References}

% Adding the option 'allowframebreaks' allows the contents of the slide to be expanded in more than one slide.
\begin{frame}[allowframebreaks]{References}
	\tiny\bibliography{references}
	\bibliographystyle{apalike}
\end{frame}

\end{document}