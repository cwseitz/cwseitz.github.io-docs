

\documentclass{article}
\title{The Fokker-Planck Equation}
\author{C.W. Seitz}
\date{\today}

\usepackage{graphicx}
\usepackage{subfigure,epsfig,amsfonts}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{float}
\usepackage{bm}

\begin{document}
\maketitle

\section{The Fokker-Planck Equation}

\subsection{Forward and Backward Kolmogorov Equations}

\subsection{Kramers-Moyal Expansion}

Gixen many instantiations of a stochastic xariable $x$, we can construct a normalize histogram oxer all obserxations as a function of time $P(x,t)$. Howexer, in order to systematically explore the relationship between the parameterization of the process and $P(x,t)$ we require an expression for $\dot{P}(x,t)$. If we make a fundamental assumption that the exolution of $P(x,t)$ follows a Markox process i.e. its exolution has the memoryless property, then we can write

\begin{equation}
P(x', t) = \int T(x', t | x, t-\tau)P(x, t-\tau)dx
\end{equation} 

which is known at the Chapman-Kolmogorox equation. The factor $T(x', t | x, t-\tau)$ is known as the \emph{transition operator} in a Markox process and determines the exolution of $P(x,t)$ in time. We proceed by writing $T(x', t | x, t-\tau)$ in a form referred to as the Kramers-Moyal expansion

\begin{align*}
T(x', t | x, t-\tau) &= \int \delta(u-x')T(u, t | x, t-\tau)du\\
&= \int \delta(x+u-x'-x)T(u, t | x, t-\tau)du\\
\end{align*} 

If we use the Taylor expansion of the $\delta$-function 

\begin{equation*}
\delta(x+u-x'-x) = \sum_{n=0}^{\infty} \frac{(u-x)^{n}}{n!}\left(-\frac{\partial}{\partial x}\right)^{n}\delta(x-x')
\end{equation*}

Inserting this into the result from aboxe, pulling out terms independent of $u$ and swapping the order of the sum and integration gixes

\begin{align}
T(x', t | x, t-\tau) &= \sum_{n=0}^{\infty} \frac{1}{n!}\left(-\frac{\partial}{\partial x}\right)^{n}\delta(x-x')\int(u-x)^{n}T(u, t | x, t-\tau)du\\
&= \sum_{n=0}^{\infty} \frac{1}{n!}\left(-\frac{\partial}{\partial x}\right)^{n}\delta(x-x')M_{n}(x,t)
\end{align} 

noticing that $M_{n}(x,t) = \int(u-x)^{n}T(u, t | x, t-\tau)du$ is just the $n$th moment of the transition operator $T$. Plugging (2.6) back in to (2.4) gixes 

\begin{align}
P(x, t) &= \int \left(1 + \sum_{n=1}^{\infty} \frac{1}{n!}\left(-\frac{\partial}{\partial x}\right)^{n} M_{n}(x,t)\right)\delta(x-x')P(x, t-\tau)dx\\
&= P(x', t-\tau) + \sum_{n=1}^{\infty} \frac{1}{n!}\left(-\frac{\partial}{\partial x}\right)^{n} \left[M_{n}(x,t)P(x,t)\right]
\end{align} 

Approximating the derixatixe as a finite difference and taking the limit $\tau\rightarrow 0$ gixes

\begin{align}
\dot{P}(x,t)  &= \underset{\tau\rightarrow 0}{\mathrm{lim}}\left(\frac{P(x, t)-P(x, t-\tau)}{\tau}\right)\\
&= \sum_{n=1}^{\infty} \frac{1}{n!}\left(-\frac{\partial}{\partial x}\right)^{n} \left[M_{n}(x,t)P(x,t)\right]
\end{align} 

which is formally known as the Kramers-Moyal (KM) expansion. The Fokker-Planck equation is a special case of (2.10) where we neglect terms $n>2$ in the \emph{diffusion approximation}.


Consider the following Ito stochastic differential equation 

\begin{align*}
d\vec{x} = F(\vec{x},t) + G(\vec{x},t)dW
\end{align*}

The SDE gixen aboxe corresponds to the Kramers-Moyal expansion (KME) of a transition density $T(x',t'|x,t)$ see (Risken 1989) for a full derixation.

\begin{align}
\frac{\partial P(x,t)}{\partial t}  &= \sum_{n=1}^{\infty} \frac{1}{n!}\left(-\frac{\partial}{\partial x}\right)^{n} \left[M_{n}(x,t)P(x,t)\right]
\end{align}

where $M_{n}$ is the $n$th moment of the transition density. In the diffusion approximation, the KME becomes the Fokker-Planck equation (FPE) (Risken 1989). For the sake of demonstration, consider the univariate case with random variable $x$ and the form of $T(x',t'|x,t)$ is a Gaussian with mean $\mu(t)$ and variance $\sigma^{2}(t)$. In this scenario, the FPE applies because $M_{n} = 0$ for all $n > 2$. Given that the drift $M_{1}(x,t) = \mu(t)$ and the diffusion $M_{2}(x,t) = \sigma^{2}(t)$, the FPE reads

\begin{align}
\frac{\partial P(x,t)}{\partial t}  &= \left(-\frac{\partial}{\partial x}M^{(1)}(t) + \frac{1}{2}\frac{\partial^{2}}{\partial x^{2}}M^{(2)}(t)\right)P(x,t)
\end{align}

We can additionally define the term in parentheses as a differential operator acting on $P(x,t)$

\begin{align}
\hat{\mathcal{L}}_{FP} = \left(-\frac{\partial}{\partial x}M^{(1)}(t) + \frac{1}{2}\frac{\partial^{2}}{\partial x^{2}}M^{(2)}(t)\right)
\end{align}

It is common to additionally define the probability current $J(x,t)$ as 

\begin{align}
J(x,t)  &= \left(M^{(1)}(t) - \frac{1}{2}\frac{\partial}{\partial x}M^{(2)}(t)\right)P(x,t)
\end{align}

This definition provides some useful intuition. The value of $J(x,t)$ is the net probability flux into the interval between $x$ and $x+dx$ at at time $t$. This also allows us to write the FPE as a continuity equation

\begin{align}
\frac{\partial P(x,t)}{\partial t} = -\frac{\partial J(x,t)}{\partial x}
\end{align}

\subsection{Solving the FPE: Heat (Diffusion) Equation}

The well-known heat equation (it has several names: diffusion equation, heat equation, Brownian motion, Wiener process) is a special case of the FPE where $M^{(1)}(t) = 0$ and $M^{(2)}(t) = \sigma^{2} = \mathrm{const}$. 

\begin{align}
\frac{\partial P(x,t)}{\partial t}  &= D\frac{\partial^{2}P(x,t)}{\partial x^{2}}
\end{align}

with $D = \sigma^{2}/2$. We would like to solve the above equation, but it is a PDE which usually require some tricks to solve e.g., integral transforms. Generally a transform can reduce a differential equation to a simpler form, like an ODE. Upon Fourier tranformation, spatial derivatives turn into factors of $ik$. That is, 

\begin{align*}
\frac{\partial}{\partial x} \psi(x,t) \rightarrow ik\tilde{\psi}(k,t)\;\;\;\;\;\; \frac{\partial^{2}}{\partial x^{2}} \psi(x,t) \rightarrow -k^{2}\tilde{\psi}(k,t)
\end{align*}


\subsubsection{Fourier Transform of the Heat Equation}

Recall the general definition of a Fourier pair

\begin{align*}
\tilde{\psi}(k) &= \mathcal{F}[\psi] = \int_{-\infty}^{\infty} \psi(x) e^{-2\pi ikx}dx\\
\psi(x) &= \mathcal{F}^{-1}[\tilde{\psi}] = \int_{-\infty}^{\infty} \tilde{\psi}(k) e^{2\pi ikx}dk
\end{align*}


Let's see the Fourier transformation of Eq. (6)

\begin{align}
\frac{\partial}{\partial t} \int_{-\infty}^{\infty}  P(x,t) e^{-2\pi ikx}dx = D\int_{-\infty}^{\infty} \frac{\partial^{2}P(x,t)}{\partial x^{2}} e^{-2\pi ikx}dx
\end{align}

As mentioned above, $\mathcal{F}[\partial_{x}\psi] = ik \mathcal{F}[\psi]$ and $\mathcal{F}[\partial_{x}^{2}\psi] = -k^{2} \mathcal{F}[\psi]$ which allows us to write the heat equation as a first order equation

\begin{align}
\frac{\partial \tilde{P}(k,t)}{\partial t}  &= -Dk^{2}\tilde{P}(k,t)
\end{align}

which suggests the solution $\tilde{p}_{0}(k) \exp\left(-D k^{2}t\right)$, which is Gaussian in $k$-space. Let's say our initial condition satisfies $\tilde{P}(x,t_{0}) = \delta (x-x_{0})$ which in the Fourier domain is $P(k,t_{0}) = \exp (-ikx_{0})$. The inverse transform is

\begin{align}
\int_{-\infty}^{\infty} \tilde{p}_{0}(k) \exp\left(ikx-D k^{2}t\right)dk &= \int_{-\infty}^{\infty} \exp\left(ik(x-x_{0})-D k^{2}t\right)dk
\end{align}

which we can rewrite as

\begin{align*}
\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} \exp\left(-\left(D k^{2}t - ik(x-x_{0}\right)\right)dk
= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} \exp\left(-Dt\left(k^{2} - \frac{ik(x-x_{0})}{Dt}\right)\right)dk
\end{align*}

Now we would like to complete the square in the exponential, since we know how to do Gaussian integrals. This can be done as follows:

\begin{align*}
\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} \exp\left(-Dt\left(k^{2} - \frac{ik(x-x_{0})}{Dt} + \frac{(x-x_{0})^{2}}{4D^{2}t^{2}} - \frac{(x-x_{0})^{2}}{4D^{2}t^{2}}\right)\right)dk
\end{align*}

We are then left to simplify,

\begin{align*}
\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} \exp\left(-Dt\left(k-\frac{i(x-x_{0})}{2Dt}\right)^{2}\right)dk &= \frac{1}{\sqrt{2\pi}}\exp\left(- \frac{(x-x_{0})^{2}}{4Dt}\right)\int_{-\infty}^{\infty} \exp\left(-Dtk'^{2}\right)dk'\\
&= \frac{1}{\sqrt{2Dt}}\exp\left(- \frac{(x-x_{0})^{2}}{4Dt}\right)
\end{align*}

which is a Gaussian distribution will time-dependent variance $\sigma=4Dt$, given originally by Einstein in his famous paper on Brownian motion in 1905. 

\subsection{Solving the FPE: Ornstein-Uhlenbeck}

The Ornstein-Uhlenbeck process is another special case of the FPE where $M^{(1)}(t) = -\gamma$ and $M^{(2)}(t) = \sigma^{2} = \mathrm{const}$. It is a stationary Gaussâ€“Markov process, which means that it is a Gaussian process, a Markov process, and is temporally homogeneous. The Ito SDE for this process reads

\begin{align}
dx = -\gamma xdt + \sigma dW
\end{align}

which of course has a corresponding Fokker-Planck equation

\begin{align}
\frac{\partial P(x,t)}{\partial t} = -\gamma\frac{\partial}{\partial x} x P(x,t) + D\frac{\partial^{2}P(x,t)}{\partial x^{2}}
\end{align}

In this form, the solution is slightly complicated by the presence of the first order spatial derivative. However, we can still find a solution via a Fourier transform:

\begin{align}
\frac{\partial \tilde{P}(k,t)}{\partial t}  =  -\gamma k \frac{\partial \tilde{P}(k,t)}{\partial k} -k^{2}D \tilde{P}(k,t)
\end{align}

Notice that this is a partial differential equation with the general form 

\begin{align}
a(\tilde{P},k,t)\partial_{k}\tilde{P} + b(\tilde{P},k,t)\partial_{t}\tilde{P} - c(\tilde{P},k,t) = 0
\end{align}

Therefore can solve the above equation using the method of characteristics. As a brief review, suppose we know a solution surface $\tilde{P}$. A vector normal to this surface has the form $\vec{u} = \langle \partial_{k}\tilde{P}, \partial_{t}\tilde{P}, -1 \rangle$. If this vector is normal to the surface, then the vector field 

\begin{align}
\vec{v} = \langle a(\tilde{P},k,t),b(\tilde{P},k,t),c(\tilde{P},k,t) \rangle
\end{align}

is tangent to the surface at every point. In other words, we would like to find a surface $\tilde{P}(k,t)$ for which the vector field above lies in the tangent plane to $\tilde{P}(k,t)$ and therefore $\vec{u}\cdot \vec{v} = 0$. The task that remains then is to find a $\tilde{P}(k,t)$ s.t. the vector $\vec{u}$ is orthogonal to $\vec{v}$. Now, if we construct a curve $\mathcal{C}$ which is an integral curve of $\vec{v}$, then this curve lies on the solution surface $\tilde{P}(k,t)$. Such a curve satisfies the ODEs

\begin{align*}
\frac{dk}{ds} &= \gamma k\\
\frac{dt}{ds} &= 1\\
\frac{d\tilde{P}}{ds} &= -k^{2}D\tilde{P}
\end{align*}

since the vector field given by the Fokker-Planck equation we have is $\vec{v} = \langle \gamma k, 1, -k^{2}D\rangle$. Clearly $t=s$ and $k = k_{0}\exp(\gamma t)$ and thus

\begin{align}
\frac{d\tilde{P}}{dt} &= -k^{2}D\tilde{P}\\
&= -Dk_{0}^{2}\exp(2\gamma t)\tilde{P}
\end{align}

and we have the solution in the Fourier domain

\begin{align}
\tilde{P}(k,t) &= \tilde{P}(k,0)\exp\left(-\frac{Dk_{0}^{2}}{2\gamma}(\exp(2\gamma t)-1)\right)\\
&= \exp\left(-ik_{0} x_{0} -\frac{Dk_{0}^{2}}{2\gamma}(\exp(2\gamma t)-1)\right)\\
&= \exp\left(-i k e^{-\gamma t} x_{0} -\frac{Dk^{2}}{2\gamma}(1-\exp(-2\gamma t))\right)
\end{align}

Let $\mu(t) = x_{0}\exp(-\gamma t)$ and $\sigma^{2}(t) = \frac{D}{\gamma}(1-e^{-2\gamma t})$

\begin{align}
\tilde{P}(k,t) &= \exp\left(-i k \mu(t) -\frac{k^{2}}{2}\sigma^{2}(t)\right)
\end{align}

Taking the inverse Fourier transform of this equation gives 


\begin{align}
P(x,t) &= \frac{1}{\sqrt{2\sigma^{2}(t)}}\exp\left(-\frac{(x-\mu(t))^{2}}{2\sigma^{2}(t)}\right)
\end{align}

\subsection{The Multivariate Case}

If we now generalize the above equation to a case where we are faced with many variables $\bm{x} = (x_{1},x_{2},...,x_{n})$. The continuity equation becomes 

\begin{align}
\frac{\partial P(\vec{x},t)}{\partial t} = -\vec{\nabla} \cdot J(\vec{x},t)
\end{align}

where the multivariate probability current now has the interpretation of the net flux into or out of a volume $dx^{n}$ centered around $\bm{x}$. If we consider each dimension, 

\begin{align}
J(x_{i},t)  &= \left(M_{i}^{(1)}(t) - \sum_{j}\frac{\partial}{\partial x_{j}}M_{ij}^{(2)}(t) \right)P(\vec{x},t)
\end{align}

The full Fokker-Planck equation then reads

\begin{align}
\frac{\partial P(\vec{x},t)}{\partial t}  &= \vec{\nabla} \cdot J(\vec{x},t)\\
&= \sum_{i=1}^{N}\left(-\frac{\partial}{\partial x_{i}}M_{i}^{(1)}(t) + \sum_{j=1}^{N} \frac{\partial^{2}}{\partial x_{i}\partial x_{j}}M_{ij}^{(2)}(t)\right)P(\vec{x},t)
\end{align}

It proves quite useful in this form because we can see that the Fokker-Planck equation represents a differentiation operator acting on the distribution $P(\vec{x},t)$

\begin{align}
\hat{\mathcal{L}}_{FP} = \sum_{i=1}^{N}\left(-\frac{\partial}{\partial x_{i}}M_{i}^{(1)}(t) + \sum_{j=1}^{N} \frac{\partial^{2}}{\partial x_{i}\partial x_{j}}M_{ij}^{(2)}(t)\right)
\end{align}

In the case that $M_{i}^{(1)}$ is actually a function of $j$ (as in the multivariate OU process), the first term will also include a sum over $j$.

\subsection{Ornstein-Uhlenbeck Process}

If the transition density is Gaussian then the density is fully specified by the first two moments $M^{(1)}(t) = \vec{\mu}(t)$ and $M^{(2)}(\vec{x},t) = \Sigma(t)$. The moments can also be functions of $\vec{x}$. Both of these possibilities are evident in the Ornstein-Uhlenbeck (OU) process. Let the drift vector be a linear function of the state $\vec{x}$ and the diffusion matrix the square of the Gaussian covariances

\begin{align*}
M^{(1)}(t) = \Gamma \vec{x}\;\;\;\;\;M^{(2)}(t) = 2D
\end{align*}

with $D = \Sigma\Sigma^{T}$ and $\Gamma$ are assumed to be independent of time (non-volatile).


The Fourier transform of (34) is fairly simple, since the FT is linear. 

\begin{align}
\frac{\partial \tilde{P}(\vec{x},t)}{\partial t}= -\sum_{ij} \left(\gamma_{ij}k_{i}\frac{\partial}{\partial k_{j}} \tilde{P}(\vec{k},t) + D_{ij}k_{i}k_{j}\tilde{P}(\vec{k},t)\right)
\end{align}

We can switch to Einstein notation to make this summation implicit

\begin{align}
\frac{\partial \tilde{P}(\vec{x},t)}{\partial t} + \gamma_{ij}k_{i}\frac{\partial}{\partial k_{j}} \tilde{P}(\vec{k},t) + D_{ij}k_{i}k_{j}\tilde{P}(\vec{k},t) = 0
\end{align}

This is in a very similar form to the single variable case (18). We then make the ansatz in analogy with (27)

\begin{align}
\tilde{P}(\vec{k},t) &= \exp\left(-\sum_{i} i k_{i}\mu_{i}(t) - \frac{1}{2}\sum_{ij}k_{i}k_{j}\sigma_{ij}(t)\right)
\end{align}

where we have written the sums explicitly to make the next step clear. We now plug this into (34), considering each term of (35) separately

\begin{align}
\frac{\partial \tilde{P}(\vec{x},t)}{\partial t} &= \frac{\partial}{\partial t} \exp\left(-\sum_{i} i k_{i}\mu_{i}(t) - \frac{1}{2}\sum_{ij}k_{i}k_{j}\sigma_{ij}(t)\right)\\
&= \tilde{P}\left(-i\sum_{i}k_{i}\dot{\mu}_{i} - \frac{1}{2}\sum_{ij}k_{i}k_{j}\dot{\sigma}_{ij}\right)
\end{align}

Now for the second term,

\begin{align}
\gamma_{ij}k_{i}\frac{\partial \tilde{P}}{\partial k_{j}}  &= \gamma_{ij}k_{i}\frac{\partial}{\partial k_{j}}\exp\left(-\sum_{i} i k_{i}\mu_{i}(t) - \frac{1}{2}\sum_{ij}k_{i}k_{j}\sigma_{ij}(t)\right)\\
&= \gamma_{ij}k_{i}\tilde{P}\left(-i\mu_{j}-k_{\ell}\sigma_{\ell j}\right)
\end{align}

We can see that, in the first part, the derivative w.r.t $k_{j}$ selects the $j$-th term of the summation over $i$. In the second part, we have the following

\begin{align}
-\frac{1}{2}\frac{\partial}{\partial k_{j}}\sum_{ij}k_{i}k_{j}\sigma_{ij} &= -\frac{1}{2}\sum_{\ell,j}\frac{\partial}{\partial k_{j}}k_{\ell}k_{j}\sigma_{\ell j}\\
&= -\sum_{\ell,j}k_{\ell}\sigma_{\ell j}\\
\end{align}

We see that the only terms in the sum over $i,j$ that survive are those for which $\sigma_{ij}\neq -\sigma_{ji}$ i.e, the antisymmetric part of $\sigma$ drops out. Furthermore, if we require that $\sigma_{\ell j} = \sigma_{j\ell}$ then the factor of $1/2$ vanishes. Finally, the third term remains unchanged and we write (dropping summations)

\begin{align}
\tilde{P}\left(-i k_{i}\dot{\mu}_{i} - \frac{1}{2}k_{i}k_{j}\dot{\sigma}_{ij} -\gamma_{ij}k_{i}i\mu_{j}-\gamma_{ij}k_{i}k_{\ell}\sigma_{\ell j} + D_{ij}k_{i}k_{j}\right) = 0
\end{align}

Since $\tilde{P} \neq 0$ we must have that

\begin{align}
\dot{\mu}_{i} &= -\gamma_{ij}\mu_{j}\\
\dot{\sigma}_{ij} &= -\gamma_{i\ell}\sigma_{lj} - \gamma_{jl}\sigma_{li} + 2D_{ij}
\end{align}

The first equation implies that $\mu_{i}(t) = \exp(-\sum_{j}\gamma_{ij}\mu_{j}t)$ or $\bm{\mu}(t) = \exp(-\bm{\Gamma}t)\bm{\mu}_{0}$

\subsection{Detailed balance of the Fokker-Planck operator}

We will now define the notation of \emph{detailed balance} for the FP operator. This is important, especially because it allows us to find the stationary distribution for the Ornstein-Uhlenbeck process in a much simpler way than the direct solution of the FPE shown above. A system that is at equilibrium will obey detailed balance in the sense that the net probability current at any point is zero. Another way of thinking about this is that, at equilibrium, the evolution of the probability density is time-reversible i.e., the Fokker-Planck equation is invariant under time reversal. This observation can drastically simplify the process of finding the equilibrium distribution. 

\end{document}