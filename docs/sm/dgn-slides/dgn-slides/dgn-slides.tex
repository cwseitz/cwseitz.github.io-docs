% Latex template: mahmoud.s.fahmy@students.kasralainy.edu.eg
% For more details: https://www.sharelatex.com/learn/Beamer

\documentclass[aspectratio=1610]{beamer}					% Document class

\setbeamertemplate{footline}[text line]{%
  \parbox{\linewidth}{\vspace*{-8pt}A brief introduction to graphical models and deep methods in computational network biology \hfill\insertshortauthor\hfill\insertpagenumber}}
\setbeamertemplate{navigation symbols}{}

\usepackage[english]{babel}				% Set language
\usepackage[utf8x]{inputenc}			% Set encoding

\mode<presentation>						% Set options
{
  \usetheme{default}					% Set theme
  \usecolortheme{default} 				% Set colors
  \usefonttheme{default}  				% Set font theme
  \setbeamertemplate{caption}[numbered]	% Set caption to be numbered
}

% Uncomment this to have the outline at the beginning of each section highlighted.
%\AtBeginSection[]
%{
%  \begin{frame}{Outline}
%    \tableofcontents[currentsection]
%  \end{frame}
%}

\usepackage{graphicx}					% For including figures
\usepackage{booktabs}					% For table rules
\usepackage{hyperref}					% For cross-referencing
\usepackage[absolute,overlay]{textpos}
\usepackage{bm}

\title{A brief introduction to graphical models and deep methods in computational network biology}	% Presentation title
\author{Clayton W. Seitz}								% Presentation author
\date{\today}									% Today's date	

\begin{document}

% Title page
% This page includes the informations defined earlier including title, author/s, affiliation/s and the date
\begin{frame}
  \titlepage
\end{frame}

% Outline
% This page includes the outline (Table of content) of the presentation. All sections and subsections will appear in the outline by default.
\begin{frame}{Outline}
  \tableofcontents
\end{frame}

% The following is the most frequently used slide types in beamer
% The slide structure is as follows:
%
%\begin{frame}{<slide-title>}
%	<content>
%\end{frame}

\section{Introduction to biological networks}

\begin{frame}{Computational network biology}

Emerging research field that encompasses theory and applications of \textcolor{red}{network models} to study complex interactions of cells, DNA, RNA, proteins, and metabolites\\
\vspace{0.2in}
Say we have a set of variables $\bm{x} = (x_{1},x_{2},...,x_{n})$ which might have some statistical dependence. $\bm{x}$ might be RNA or protein expression data, for example\\
\vspace{0.2in}
\begin{itemize}
\item Often we are handed a batch of empirical samples $\bm{X} = \{\bm{x_{1}},..,\bm{x_{k}}\}$
\item We want to learn about the generating distribution $P(\bm{x},t)$
\end{itemize}

\vspace{0.2in}
Joint effort between physics, computer science, and biology

\begin{textblock*}{3cm}(12.25cm,5.5cm)
\begin{figure}
\includegraphics[width=3cm]{net.png}
\end{figure}
\end{textblock*}

\end{frame}

\begin{frame}{A gene interaction network}
\begin{center}
\begin{textblock*}{13cm}(1.3cm,1.5cm)
\begin{figure}
\includegraphics[width=8cm]{gene-network.png}
\caption{\textbf{Landscape of genetic interactions} in cells. Edges between genes denote Pearson correlation coefficients ($\rho > 0.2$) calculated from the
complete genetic interaction matrix.}
\end{figure}
\end{textblock*}
\end{center}
\end{frame}

\begin{frame}{A protein interaction network}
\begin{center}
\begin{textblock*}{13cm}(1.5cm,1cm)
\begin{figure}
\includegraphics[width=10cm]{protein-interactome.png}
\caption{\textbf{Human protein interactome} of 17,706 proteins and 351,444 interactions (A) Overall complex network of human interactome. (B) Degree
(connectivity) distribution of proteins by following a power-law tail. (C) Several selected network topological characteristics of the interactome.}
\end{figure}
\end{textblock*}
\end{center}
\end{frame}

\begin{frame}{A cellular interaction network (model neurons)}
\begin{textblock*}{12cm}(0.5cm,0.75cm)
\begin{figure}
\includegraphics[width=11cm]{neural-network.png}
\caption{\textbf{Asychronous spiking of model neurons} (A) Steady-state raster plot of $N=100$ uncoupled EIF neurons undergoing stimulation with GWN with $\mu = 2\mu A/\mathrm{cm}^{2}$ and $\sigma = 9 \;\mu A/\mathrm{cm}^{2}$}
\end{figure}
\end{textblock*}
\begin{textblock*}{3cm}(12.25cm,0.5cm)
\begin{figure}
\includegraphics[width=3cm]{neural-network-graph.png}
\end{figure}
\end{textblock*}
\end{frame}

\begin{frame}{Probabilistic graphical models (PGMs)}

Probabilistic graphical models are a class of machine \\learning algorithms that represent statistical \\dependencies of probability distributions as graphs\\
\vspace{0.2in}
Two main types used in machine learning: \\\textcolor{red}{Bayesian Networks} (BNs), \textcolor{red}{Markov Random Fields} (MRFs), \\but there are others\\
\vspace{0.2in}
Major advantage is that they are \textcolor{red}{structured models}\\They do not scale as easily as deep networks


\begin{textblock*}{5cm}(10.75cm,1cm)
\begin{figure}
\includegraphics[width=4cm]{mrf.png}
\end{figure}
\end{textblock*}

\begin{textblock*}{5cm}(10.75cm,5cm)
\begin{figure}
\includegraphics[width=2.75cm]{bayes-net.png}
\end{figure}
\end{textblock*}

\end{frame}


\begin{frame}{Probabilistic graphical models (PGMs)}

\begin{textblock*}{10cm}(1cm,1.5cm)
Say we have a joint probability over gene expression $P(\bm{X})$\\A PGM describes how $P(\bm{X})$ factors\\
\end{textblock*}

\begin{textblock*}{9cm}(1cm,3.5cm)
\textcolor{red}{Markov Random Fields} (MRFs) e.g., Ising model
\begin{equation*}
P(\bm{X};\Theta) = \frac{1}{Z}\prod_{i=1}^{N}P(\bm{X_{i}},\mathcal{C}(X_{i});\Theta_{i})
\end{equation*}
\end{textblock*}

\begin{textblock*}{7cm}(1cm,6.5cm)
\textcolor{red}{Bayesian Network} (BNs) - include causality
\begin{equation*}
P(\bm{X}|\mathcal{G},\Theta) = \prod_{i=1}^{N}P(\bm{X_{i}}|\mathcal{C}(X_{i}),\Theta_{i})
\end{equation*}
\end{textblock*}


\begin{textblock*}{5cm}(10.75cm,1cm)
\begin{figure}
\includegraphics[width=4cm]{mrf.png}
\end{figure}
\end{textblock*}

\begin{textblock*}{5cm}(10.75cm,5cm)
\begin{figure}
\includegraphics[width=2.75cm]{bayes-net.png}
\end{figure}
\end{textblock*}

\begin{textblock*}{15cm}(1cm,9cm)
BNs as well as hybrid models have been used to examine gene expression
\end{textblock*}

\end{frame}

\begin{frame}{An example graphical model}

\begin{figure}
\includegraphics[width=11cm]{pi3k-pathway.png}
\caption{\textbf{PI3K pathway graph} discovery using graphical modeling (Ni et al. Bioinformatics 2018). c - transcript count, g - gene, p - protein, m - methylation}
\end{figure}
\end{frame}

\begin{frame}{A tradeoff between mechanistic understanding and scale}

Fine structure of molecular interactions sometimes can be resolved for low dimensionality\\
\vspace{0.2in}
\textcolor{red}{Computational complexity} often scales exponentially with an increase in variables, density of interactions\\
\vspace{0.2in}
In high-dimensional biological networks we often turn to classic dimensionality reduction or deep methods\\
\vspace{0.2in}
Introducing \textcolor{red}{latent variables} into the model can reduce \\computational complexity


\begin{textblock*}{3cm}(12.25cm,5.5cm)
\begin{figure}
\includegraphics[width=3cm]{net.png}
\end{figure}
\end{textblock*}

\end{frame}

\begin{frame}{Latent variables}
Modeling all possible conditional dependencies quickly becomes intractable, lots of parameters\\
\vspace{0.2in}
Introducing latent variables $\bm{z}$ can reduce the number of needed parameters\\
\vspace{0.2in}

\begin{figure}
\includegraphics[width=0.8\textwidth]{manifold.png}
\end{figure}

\end{frame}


\begin{frame}{Variational autoencoders (VAEs)}
The VAE architecture has been very succesful when applied to RNA-seq datasets see (Lopez Nature Methods 2020)

\begin{center}
\begin{figure}
\includegraphics[width=0.8\textwidth]{vae}
\caption{\textbf{Variational autoencoder architecture} Lopez 2020 EMBO}
\end{figure}
\end{center}
\end{frame}

\begin{frame}{Using the VAE for cell phenotyping}

\begin{textblock*}{15cm}(0.75cm,1.5cm)
Can do hypothesis testing (Bayes factor) but does not explicitly capture visible-visible causal relationships (captures latent-visible)\\
\vspace{0.1in}
558 genes/3005 cells for CORTEX, 7,397 genes/4016 cells for HEMATO (Lopez Nature Methods 2020)
\end{textblock*}
\begin{textblock*}{7cm}(0.75cm,3.5cm)
\begin{figure}
\includegraphics[width=7cm]{cortex-legend.png}
\end{figure}
\end{textblock*}
\begin{textblock*}{7cm}(0.75cm,5.5cm)
\begin{figure}
\includegraphics[width=7cm]{cortex.png}
\end{figure}
\end{textblock*}
\begin{textblock*}{7cm}(8.5cm,3.5cm)
\begin{figure}
\includegraphics[width=7cm]{hemato-legend.png}
\end{figure}
\end{textblock*}
\begin{textblock*}{7cm}(8.5cm,5.75cm)
\begin{figure}
\includegraphics[width=7cm]{hemato.png}
\end{figure}
\end{textblock*}
\end{frame}

\begin{frame}{}
RNA-seq has single cell-specificity and time resolution but lacks spatial resolution and data is noisy\\
\vspace{0.2in}
FISH techniques have single-cell specificity, spatial resolution, less noisy, but \textcolor{red}{lack time resolution} in single cell studies - RNA counts are not static (circadian rhythms, cell-cycle, drug-treatment)\\
\vspace{0.2in}
\emph{in silico} models make predictions based on \textcolor{red}{kinetic parameters} and gene regulatory networks, arbitrarily precise time resolution\\
\vspace{0.2in}
Using \emph{in silico} models to predict stationary statistics read out by multiplexed FISH? Or if dynamics are not relevant (maybe in tissue experiments), use graphical models on multiplexed FISH + spatial info
\end{frame}

\begin{frame}{}

While we can collect single-cell time-series data, even data collected at one time
point will contain variability due to (1) asynchrony of cells within a population (in terms of progression through a biological process), and (2) biological heterogeneity and often the presence of multiple cell (sub)types.\\
\vspace{0.2in}
This means that the assumption that each cell is drawn iid is not necessarily valid\\
\vspace{0.2in}
Partial information decomposition, variational autoencoder on static tissue data. Not confident that we can infer causality in this data format\\
\vspace{0.2in}
In-vitro, we can measure different time points which permits a larger class of models
\end{frame}

\begin{frame}{Linear dynamics of transcription and translation}

\begin{textblock*}{15cm}(1cm,1.25cm)
If we assume linearity (first-order reactions) in a gene regulatory network
\end{textblock*}

\begin{textblock*}{7cm}(0cm,1.75cm)

\begin{figure}
\begin{align*}
\dot{x_{i}} &= \sum_{j}m_{ji}y_{j} - \alpha_{i}x_{i} + \eta_{i}\\
\dot{y_{i}} &= r_{i}x_{i} - \beta_{i}y_{i} + \xi{i}
\end{align*}
\end{figure}
\end{textblock*}

\begin{textblock*}{7cm}(7cm,1.75cm)
\begin{figure}
\includegraphics[width=7cm]{linear.png}
\end{figure}
\end{textblock*}

\begin{textblock*}{15cm}(1cm,5.5cm)
For example, a three-dimensional gene network:

\begin{equation*}
\begin{bmatrix} 
    \dot{x_{1}}\\
	\dot{x_{2}}\\
	\dot{x_{3}}\\
    \dot{y_{1}}\\
	\dot{y_{2}}\\
	\dot{y_{3}}\\
    \end{bmatrix}
    =
\begin{bmatrix} 
    -\alpha_{1} & 0 & 0 & m_{11} & m_{21} & m_{31} \\
	0 & -\alpha_{2} & 0 & m_{12} & m_{22} & m_{32}\\
	0 & 0 & -\alpha_{3} & m_{13} & m_{23} & m_{33} \\
    r_{1} & 0 & 0 & -\beta_{1}  & 0 & 0 \\
	0 & r_{2} & 0 & 0  & -\beta_{2} & 0 \\
	0 & 0 & r_{3} & 0 & 0 & -\beta_{3}  \\
\end{bmatrix}
\begin{bmatrix} 
    x_{1}\\
	x_{2}\\
	x_{3}\\
    y_{1}\\
	y_{2}\\
	y_{3}\\
    \end{bmatrix}
+ 
\begin{bmatrix} 
    \eta_{1}\\
	\eta_{2}\\
	\eta_{3}\\
    \xi_{1}\\
	\xi_{2}\\
	\xi_{3}\\
    \end{bmatrix}
\end{equation*}

\end{textblock*}

\end{frame}

\begin{frame}{Example gene regulatory network in yeast}

\begin{figure}
\includegraphics[width=9.5cm]{yeast-net.png}
\end{figure}

\end{frame}

\begin{frame}{Linear dynamics of transcription and translation}

\begin{textblock*}{15cm}(1cm,1.25cm)
Assumptions: gene-gene interactions are linear, noise is Gaussian, long protein lifetimes
\end{textblock*}

\begin{textblock*}{7cm}(0cm,1.75cm)
\begin{figure}
\begin{align*}
\dot{x_{i}} &= \sum_{j}m_{ij}y_{j} - \alpha_{i}x_{i} + \eta_{i}\\
\dot{y_{i}} &= r_{i}x_{i} - \beta_{i}y_{i}
\end{align*}
\end{figure}
\end{textblock*}

\begin{textblock*}{7cm}(1cm,4.75cm)
If we assume that $\dot{y_{i}} \approx 0$ we have a Langevin equation for $x(t)$ and \\$y/x = \beta/r$
\end{textblock*}

\begin{textblock*}{7cm}(7cm,1.75cm)
\begin{figure}
\includegraphics[width=7cm]{linear.png}
\end{figure}
\end{textblock*}

\begin{textblock*}{15cm}(1cm,6.5cm)

Let $\gamma_{ij} = m_{ij}\beta/r$. An example of a 3-gene system:

\begin{equation*}
\begin{bmatrix} 
    \dot{x_{1}}\\
	\dot{x_{2}}\\
	\dot{x_{3}}\\
    \end{bmatrix}
    =
\begin{bmatrix} 
    -\alpha_{1} & \gamma_{21} & \gamma_{31} \\
	\gamma_{12} & -\alpha_{2} & \gamma_{32}\\
	\gamma_{13} & \gamma_{23} & -\alpha_{3} \\
\end{bmatrix}
\begin{bmatrix} 
    x_{1}\\
	x_{2}\\
	x_{3}\\
    \end{bmatrix}
+ 
\begin{bmatrix} 
    \eta_{1}\\
	\eta_{2}\\
	\eta_{3}\\
    \end{bmatrix}
\end{equation*}

\end{textblock*}

\end{frame}

\begin{frame}{Ornstein-Uhlenbeck process}

We have a linear SDE, 

\begin{align*}
dx_{i} = \gamma_{ij}x_{j}dt + \sigma_{ij}dW
\end{align*}

which has a corresponding Fokker-Planck equation:

\begin{align}
\frac{\partial \tilde{P}(\vec{x},t)}{\partial t}= -\bm{\gamma_{ij}}\frac{\partial}{\partial x_{j}} x_{i}\tilde{P}(\vec{x},t) + D_{ij}\frac{\partial^{2} \tilde{P}(\vec{x},t)}{\partial x_{i}x_{j}}
\end{align}

If the real part of the eigenvalues of $\gamma_{ij}$ are greater than zero, a stationary distribution exists

\end{frame}

\begin{frame}{Conditional distributions of a Gaussian}

Partition variables $\{x_{n}\}_{n=1}^{N}$ into sets $\bm{x}_{a}$ and $\bm{x}_{b}$. 

\begin{align*}
\mu = \begin{bmatrix} 
    \mu_{a}\\
	\mu_{b}\\
    \end{bmatrix}
\;\;\;\;
\Sigma = \begin{bmatrix} 
    \Sigma_{aa} & \Sigma_{ab}\\
    \Sigma_{ba} & \Sigma_{bb}\\
    \end{bmatrix}
\end{align*}

The conditional distribution $p(\bm{x}_{a}|\bm{x}_{b})$ must also be normal with parameters

\begin{align*}
\mu_{a|b} &= \mu_{a} + \Sigma_{ab}\Sigma_{bb}^{-1}(x_{b}-\mu_{b})\\
\Sigma_{a|b} &= \Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba}
\end{align*}

Bayesian networks loosely express causal relationships. We can compare $p(\bm{x}_{a}|\bm{x}_{b})$ and $p(\bm{x}_{a})$. \textcolor{red}{We can use this to assess quality of inference algorithms estimating the underlying network structure parameterized by the damping matrix} $\Gamma_{ij}$

\end{frame}

\begin{frame}{Marginal distributions of a Gaussian}

The conditional distribution $p(x_{1}|x_{2})$ between two variables $\bm{a}=x_{1}$, $\bm{b}=x_{2}$ has parameters

\begin{align*}
\mu_{1} &= \mu_{1} + \Sigma_{12}\Sigma_{22}^{-1}(x_{2}-\mu_{2})\\
\sigma_{1|2}^{2} &= \sigma_{1}^{2} -\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}
\end{align*}

The multivariate normal has the nice property that marginal distributions are

\begin{align*}
p(x_{1}) &= \mathcal{N}\left(\mu_{1},\sigma_{1}^{2}\right)
\end{align*}

Conditional independence implies that $\mathcal{N}\left(\mu_{1},\sigma_{1}^{2}\right) = \mathcal{N}\left(\mu_{1|2},\sigma_{1|2}^{2}\right)$. We can then factor $p(\bm{x})$ into a Bayesian network. \\


\end{frame}

\begin{frame}{Handling systems out of equilibrium}

In practice we cannot necessarily assume that the data is Gaussian or that the distribution is stationary. It is very difficult to guarantee equilibrium as we can for simulations\\
\vspace{0.2in}
We also may need to perform preprocessing on the data to determine if there is population heterogeneity (VAE?)

\end{frame}

\section{References}

% Adding the option 'allowframebreaks' allows the contents of the slide to be expanded in more than one slide.
\begin{frame}[allowframebreaks]{References}
	\tiny\bibliography{references}
	\bibliographystyle{apalike}
\end{frame}

\end{document}
