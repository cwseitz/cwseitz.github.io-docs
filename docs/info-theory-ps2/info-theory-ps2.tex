\documentclass[12pt]{article}
\usepackage{amsmath} % AMS Math Package
\usepackage{amsthm} % Theorem Formatting
\usepackage{amssymb}    % Math symbols such as \mathbb
\usepackage{graphicx} % Allows for eps images
\usepackage[dvips,letterpaper,margin=1in,bottom=0.7in]{geometry}
\usepackage{amsmath}


\newtheorem{p}{Problem}[section]
\usepackage{cancel}
\newtheorem*{lem}{Lemma}
\theoremstyle{definition}
\newtheorem*{dfn}{Definition}
 \newenvironment{s}{%\small%
        \begin{trivlist} \item \textbf{Solution}. }{%
            \hspace*{\fill} $\blacksquare$\end{trivlist}}%


\begin{document}

{\noindent\Huge\bf  \\[0.5\baselineskip] {\fontfamily{cmr}\selectfont  Problem Set 2}         }\\[2\baselineskip] % Title
{ {\bf \fontfamily{cmr}\selectfont Information and Coding Theory}\\ {\textit{\fontfamily{cmr}\selectfont     February 12, 2021}}}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    {\large \textsc{Clayton Seitz}
\\[1.4\baselineskip] 

\begin{p}
Find tight upper and lower bounds on two extremely biased coins where the first coin is distributed according to 

\[P = \begin{cases} 
      0 & \; \epsilon \\
      1 & \; 1 - \epsilon \\
   \end{cases}
\]

and the second is distributed according to

\[Q = \begin{cases} 
      0 & \; 2\epsilon \\
      1 & \; 1 - 2\epsilon \\
   \end{cases}
\]

\end{p}

\begin{s}
I will assume that distinguishing the two coins means that, given a sequence of $n$ flips, we can say whether it is coin $P$ or coin $Q$ 90 percent of the time. To start, we write out the KL-Divergence between the distributions $P$ and $Q$ for a sequence of $n$ coin tosses.

\begin{eqnarray*}
D(P||Q) &=& \epsilon\log \frac{1}{2\epsilon} + (1-\epsilon)\log \frac{1}{1-2\epsilon}\\
&=& \epsilon\log \frac{1-2\epsilon}{2\epsilon} + \epsilon\log \left(\frac{1}{1-2\epsilon}\right)^{1/\epsilon}\\
&=& \epsilon\left(\log \frac{1}{2\epsilon}(1-2\epsilon)^{\frac{1-\epsilon}{\epsilon}}\right)\\
&=& \frac{\epsilon}{2\ln 2}\left(\ln \frac{(1-2\epsilon)^{\frac{1-\epsilon}{\epsilon}}}{2\epsilon}\right)\\
&=& \frac{\epsilon}{2\ln 2}\left(\ln \left(1 + \frac{(1-2\epsilon)^{\frac{1-\epsilon}{\epsilon}}-2\epsilon}{2\epsilon}\right)\right)\\
&\leq& \frac{1}{3\ln 2}(1-2\epsilon)^{\frac{1-\epsilon}{\epsilon}}-2\epsilon
\end{eqnarray*}

At the same time, we know that

\begin{equation*}
n \geq \frac{1}{2\ln 2 \cdot D(P||Q)}\left(\frac{8}{5}\right)^{2}
\end{equation*}

which means that

\begin{equation*}
n \geq \frac{3}{2}\frac{1}{(1-2\epsilon)^{\frac{1-\epsilon}{\epsilon}}-2\epsilon}\left(\frac{8}{5}\right)^{2}
\end{equation*}

\end{s}

\begin{p}
Show that $0 \leq \mathbf{JSD}(P,Q) \leq 1$
\end{p}

\begin{s}

\begin{eqnarray*}
\mathbf{JSD}(P,Q) &=& \frac{1}{2}D(P||M) + \frac{1}{2}D(Q||M)\\
\end{eqnarray*}

The lower bound must be true because $D(P||M) \geq 0$ and $Q(P||M) \geq 0$. For the upper bound, consider just one of the terms

\begin{eqnarray*}
D(P||M) &=& \frac{1}{2}\sum_{x\sim P} P(x)\log \frac{P(x)}{M(x)}\\
&=&  \frac{1}{2}\sum_{x\sim P} P(x)\log \frac{2P(x)}{P(x)+Q(x)}\\
&\leq &  \frac{1}{2}\sum_{x\sim P} P(x)\log 2 = \frac{1}{2}
\end{eqnarray*}

Therefore, $\mathbf{JSD}(P,Q) \leq 1$.

Show that $\mathbf{JSD}(P,Q) \geq \frac{1}{8\ln 2} \cdot ||P-Q||_{1}^{2}$

\begin{eqnarray*}
\mathbf{JSD}(P,Q) &=&  \frac{1}{2}\left[D(P||M) + D(Q||M)\right]\\
&\geq & \frac{1}{4\ln 2}\left[||P-M||_{1}^{2} + ||Q-M||_{1}^{2}\right]\\
&= & \frac{1}{4\ln 2}\left[\left(\sum |P-M|\right)^{2} + \left(\sum |Q-M|\right)^{2} \right]\\ &= & \frac{1}{8\ln 2}\left[\left(\sum |P-Q|\right)^{2} + \left(\sum |Q-P|\right)^{2} \right]\\
\\ &= & \frac{1}{8\ln 2} \cdot ||P-Q||_{1}^{2}\\
\end{eqnarray*}


\begin{equation*}
\mathbf{JSD}_{\lambda}(P_{1} \; ... \; P_{k}) =  \sum_{i} \lambda_{i}D(P_{i}||M)
\end{equation*}

where $M = \sum_{i} \lambda_{i}P_{i}$. Show that 

\begin{equation*}
0 \leq \mathbf{JSD}_{\lambda}(P_{1} \; ... \; P_{k}) \leq H(\lambda)
\end{equation*}

As before, the lower bound must be true because $D(P_{i}||M) \geq 0$ and $\lambda$ is non-negative. As for the upper bound,

\begin{eqnarray*}
\mathbf{JSD}_{\lambda}(P_{1} \; ... \; P_{k}) &=&  \sum_{i} \lambda_{i}D(P_{i}||M)\\
&=& \sum_{i} \lambda_{i}P_{i} \log \frac{P_{i}}{M}\\
&=& H(\sum_{i} \lambda_{i}P_{i}) - \sum_{i}\lambda_{i}H(P_{i})\\
&=& H(\lambda) - \sum_{i}\lambda_{i}H(P_{i})\\
&\leq & H(\lambda)
\end{eqnarray*}

\end{s}

\begin{p}
Differential entropy of the multivariate Gaussian

\begin{equation*}
\phi(x) = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}\exp-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)
\end{equation*}

\end{p}

\begin{s}


\begin{eqnarray*}
h(x) &=& -\int \phi(x)\log \phi(x) dx\\
&=& \int \phi(x) \left[\frac{n}{2}\log (2\pi e) +\frac{1}{2} \log|\Sigma| + \frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)\right]dx\\
&=& \frac{n}{2}\log (2\pi e) +\frac{1}{2} \log|\Sigma| + \mathbf{E}\left[\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)\right]\\
&=& \frac{n}{2}\log (2\pi e) +\frac{1}{2} \log|\Sigma|\\
\end{eqnarray*}

\end{s}

\end{document}