\documentclass[12pt]{article}
\usepackage{amsmath} % AMS Math Package
\usepackage{amsthm} % Theorem Formatting
\usepackage{amssymb}    % Math symbols such as \mathbb
\usepackage{graphicx} % Allows for eps images
\usepackage[dvips,letterpaper,margin=1in,bottom=0.7in]{geometry}
\usepackage{amsmath}


\newtheorem{p}{Problem}[section]
\usepackage{cancel}
\newtheorem*{lem}{Lemma}
\theoremstyle{definition}
\newtheorem*{dfn}{Definition}
 \newenvironment{s}{%\small%
        \begin{trivlist} \item \textbf{Solution}. }{%
            \hspace*{\fill} $\blacksquare$\end{trivlist}}%


\begin{document}

{\noindent\Huge\bf  \\[0.5\baselineskip] {\fontfamily{cmr}\selectfont  Problem Set 2}         }\\[2\baselineskip] % Title
{ {\bf \fontfamily{cmr}\selectfont Information and Coding Theory}\\ {\textit{\fontfamily{cmr}\selectfont     February 12, 2021}}}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    {\large \textsc{Clayton Seitz}
\\[1.4\baselineskip] 

\begin{p}
Find tight upper and lower bounds on two extremely biased coins where the first coin is distributed according to 

\[P = \begin{cases} 
      0 & \; \epsilon \\
      1 & \; 1 - \epsilon \\
   \end{cases}
\]

and the second is distributed according to

\[Q = \begin{cases} 
      0 & \; 2\epsilon \\
      1 & \; 1 - 2\epsilon \\
   \end{cases}
\]

\end{p}

\begin{s}
I will assume that distinguishing the two coins means that, given a sequence of $n$ flips, we can say whether it is coin $P$ or coin $Q$ 90 percent of the time. To start, we write out the KL-Divergence between the distributions $P$ and $Q$ for a sequence of $n$ coin tosses.

\begin{eqnarray*}
D(P||Q) &=& \epsilon\log \frac{1}{2\epsilon} + (1-\epsilon)\log \frac{1}{1-2\epsilon}\\
&=& \epsilon\log \frac{1-2\epsilon}{2\epsilon} + \epsilon\log \left(\frac{1}{1-2\epsilon}\right)^{1/\epsilon}\\
&=& \epsilon\left(\log \frac{1}{2\epsilon}(1-2\epsilon)^{\frac{1-\epsilon}{\epsilon}}\right)\\
&=& \frac{\epsilon}{2\ln 2}\left(\ln \frac{(1-2\epsilon)^{\frac{1-\epsilon}{\epsilon}}}{2\epsilon}\right)\\
&=& \frac{\epsilon}{2\ln 2}\left(\ln \left(1 + \frac{(1-2\epsilon)^{\frac{1-\epsilon}{\epsilon}}-2\epsilon}{2\epsilon}\right)\right)\\
&\leq& \frac{1}{3\ln 2}(1-2\epsilon)^{\frac{1-\epsilon}{\epsilon}}-2\epsilon
\end{eqnarray*}

At the same time, we know that

\begin{equation*}
n \geq \frac{1}{2\ln 2 \cdot D(P||Q)}\left(\frac{8}{5}\right)^{2}
\end{equation*}

which means that

\begin{equation*}
n \geq \frac{3}{2}\frac{1}{(1-2\epsilon)^{\frac{1-\epsilon}{\epsilon}}-2\epsilon}\left(\frac{8}{5}\right)^{2}
\end{equation*}

\end{s}

\begin{p}
Show that $0 \leq \mathbf{JSD}(P,Q) \leq 1$
\end{p}

\begin{s}

\begin{eqnarray*}
\mathbf{JSD}(P,Q) &=& \frac{1}{2}D(P||M) + \frac{1}{2}D(Q||M)\\
\end{eqnarray*}

The lower bound must be true because $D(P||M) \geq 0$ and $Q(P||M) \geq 0$. For the upper bound, consider just one of the terms

\begin{eqnarray*}
D(P||M) &=& \frac{1}{2}\sum_{x\sim P} P(x)\log \frac{P(x)}{M(x)}\\
&=&  \frac{1}{2}\sum_{x\sim P} P(x)\log \frac{2P(x)}{P(x)+Q(x)}\\
&\leq &  \frac{1}{2}\sum_{x\sim P} P(x)\log 2 = 1
\end{eqnarray*}

Therefore, $\mathbf{JSD}(P,Q) \leq 1$.

\end{s}


\end{document}