\documentclass[12pt]{article}
\usepackage{amsmath} % AMS Math Package
\usepackage{amsthm} % Theorem Formatting
\usepackage{amssymb}    % Math symbols such as \mathbb
\usepackage{graphicx} % Allows for eps images
\usepackage[dvips,letterpaper,margin=1in,bottom=0.7in]{geometry}
\usepackage{amsmath}


\newtheorem{p}{Problem}[section]
\usepackage{cancel}
\newtheorem*{lem}{Lemma}
\theoremstyle{definition}
\newtheorem*{dfn}{Definition}
 \newenvironment{s}{%\small%
        \begin{trivlist} \item \textbf{Solution}. }{%
            \hspace*{\fill} $\blacksquare$\end{trivlist}}%


\begin{document}

{\noindent\Huge\bf  \\[0.5\baselineskip] {\fontfamily{cmr}\selectfont  Problem Set 2}         }\\[2\baselineskip] % Title
{ {\bf \fontfamily{cmr}\selectfont Information and Coding Theory}\\ {\textit{\fontfamily{cmr}\selectfont     February 12, 2021}}}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    {\large \textsc{Clayton Seitz}
\\[1.4\baselineskip] 

\begin{p}
Find tight upper and lower bounds on two extremely biased coins where the first coin is distributed according to 

\[P = \begin{cases} 
      0 & \; \epsilon \\
      1 & \; 1 - \epsilon \\
   \end{cases}
\]

and the second is distributed according to

\[Q = \begin{cases} 
      0 & \; 2\epsilon \\
      1 & \; 1 - 2\epsilon \\
   \end{cases}
\]

\end{p}

\begin{s}
I will assume that distinguishing the two coins means that, given a sequence of $n$ flips, we can say whether it is coin $P$ or coin $Q$ in at least $\frac{9}{10}n$ trials, on average. To start, we write out the KL-Divergence between the distributions $P$ and $Q$

\begin{align*}
D(P||Q) &= \epsilon\log \frac{\epsilon}{2\epsilon} + (1-\epsilon)\log \frac{1-\epsilon}{1-2\epsilon}\\
&= (1-\epsilon)\log \frac{1-\epsilon}{1-2\epsilon} - \epsilon\\
&= (1-\epsilon)\log \left(1 + \frac{\epsilon}{1-2\epsilon}\right) - \epsilon\\
&\leq (1-\epsilon)\left(\frac{\epsilon}{1-2\epsilon}\right) - \epsilon\\
&= \frac{1}{2\ln 2}\cdot \frac{\epsilon^{2}}{1-2\epsilon}
\end{align*}

Now, for a sequence $\bar{x}$ drawn from the product distribution $Q^{n}$, we can bound the probability that $\bar{x}$ is of a particular type $P$

\begin{equation*}
\frac{1}{(n+1)} 2^{-nD(P||Q)} \leq \underset{\bar{x} \sim Q^{n}}{\mathbb{P}}\left[P_{\bar{x}} = P\right] \leq 2^{-nD(P||Q)}
\end{equation*}

For the lower bound, we have

\begin{equation*}
\underset{\bar{x} \sim Q^{n}}{\mathbb{P}}\left[\sum_{i}\bar{x}_{i} \geq 
2\epsilon n\right]\geq 
\left(\frac{(1-2\epsilon)n + 1}{n+1}\right) 2^{-nD(P||Q)}
\end{equation*}

because there are $(1-2\epsilon)n + 1$ choices between $2\epsilon n$ and $n$ and we would like to know what is the probability a sequence drawn from $Q^{n}$ has a deviation of at least $2\epsilon n$ heads. Now for the upper bound we have

\begin{equation*}
\underset{\bar{x} \sim P^{n}}{\mathbb{P}}\left[\sum_{i}\bar{x}_{i} \geq 
2\epsilon n\right] \leq ((1-2\epsilon)n + 1)\cdot 2^{-nD(P||Q)}
\end{equation*}

\end{s}

\begin{p}
Show that $0 \leq \mathbf{JSD}(P,Q) \leq 1$
\end{p}

\begin{s}

\begin{eqnarray*}
\mathbf{JSD}(P,Q) &=& \frac{1}{2}D(P||M) + \frac{1}{2}D(Q||M)\\
\end{eqnarray*}

The lower bound must be true because $D(P||M) \geq 0$ and $D(Q||M) \geq 0$. These bounds can be seen directly from the fact that the ratio $\frac{P(x)}{P(x)+Q(x)}$ must lie between 1 and 2. To be precise, consider just one of the terms

\begin{eqnarray*}
D(P||M) &=& \frac{1}{2}\sum_{x\sim P} P(x)\log \frac{P(x)}{M(x)}\\
&=&  \frac{1}{2}\sum_{x\sim P} P(x)\log \frac{2P(x)}{P(x)+Q(x)}\\
&\leq &  \frac{1}{2}\sum_{x\sim P} P(x)\log 2 = \frac{1}{2}
\end{eqnarray*}

Therefore, $\mathbf{JSD}(P,Q) \leq 1$.
\\
\\
\\
Show that $\mathbf{JSD}(P,Q) \geq \frac{1}{8\ln 2} \cdot ||P-Q||_{1}^{2}$

\begin{eqnarray*}
\mathbf{JSD}(P,Q) &=&  \frac{1}{2}\left[D(P||M) + D(Q||M)\right]\\
&\geq & \frac{1}{4\ln 2}\left[||P-M||_{1}^{2} + ||Q-M||_{1}^{2}\right]\\
&= & \frac{1}{4\ln 2}\left[\left(\sum |P-M|\right)^{2} + \left(\sum |Q-M|\right)^{2} \right]\\ &= & \frac{1}{8\ln 2}\left[\left(\sum |P-Q|\right)^{2} + \left(\sum |Q-P|\right)^{2} \right]\\
\\ &= & \frac{1}{8\ln 2} \cdot ||P-Q||_{1}^{2}\\
\end{eqnarray*}

Show that $\mathbf{JSD(P,Q)} \leq \frac{1}{2}\cdot ||P-Q||_{1}$ 

\begin{align*}
\mathbf{JSD}_{\lambda}(P,Q) &=  \frac{1}{2}\left[D(P||M) + D(Q||M)\right]\\
&= \frac{1}{2}\left[\sum_{x\sim P} P(x)\log\frac{P(x)}{M(x)} + \sum_{x\sim Q} Q(x)\log\frac{Q(x)}{M(x)}\right]\\
&= \frac{1}{2}\sum_{x\sim \chi}\left[P(x)\log\frac{2P(x)}{P(x)+Q(x)} + Q(x)\log\frac{2Q(x)}{P(x)+Q(x)} \right]\\
&= \frac{1}{2}\sum_{x\sim \chi}\left[P(x)+Q(x)\right]\cdot\frac{P(x)}{P(x)+Q(x)}\log\frac{2P(x)}{P(x)+Q(x)}\\
&\quad + \frac{Q(x)}{P(x)+Q(x)}\log\frac{2Q(x)}{P(x)+Q(x)}\\
&= \frac{1}{2}\sum_{x\sim \chi}\left[P(x)+Q(x)\right] \cdot |1-H\left(\frac{P(x)}{P(x)+Q(x)},\frac{Q(x)}{P(x)+Q(x)}\right)|\\
&\leq \frac{1}{2}\sum_{x\sim \chi} |P(x)-Q(x)|
\end{align*}

Since the entropy is strictly non-negative.

\begin{equation*}
\mathbf{JSD}_{\lambda}(P_{1} \; ... \; P_{k}) =  \sum_{i} \lambda_{i}D(P_{i}||M)
\end{equation*}

where $M = \sum_{i} \lambda_{i}P_{i}$. Show that 

\begin{equation*}
0 \leq \mathbf{JSD}_{\lambda}(P_{1} \; ... \; P_{k}) \leq H(\lambda)
\end{equation*}

As before, the lower bound must be true because $D(P_{i}||M) \geq 0$ and $\lambda$ is non-negative. As for the upper bound,

\begin{eqnarray*}
\mathbf{JSD}_{\lambda}(P_{1} \; ... \; P_{k}) &=&  \sum_{i} \lambda_{i}D(P_{i}||M)\\
&=& \sum_{i} \lambda_{i}P_{i} \log \frac{P_{i}}{M}\\
&=& H(\sum_{i} \lambda_{i}P_{i}) - \sum_{i}\lambda_{i}H(P_{i})\\
&=& H(\lambda) - \sum_{i}\lambda_{i}H(P_{i})\\
&\leq & H(\lambda)
\end{eqnarray*}

\end{s}

\begin{p}
Counting using the method of types
\end{p}

\begin{s}
Sanov's Theorem states that 

\begin{equation*}
Q^{n}(E) = (n+1)^{r}2^{-nD(P^{*}||Q)}
\end{equation*}

where $P^{*}$ is the distribution in $E$ that is closest to $Q$. Since $Q$ is a uniform distribution we have that 

\begin{equation*}
D(P^{*}||Q) = \log m - H(P)
\end{equation*}

Therefore, if we let $H^{*}$ be the entropy of the distribution which has maximum entropy, Sanov's theorem becomes 

\begin{equation*}
Q^{n}(E) = (n+1)^{r}2^{-n(\log m - H^{*})}
\end{equation*}

\begin{align*}
|S| &= m^{n} (n+1)^{r}2^{-n(\log m - H^{*})} \\
&= (n+1)^{r}2^{-nH^{*}}  
\end{align*}

and therefore, in general, $|S| \leq (n+1)^{r}2^{-nH^{*})}  $.


\end{s}

\begin{p}
Differential entropy of the multivariate Gaussian with $n$ variables

\begin{equation*}
\phi(x) = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}\exp-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)
\end{equation*}

\end{p}

\begin{s}


\begin{eqnarray*}
h(x) &=& -\int \phi(x)\log \phi(x) dx\\
&=& \int \phi(x) \left[\frac{n}{2}\log (2\pi e) +\frac{1}{2} \log|\Sigma| + \frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)\right]dx\\
&=& \frac{n}{2}\log (2\pi e) +\frac{1}{2} \log|\Sigma| + \mathbf{E}\left[\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)\right]\\
&=& \frac{n}{2}\log (2\pi e) +\frac{1}{2} \log|\Sigma|\\
\end{eqnarray*}

Now, we would like to show that 

\begin{equation*}
|\alpha\Sigma_{1} + (1-\alpha)\Sigma_{2}| \leq |\Sigma_{1}|^{\alpha} \cdot |\Sigma_{2}|^{1-\alpha}
\end{equation*}

\end{s}

\begin{p}
Chernoff bound for read-k families
\end{p}

\begin{s}
We can modify Shearer's Lemma for the scenario where we are dealing with subsets of $\it{at \;most}$ size $k$.

\begin{equation*}
k\cdot H(X_{1} \; ... \; X_{n}) \geq \sum_{S\in \mathcal{F}} H(X_{S})
\end{equation*}

where $\mathcal{F}$ is a collection of subsets  $S$ of $[n]$ which are at most of length $k$ and $X_{S}$ is a particular subset. Consider the following modification

\begin{equation*}
k\cdot H(X_{1} \; ... \; X_{n}) - k\cdot H(Y_{1} \; ... \; Y_{n})\geq  \sum_{S\in \mathcal{F}} H(X_{S}) - k\cdot H(Y_{1} \; ... \; Y_{n}) 
\end{equation*}

since $X$ is uniformly distributed we have that

\begin{align*}
k\cdot D(Y_{1} \; ... \; Y_{n}||X_{1} \; ... \; X_{n}) &\geq \sum_{S\in \mathcal{F}} H(X_{S}) - k\cdot H(Y_{1} \; ... \; Y_{n}) \\
&= \sum_{S\in \mathcal{F}} H(X_{S})  - k\cdot H(Y_{1} \; ... \; Y_{n}) \\
&= k\sum H(X)  - k\cdot H(Y_{1} \; ... \; Y_{n}) \\
&\geq k\sum H(X)  - \sum H(Y_{S})\\
&= \sum_{i} D(Y_{S_{i}}||X_{S_{i}}) 
\end{align*}

We can see why this last equality is true by using the fact that $X$ is uniformly distributed and each $X$ is independent

\begin{equation*}
D(Y_{S}||X_{S})  = \sum H(X_{S}) - H(Y_{S}) = k\sum H(X) - \sum H(Y_{S})
\end{equation*}

Since both are uniformly distributed, we can calculate the ratio of the number of subsets where the condition is satisfied to the total number of possible subsets. This can be written in terms of entropies

\begin{align*}
\mathbb{P}_{X_{1}\; ... \; X_{n}} \left[\sum f_{i}(X_{S}) \geq t\right] &= \frac{|A|}{|X|} \\
&= 2^{H(Y_{1} \; ... \; Y_{n}) - H(X_{1} \; ... \; X_{n})}\\
&= 2^{-D(Y_{1} \; ... \; Y_{n}||X_{1} \; ... \; X_{n})}
\end{align*}

when the distributions are uniform. Also, since the expectation 

\begin{align*}
D(Y_{S_{i}}||X_{S_{i}}) &\geq D(f_{i}(Y_{S_{i}})||f_{i}(X_{S_{i}})) \\
&= \mathbb{E}_{f_{i}(Y_{S_{i}})}\left[\log\frac{\mathbf{Pr}[f_{i}(Y_{S_{i}})]}{\mathbf{Pr}[f_{i}(X_{S_{i}})]} \right]\\
&= \mathbf{Pr}[f_{i}(Y_{S_{i}})=0]\log\frac{\mathbf{Pr}[f_{i}(Y_{S_{i}})=0]}{\mathbf{Pr}[f_{i}(X_{S_{i}})=0]} \\
&+ \mathbf{Pr}[f_{i}(Y_{S_{i}})=1]\log\frac{\mathbf{Pr}[f_{i}(Y_{S_{i}})=1]}{\mathbf{Pr}[f_{i}(X_{S_{i}})=1]}\\
&= \nu_{i}\log \frac{\nu_{i}}{\mu_{i}} + (1-\nu_{i})\log \frac{1-\nu_{i}}{1-\mu_{i}}\\
&= D(\nu_{i}||\mu_{i})
\end{align*}

Finally, we can combine these results and use the convexity of KL-Divergence to show that

\begin{align*}
\mathbb{P}_{X_{1}\; ... \; X_{n}} \left[\sum_{i} f_{i}(X_{S_{i}}) \geq t\right] &= 2^{-D(Y_{1} \; ... \; Y_{n}||X_{1} \; ... \; X_{n})}\\
&\leq 2^{-\frac{1}{k}\sum_{i} D(Y_{S_{i}}||X_{S_{i}})}\\
&= 2^{-\frac{1}{k}\sum_{i} D(\nu_{i}||\mu_{i})}\\
&\leq 2^{-\frac{r}{k}D(\mu + \epsilon||\mu)}
\end{align*}


\end{s}

\end{document}