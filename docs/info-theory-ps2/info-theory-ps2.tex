\documentclass[12pt]{article}
\usepackage{amsmath} % AMS Math Package
\usepackage{amsthm} % Theorem Formatting
\usepackage{amssymb}    % Math symbols such as \mathbb
\usepackage{graphicx} % Allows for eps images
\usepackage[dvips,letterpaper,margin=1in,bottom=0.7in]{geometry}
\usepackage{amsmath}


\newtheorem{p}{Problem}[section]
\usepackage{cancel}
\newtheorem*{lem}{Lemma}
\theoremstyle{definition}
\newtheorem*{dfn}{Definition}
 \newenvironment{s}{%\small%
        \begin{trivlist} \item \textbf{Solution}. }{%
            \hspace*{\fill} $\blacksquare$\end{trivlist}}%


\begin{document}

{\noindent\Huge\bf  \\[0.5\baselineskip] {\fontfamily{cmr}\selectfont  Problem Set 2}         }\\[2\baselineskip] % Title
{ {\bf \fontfamily{cmr}\selectfont Information and Coding Theory}\\ {\textit{\fontfamily{cmr}\selectfont     February 12, 2021}}}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    {\large \textsc{Clayton Seitz}
\\[1.4\baselineskip] 

\begin{p}
Find tight upper and lower bounds on two extremely biased coins where the first coin is distributed according to 

\[P = \begin{cases} 
      0 & \; \epsilon \\
      1 & \; 1 - \epsilon \\
   \end{cases}
\]

and the second is distributed according to

\[Q = \begin{cases} 
      0 & \; 2\epsilon \\
      1 & \; 1 - 2\epsilon \\
   \end{cases}
\]

\end{p}

\begin{s}
I will assume that distinguishing the two coins means that, given a sequence of $n$ flips, we can say whether it is coin $P$ or coin $Q$ in at least $\frac{9}{10}n$ trials, on average. To start, we write out the KL-Divergence between the distributions $P$ and $Q$

\begin{align*}
D(P||Q) &= \epsilon\log \frac{\epsilon}{2\epsilon} + (1-\epsilon)\log \frac{1-\epsilon}{1-2\epsilon}\\
&= (1-\epsilon)\log \frac{1-\epsilon}{1-2\epsilon} - \epsilon\\
&= (1-\epsilon)\log \left(1 + \frac{\epsilon}{1-2\epsilon}\right) - \epsilon\\
&\leq (1-\epsilon)\left(\frac{\epsilon}{1-2\epsilon}\right) - \epsilon\\
&= \frac{1}{2\ln 2}\cdot \frac{\epsilon^{2}}{1-2\epsilon}
\end{align*}


At the same time, we know that

\begin{equation*}
n \geq \frac{1}{2\ln 2 \cdot D(P||Q)}\left(\frac{8}{5}\right)^{2}
\end{equation*}

given the constraint on successful predictions. Substituting for $D(P||Q)$ gives us the lower bound on $n$

\begin{equation*}
n \geq \left(\frac{1}{\epsilon^{2}} - \frac{2}{\epsilon}\right)\left(\frac{8}{5}\right)^{2}
\end{equation*}

As for the upper bound on $n$, we can use Chernoff bounds which 
states the probability of significant deviations from the expected number of 
heads (tails). 

\begin{equation*}
\mathbb{P}_{P}\left[ \sum_{i}x_{i} \geq \frac{n}{2} + \epsilon n \right] \leq (n+1)\cdot 2^{-c\cdot n\cdot \epsilon^{2}}
\end{equation*}


\end{s}

\begin{p}
Show that $0 \leq \mathbf{JSD}(P,Q) \leq 1$
\end{p}

\begin{s}

\begin{eqnarray*}
\mathbf{JSD}(P,Q) &=& \frac{1}{2}D(P||M) + \frac{1}{2}D(Q||M)\\
\end{eqnarray*}

The lower bound must be true because $D(P||M) \geq 0$ and $Q(P||M) \geq 0$. For the upper bound, consider just one of the terms

\begin{eqnarray*}
D(P||M) &=& \frac{1}{2}\sum_{x\sim P} P(x)\log \frac{P(x)}{M(x)}\\
&=&  \frac{1}{2}\sum_{x\sim P} P(x)\log \frac{2P(x)}{P(x)+Q(x)}\\
&\leq &  \frac{1}{2}\sum_{x\sim P} P(x)\log 2 = \frac{1}{2}
\end{eqnarray*}

Therefore, $\mathbf{JSD}(P,Q) \leq 1$.

Show that $\mathbf{JSD}(P,Q) \geq \frac{1}{8\ln 2} \cdot ||P-Q||_{1}^{2}$

\begin{eqnarray*}
\mathbf{JSD}(P,Q) &=&  \frac{1}{2}\left[D(P||M) + D(Q||M)\right]\\
&\geq & \frac{1}{4\ln 2}\left[||P-M||_{1}^{2} + ||Q-M||_{1}^{2}\right]\\
&= & \frac{1}{4\ln 2}\left[\left(\sum |P-M|\right)^{2} + \left(\sum |Q-M|\right)^{2} \right]\\ &= & \frac{1}{8\ln 2}\left[\left(\sum |P-Q|\right)^{2} + \left(\sum |Q-P|\right)^{2} \right]\\
\\ &= & \frac{1}{8\ln 2} \cdot ||P-Q||_{1}^{2}\\
\end{eqnarray*}

Show that $\mathbf{JSD(P,Q)} \leq \frac{1}{2}\cdot ||P-Q||_{1}$ 

\begin{align*}
\mathbf{JSD}_{\lambda}(P,Q) &=  \frac{1}{2}\left[D(P||M) + D(Q||M)\right]\\
&= \frac{1}{2}\left[\sum_{x\sim P} P(x)\log\frac{P(x)}{M(x)} + \sum_{x\sim Q} Q(x)\log\frac{Q(x)}{M(x)}\right]\\
&= \frac{1}{2}\sum_{x\sim \chi}\left[P(x)\log\frac{2P(x)}{P(x)+Q(x)} + Q(x)\log\frac{2Q(x)}{P(x)+Q(x)} \right]\\
&= \frac{1}{2}\sum_{x\sim \chi}\left[P(x)+Q(x)\right]\cdot\frac{P(x)}{P(x)+Q(x)}\log\frac{2P(x)}{P(x)+Q(x)}\\
&\quad + \frac{Q(x)}{P(x)+Q(x)}\log\frac{2Q(x)}{P(x)+Q(x)}\\
&= \frac{1}{2}\sum_{x\sim \chi}\left[P(x)+Q(x)\right] \cdot |1-H\left(\frac{P(x)}{P(x)+Q(x)},\frac{Q(x)}{P(x)+Q(x)}\right)|\\
&\leq \sum_{x\sim \chi} |P(x)-Q(x)|
\end{align*}


\begin{equation*}
\mathbf{JSD}_{\lambda}(P_{1} \; ... \; P_{k}) =  \sum_{i} \lambda_{i}D(P_{i}||M)
\end{equation*}

where $M = \sum_{i} \lambda_{i}P_{i}$. Show that 

\begin{equation*}
0 \leq \mathbf{JSD}_{\lambda}(P_{1} \; ... \; P_{k}) \leq H(\lambda)
\end{equation*}

As before, the lower bound must be true because $D(P_{i}||M) \geq 0$ and $\lambda$ is non-negative. As for the upper bound,

\begin{eqnarray*}
\mathbf{JSD}_{\lambda}(P_{1} \; ... \; P_{k}) &=&  \sum_{i} \lambda_{i}D(P_{i}||M)\\
&=& \sum_{i} \lambda_{i}P_{i} \log \frac{P_{i}}{M}\\
&=& H(\sum_{i} \lambda_{i}P_{i}) - \sum_{i}\lambda_{i}H(P_{i})\\
&=& H(\lambda) - \sum_{i}\lambda_{i}H(P_{i})\\
&\leq & H(\lambda)
\end{eqnarray*}

\end{s}

\begin{p}
Counting using the method of types
\end{p}

\begin{s}
Sanov's Theorem states that 

\begin{equation*}
Q^{n}(E) = (n+1)^{r}2^{-nD(P^{*}||Q)}
\end{equation*}

where $P^{*}$ is the distribution in $E$ that is closest to $Q$. Since $Q$ is a uniform distribution we have that 

\begin{equation*}
D(P^{*}||Q) = \log m - H(P)
\end{equation*}

Therefore, if we let $H^{*}$ be the entropy of the distribution which has maximum entropy, Sanov's theorem becomes 

\begin{equation*}
Q^{n}(E) = (n+1)^{r}2^{-n(\log m - H^{*})}
\end{equation*}

\begin{align*}
|S| &= m^{n} (n+1)^{r}2^{-n(\log m - H^{*})} \\
&= (n+1)^{r}2^{-nH^{*}}  
\end{align*}

and therefore, in general, $|S| \leq (n+1)^{r}2^{-nH^{*})}  $.


\end{s}

\begin{p}
Differential entropy of the multivariate Gaussian with $n$ variables

\begin{equation*}
\phi(x) = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}\exp-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)
\end{equation*}

\end{p}

\begin{s}


\begin{eqnarray*}
h(x) &=& -\int \phi(x)\log \phi(x) dx\\
&=& \int \phi(x) \left[\frac{n}{2}\log (2\pi e) +\frac{1}{2} \log|\Sigma| + \frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)\right]dx\\
&=& \frac{n}{2}\log (2\pi e) +\frac{1}{2} \log|\Sigma| + \mathbf{E}\left[\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)\right]\\
&=& \frac{n}{2}\log (2\pi e) +\frac{1}{2} \log|\Sigma|\\
\end{eqnarray*}

Now, we would like to show that 

\begin{equation*}
|\alpha\Sigma_{1} + (1-\alpha)\Sigma_{2}| \leq |\Sigma_{1}|^{\alpha} \cdot |\Sigma_{2}|^{1-\alpha}
\end{equation*}

\end{s}

\begin{p}
Chernoff bound for read-k families
\end{p}

\begin{s}
We can modify Shearer's Lemma for the scenario where we are dealing with subsets of $\it{at \;most}$ size $k$.

\begin{equation*}
k\cdot H(X_{1} \; ... \; X_{n}) \geq \sum_{S\in \mathcal{F}} H(X_{S})
\end{equation*}

where $\mathcal{F}$ is a collection of subsets  $S$ of $[n]$ which are at most of length $k$ and $X_{S}$ is a particular subset. Consider the following modification

\begin{equation*}
k\cdot H(X_{1} \; ... \; X_{n}) - k\cdot H(Y_{1} \; ... \; Y_{n})\geq  \sum_{S\in \mathcal{F}} H(X_{S}) - k\cdot H(Y_{1} \; ... \; Y_{n}) 
\end{equation*}

since $X$ is uniformly distributed we have that

\begin{align*}
k\cdot D(Y_{1} \; ... \; Y_{n}||X_{1} \; ... \; X_{n}) &\geq \sum_{S\in \mathcal{F}} H(X_{S}) - k\cdot H(Y_{1} \; ... \; Y_{n}) \\
&= \sum_{S\in \mathcal{F}} H(X_{S})  - k\cdot H(Y_{1} \; ... \; Y_{n}) \\
&= k\sum H(X)  - k\cdot H(Y_{1} \; ... \; Y_{n}) \\
&\geq k\sum H(X)  - \sum H(Y_{S})\\
&= D(Y_{S}||X_{S}) 
\end{align*}

We can see why this last equality is true by using the fact that $X$ is uniformly distributed and each $X$ is independent

\begin{equation*}
D(Y_{S}||X_{S})  = \sum H(X_{S}) - H(Y_{S}) = k\sum H(X) - \sum H(Y_{S})
\end{equation*}

Since both are uniformly distributed, we can calculate the ratio of the number of subsets where the condition is satisfied to the total number of possible subsets. This can be written in terms of entropies

\begin{align*}
\mathbb{P}_{X_{1}\; ... \; X_{n}} \left[\sum f_{i}(X_{S}) \leq t\right] &= \frac{|A|}{|X|} \\
&= 2^{H(Y_{1} \; ... \; Y_{n}) - H(X_{1} \; ... \; X_{n})}\\
&= 2^{-D(Y_{1} \; ... \; Y_{n}||X_{1} \; ... \; X_{n})}
\end{align*}

Since the distributions are uniform. Now, 

\begin{align*}
D(Y_{S_{i}}||X_{S_{i}}) &\geq D(f(Y_{S_{i}})||f(X_{S_{i}})) \\
&= \sum f(Y_{S_{i}})\log\frac{f(Y_{S_{i}})}{f(X_{S_{i}})}\\
&= \nu_{i}\frac{\nu_{i}}{\mu_{i}} + (1-\nu_{i})\frac{1-\nu_{i}}{1-\mu_{i}}\\
&= D(\nu_{i}||\mu_{i})
\end{align*}


\end{s}

\end{document}