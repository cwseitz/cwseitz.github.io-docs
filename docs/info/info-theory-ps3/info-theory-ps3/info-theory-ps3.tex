\documentclass[12pt]{article}
\usepackage{amsmath} % AMS Math Package
\usepackage{amsthm} % Theorem Formatting
\usepackage{amssymb}    % Math symbols such as \mathbb
\usepackage{graphicx} % Allows for eps images
\usepackage[dvips,letterpaper,margin=1in,bottom=0.7in]{geometry}
\usepackage{amsmath}


\newtheorem{p}{Problem}[section]
\usepackage{cancel}
\newtheorem*{lem}{Lemma}
\theoremstyle{definition}
\newtheorem*{dfn}{Definition}
 \newenvironment{s}{%\small%
        \begin{trivlist} \item \textbf{Solution}. }{%
            \hspace*{\fill} $\blacksquare$\end{trivlist}}%


\begin{document}

{\noindent\Huge\bf  \\[0.5\baselineskip] {\fontfamily{cmr}\selectfont  Problem Set 3}         }\\[2\baselineskip] % Title
{ {\bf \fontfamily{cmr}\selectfont Information and Coding Theory}\\ {\textit{\fontfamily{cmr}\selectfont     February 26, 2021}}}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    {\large \textsc{Clayton Seitz}
\\[1.4\baselineskip] 

\begin{p}
A single dice is rolled and we gain a dollar if the outcome is 2,3,4,5 and lose a dollar if the outcome is 1 or 6. Find the expected gain and the maximum entropy distribution over the possible outcomes of a roll.
\end{p}

\begin{s}

Let $Q$ be the uniform distribution over the dice universe $\chi =\left\{1,2,3,4,5,6\right\}$ where an outcome of a roll is $x \in \chi$. Furthermore, let $\phi(x)$ be the gain given the outcome of a roll $x$ according the problem definition

\[\phi(x) = \begin{cases} 
      1 & \; x \in \left\{2,3,4,5\right\}\\
      -1 & \; x \in \left\{1,6\right\}\\
   \end{cases}
\]

and $\bar{x}\sim Q^{n}$ be a draw of a sequence of $n$ rolls from the product distribution $Q^{n}$. We can then calculate the expected gain over $n$ rolls where $\bar{x} = \left\{x_{1} \;...\; x_{n}\right\}$ denotes the outcome of a sequence of rolls

\begin{align*}
\underset{\bar{x} \sim Q^{n}}{\mathbf{E}}\left[\phi(\bar{x})\right] &= \sum_{n}\left(\underset{x_{n} \sim Q}{\mathbf{E}}\left[\phi(x_{n})\right] \right)\\
&= n\cdot \left(\sum_{x\in \chi} \phi(x)\cdot q(x)\right)\\
&=n\cdot \left(\frac{1}{6}\sum_{x\in \chi}\phi(x)\right)\\
&= \frac{n}{3}
\end{align*}

Now, we would like to find the maximum entropy distribution $P^{*}$ over support $\chi$ such that

\begin{equation*}
\underset{\bar{x} \sim (P^{*})^{n}}{\mathbf E}\left[\phi(\bar{x})\right] > \alpha
\end{equation*}

where $\alpha =  \frac{n}{3}$. We begin by defining the linear family of distributions that satisfy this constraint on the expected gain

\begin{equation*}
\mathcal{L} = \left\{ P : \underset{\bar{x} \sim P^{n}}{\mathbf E}\left[\phi(\bar{x})\right] = 
n\cdot\left(\sum_{x\in \chi} \phi(x)\cdot p(x)\right) > \alpha \right\}
\end{equation*}

We would like to find the distribution $P^{*}$ such that $P^{*} = \mathbf{Proj}_{\mathcal{L}}(Q)$. To determine $P^{*}$, we include two constraints in the Lagrangian weighted by coefficients $\lambda_{0},\lambda_{1}\in \mathbb{R}$. For a particular distribution $P \in \mathcal{L}$ we have 

\begin{align*}
\mathbf{\Lambda}(P, \lambda_{0}, \lambda_{1}) &= D(P||Q) + \lambda_{0}\cdot \left(\sum_{x} p(x) - 1\right) + \lambda_{1}\cdot (\alpha-\Phi)\\
&= \sum_{x\in \chi} \left(p(x)\log\frac{p(x)}{q(x)} + \lambda_{0}\cdot \left(p(x) - \frac{1}{d}\right) + \lambda_{1}\cdot \left(\frac{\alpha}{d}-\phi(x)\right)\right)\\
\end{align*}

where we have let $\Phi = \sum_{x \in \chi} \phi(x)\cdot p(x)$ denote the expected gain and defined a linear penalty for deviation away from $\alpha$ that subtracts from the Lagrangian for $\Phi > \alpha$. We find the extremum by setting the gradient of this Lagrangian with respect to $p(x)$ to zero for every $x\in \chi$

\begin{equation*}
\log\left(\frac{p^{*}(x)}{q(x)}\right) + \frac{1}{\ln 2} + \lambda_{0} -\lambda_{1}\cdot \phi(x) = 0
\end{equation*}

Therefore, the optimal distribution can be found from $P$ by computing the following for each $x\in \chi$

\begin{equation*}
p^{*}(x) = q(x)\cdot 2^{\; \lambda_{1}\;\cdot\; \phi(x) - \lambda_{0}}
\end{equation*}

\end{s}

\begin{p}
Exponential families and maximum entropy
\end{p}

\begin{s}

\begin{align*}
H(Q) &= -\sum_{x\sim \chi} Q(x)\log\exp\left\{\lambda_{0} + \sum_{i\sim [k]} \lambda_{i}f_{i}(x)\right\}\\ 
&= -\frac{1}{\ln 2}\sum_{x\sim \chi} Q(x)\left\{\lambda_{0} + \sum_{i\sim [k]} \lambda_{i}f_{i}(x)\right\}\\
&= -\frac{1}{\ln 2}\left(\lambda_{0} + \sum_{x\sim \chi} Q(x)\left\{\sum_{i\sim [k]} \lambda_{i}f_{i}(x)\right\}\right)\\
&= -\frac{1}{\ln 2}\left(\lambda_{0} + \sum_{i\sim [k]} \lambda_{i}\alpha_{i}\right)
\end{align*}

Now we will show that the KL-Divergence is simply the difference of the entropies of $Q$ and $P$

\begin{align*}
D(P||Q) &= \sum_{x\sim \chi} p(x)\log\frac{p(x)}{q(x)}\\
&= -\frac{1}{\ln 2}\sum_{x\sim \chi} p(x)\left\{\lambda_{0} + \sum_{i\sim [k]} \lambda_{i}f_{i}(x)\right\} - H(P)\\
&= -\frac{1}{\ln 2}\left(\lambda_{0} + \sum_{i\sim [k]} \lambda_{i}\alpha_{i}\right) - H(P)\\
&= H(Q) - H(P)
\end{align*}

using the result from above. Finally, using the fundamental lower bound on the KL-Divergence we can show that $Q$ is the maximum entropy distribution in the family $\mathcal{L}$

\begin{align*}
D(P||Q) &=  H(Q) - H(P) \geq 0
\end{align*}

which requires that $H(Q) \geq H(P)$.

\end{s}


\begin{p}
Minimax rates for denoising
\end{p}

\begin{s}
\\
\\
We will start by writing out the form of the KL-Divergence between the joint distributions over $(X,Y)$ denoted as $P_{f},P_{g}\in \Pi$ for a set of so far undefined functions $\Pi$. Using the chain rule for KL-Divergence,

\begin{align*}
D(P(X,Y)||Q(X,Y)) &= D(P(X)||Q(X)) + D(P(Y|X)||Q(Y|X))\\
&= D(P(Y|X)||Q(Y|X))\\
&= D(\mathcal{N}(f(x),\sigma^{2})||\mathcal{N}(g(x),\sigma^{2}))\\
\end{align*}

which is the KL-Divergence between two Gaussians. This can be computed as follows for the special case that $\sigma$ is the same for both distributions

\begin{align*}
D(\mathcal{N}(f(x),\sigma^{2})||\mathcal{N}(g(x),\sigma^{2})) &= 
\frac{1}{\ln 2}\int_{0}^{1} \exp\left(-(x-f(x))^{2}/2\sigma\right)\\ &\cdot \ln \left(\frac{\exp\left(-(x-f(x))^{2}/2\sigma\right)}{\exp\left(-(x-g(x))^{2}/2\sigma\right)}\right)dx\\
&= \frac{1}{2\ln 2\cdot \sigma^{2}}\int_{0}^{1} \exp\left(-(x-f(x))^{2}\right)\\ &\cdot \left((x-g(x))^{2}-(x-f(x))^{2}\right)dx\\
&= \frac{1}{2\ln 2\cdot \sigma^{2}}\int_{0}^{1} |f(x)-g(x)|^{2}dx\\
&= \frac{1}{2\ln 2\cdot \sigma^{2}}\cdot ||f(x)-g(x)||_{2}^{2}\\
\end{align*}

Next we will prove a lower bound on the minimax loss for $n$ samples when we have a set of functions $S$. s.t. $f,g \in S$. First, we can manipulate the result from above and show that

\begin{equation*}
D(P_{f}||P_{g}) = \frac{1}{2\ln 2\cdot \sigma^{2}}\cdot ||f(x)-g(x)||_{2}^{2} \leq \frac{32\delta^{2}}{\ln 2\cdot \sigma^{2}}
\end{equation*}

since we have said that $||f-g||_{2}^{2} \leq 8\delta$. At the same time, we can use what we know about lower bounds for minimax rates via multiple hypotheses. Let's say that a sequence of $(X,Y)$ pairs $\bar{z}$ is drawn from the joint distribution $P_{s}$ where $s \in S$ and a tester $T$ determines which $P_{s}$ the data was drawn from (which amounts to denoising the data by determining the function $s$ that generated it). The probability of error by $T$ that outputs the distribution $s\in S$ the data was drawn from is lower bounded by

\begin{equation*}
\mathbf{Pr}\left[T(\bar{z}) \neq s]\right] \geq 1 - \frac{n\cdot \underset{s_{1},s_{2}\in S}{\mathbf{E}}\left[D(P_{s_{1}}||P_{s_{2}})\right] + 1}{\log |S|}
\end{equation*}

Also, we define the loss function to be $\ell = ||f-g||_{2}^{2}$

\begin{align*}
M_{n}(\Pi, \ell) &\geq \ell(\delta)\cdot \underset{T}{\inf}\left\{\mathbf{Pr}\left[T(\bar{x}) \neq s]\right]\right\}\\
&= \delta^{2}\cdot\left(1 - \frac{n\cdot \underset{s_{1},s_{2}\in S}{\mathbf{E}}\left[D(P_{s_{1}}||P_{s_{2}})\right] + 1}{\log |S|}\right)\\
&= \delta^{2}\cdot\left(1 - \frac{n\cdot \left(32\delta^{2}/\sigma^{2}\ln 2\right) + 1}{\log |S|}\right)
\end{align*}

where we substituted the result from above since the expectation over all possible pairs of functions can be at most the largest distance between two functions. Now, we define the set of functions $S$ to be a series of non-intersecting bump functions which are $L$-Lipschitz since

\begin{align*}
|B_{\epsilon}(x_{1}) - B_{\epsilon}(x_{2})| &= L\cdot ||x_{2}|-|x_{1}||\\
&\leq L\cdot |x_{2}-x_{1}|
\end{align*}

and if we make the assumption that $\epsilon < 1$

\begin{align*}
\int_{-1}^{1}(B_{\epsilon}(x))^{2}dx &= \int_{-\epsilon}^{\epsilon}(B_{\epsilon}(x))^{2}dx\\
&= 2\int_{0}^{\epsilon} (L\cdot (\epsilon-|x|)^{2}dx\\
&= 2L^{2}\int_{0}^{\epsilon} (x-\epsilon)^{2}dx\\
&= \frac{2L^{2}\epsilon^{3}}{3}
\end{align*}

Next, we will show that the squared $L_{2}$ distance can be written in terms of the Hamming distance between the two position vectors

\begin{align*}
||f_{a}-f_{b}||_{2}^{2} &= \int_{-1}^{1} |f_{a}(x)-f_{b}(x)|^{2}dx\\
&= \int_{-1}^{1} |\sum_{i=1}^{m}(a_{i}-b_{i})B_{\epsilon}(x-z_{i})|^{2}dx\\
&= \int_{-1}^{1} |\sum_{a_{i}\neq b_{i}}B_{\epsilon}(x-z_{i})|^{2}dx\\
&= \sum_{a_{i}\neq b_{i}}\int_{-1}^{1} \left(B_{\epsilon}(x-z_{i})\right)^{2}dx\\
&= \frac{2L^{2}\epsilon^{3}}{3}d_{H}(a,b)
\end{align*}

Finally, there exists a constant $c_{0}$ such that

\begin{equation*}
\mathcal{M}_{n}(\Pi, \ell) \geq c_{0}\cdot \left(\frac{\sigma^{2}\cdot L}{n}\right)^{2/3}
\end{equation*}

\end{s}

\end{document}