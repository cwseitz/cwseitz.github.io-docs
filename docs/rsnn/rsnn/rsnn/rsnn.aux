\relax 
\@writefile{toc}{\contentsline {chapter}{Abstract}{v}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Towards a theory of spatiotemporal correlation transfer in plastic neuronal networks}{1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Literature Review and Theoretical Foundations}{4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces (a) Diagram of a recurrent neural network receving a time-dependent feedforward stimulus. (b) Diagram of the stochastic encoding scheme used a recurrent neural network with noise\relax }}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}The mean-field approximation}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}The diffusion approximation and linear response}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Reshaping network structure with synaptic plasticity}{17}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Computational graph describing the update of synaptic weights $J_{ij}$ and the cross-covariance matrix $C(\tau )$ over multiple stimulus blocks of length $\Delta T_{\mathrm  {stim}}$. The learning period $\Delta T_{\mathrm  {lrn}}<< T_{\mathrm  {stim}}$ determines the frequency of learning updates within the period $\Delta T_{\mathrm  {stim}}$\relax }}{25}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Spatial properties of network organizatioon and plasticity}{26}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Results}{27}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Demonstration of the balanced state}{28}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}A spatiotemporal plasticity rule}{28}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}A spatially extended network}{29}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Methods}{29}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Monte-Carlo simulations}{29}\protected@file@percent }
\newlabel{RF1}{30}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces \relax }}{30}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces \relax }}{31}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces \textbf  {Average degree in a spatially-extended E-I network} (A) Schematic illustrating the notation used for excitatory-excitatory, excitatory-inhibitory, inhibitory-excitatory, and inhibitory-inhibitory synapses. (B) An example dense excitatory-inihbitory network ($\Gamma _{0}=0.2$) and $p_{E} = 0.8$. (C) Average number of synapses in the dense network normalized to the target population size as a function of parameters $\sigma _{E}$ and $\sigma _{I}$. (D) An example sparse excitatory-inihbitory network ($\Gamma _{0}=0.8$) and $p_{E} = 0.8$. (E) Average number of synapses in the sparse network normalized to the target population size as a function of parameters $\sigma _{E}$ and $\sigma _{I}$.\relax }}{32}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces EIF balanced network parameters\relax }}{34}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}The threshold integration method}{35}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}Generating spatially extended networks}{35}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces \textbf  {Synapse probabilities for gaussian connectivity} (a) Binomial probabilities for two identical gaussian kernels ($\sigma =1$) separated by a distance $|\Delta \mathbf  {r}_{ij}|$ and the probability of no synapse $\Gamma _{0}$ (left) and the corresponding multinomial probabilities (right). (b) Binomial probabilities for two different gaussian kernels ($\sigma _{1}=1, \sigma _{2}=2$) separated by a distance $|\Delta \mathbf  {r}_{ij}|$ and the probability of no synapse $\Gamma _{0}$ (left) and the corresponding multinomial probabilities (right). \relax }}{36}\protected@file@percent }
\bibdata{ft2,master,refs}
\bibstyle{plain}
\@writefile{toc}{\contentsline {chapter}{References}{39}\protected@file@percent }
