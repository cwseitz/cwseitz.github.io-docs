\relax 
\@writefile{toc}{\contentsline {chapter}{Acknowledgments}{v}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Abstract}{vi}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Spatiotemporal correlation transfer in plastic excitatatory-inhibitory neuronal networks}{1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Literature Review}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}The diffusion approximation}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Beyond the diffusion approximation}{13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Reshaping network structure with synaptic plasticity}{15}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces (a) Diagram of a recurrent neural network receving a time-dependent feedforward stimulus. (b) Diagram of the stochastic encoding scheme used a recurrent neural network with noise\relax }}{16}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Computational graph describing the update of synaptic weights $J_{ij}$ and the cross-covariance matrix $C(\tau )$ over multiple stimulus blocks of length $\Delta T_{\mathrm  {stim}}$. The learning period $\Delta T_{\mathrm  {lrn}}<< T_{\mathrm  {stim}}$ determines the frequency of learning updates within the period $\Delta T_{\mathrm  {stim}}$\relax }}{24}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Reflecting on the interpretation of network dynamics}{25}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Results}{26}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}The two-dimensional Gaussian network}{26}\protected@file@percent }
\newlabel{RF1}{27}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces \relax }}{27}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces \relax }}{28}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces \relax }}{29}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Computational graph describing the update of synaptic weights $J_{ij}$ and the cross-covariance matrix $C(\tau )$ over multiple stimulus periods of length $\Delta T_{\mathrm  {stim}}$. The learning period $\Delta T_{\mathrm  {lrn}}<< T_{\mathrm  {stim}}$ determines the frequency of learning updates within the period $\Delta T_{\mathrm  {stim}}$\relax }}{30}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces \textbf  {Synapse probabilities for gaussian connectivity} (a) Binomial probabilities for two identical gaussian kernels ($\sigma =1$) separated by a distance $|\Delta \mathbf  {r}_{ij}|$ and the probability of no synapse $\Gamma _{0}$ (left) and the corresponding multinomial probabilities (right). (b) Binomial probabilities for two different gaussian kernels ($\sigma _{1}=1, \sigma _{2}=2$) separated by a distance $|\Delta \mathbf  {r}_{ij}|$ and the probability of no synapse $\Gamma _{0}$ (left) and the corresponding multinomial probabilities (right). \relax }}{31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Homogeneous Gaussian networks}{31}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces \textbf  {The homogeneous Gaussian network}. (A) An example homogeneous network containing $N=100$ neurons. (B) An example neuron extracted from (A) with outgoing synapses labeled in blue and incoming synapses labeled in red. (C,D) The ratio $\langle N_{ij}\rangle /N$ as a function parameters ($\sigma , \Gamma _{0}$) for a sparse network (C) and a network with variable sparsity (D). (E,F) The ratio $\langle N_{ij}\rangle /N$ for fixed $\sigma $ and variable sparsity. (G,H) Binomial probability maps for two nearby neurons expressed as a sum $p_{ij}+p_{ji}$ and product $p_{ij}p_{ji}$\relax }}{32}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Excitatory-inhibitory Gaussian networks}{33}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces \textbf  {Average degree in an excitatory-inhibitory network} (A) Schematic illustrating the notation used for excitatory-excitatory, excitatory-inhibitory, inhibitory-excitatory, and inhibitory-inhibitory synapses. (B) An example dense excitatory-inihbitory network ($\Gamma _{0}=0.2$) and $p_{E} = 0.8$. (C) Average number of synapses in the dense network normalized to the target population size as a function of parameters $\sigma _{E}$ and $\sigma _{I}$. (D) An example sparse excitatory-inihbitory network ($\Gamma _{0}=0.8$) and $p_{E} = 0.8$. (E) Average number of synapses in the sparse network normalized to the target population size as a function of parameters $\sigma _{E}$ and $\sigma _{I}$.\relax }}{35}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Methods}{36}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Signal processing techniques}{36}\protected@file@percent }
\bibdata{ft2,master,refs}
\bibstyle{plain}
\@writefile{toc}{\contentsline {chapter}{References}{39}\protected@file@percent }
