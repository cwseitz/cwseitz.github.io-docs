% ---- ETD Document Class and Useful Packages ---- %
\documentclass{ucetd}
\usepackage{subfigure,epsfig,amsfonts}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage{url}


%% Use these commands to set biographic information for the title page:
\title{Stochastic computation in recurrent networks of spiking neurons}
\author{Clayton W. Seitz}
\department{Graduate Program in Biophysics}
\division{Physical and Biological Sciences}
\degree{Master of Science}
\date{Winter 2021}

%% Use these commands to set a dedication and epigraph text

\epigraph{Epigraph}

\begin{document}
%% Basic setup commands
% If you don't want a title page comment out the next line and uncomment the line after it:
\maketitle
%\omittitle

% These lines can be commented out to disable the copyright/dedication/epigraph pages
\makecopyright
%\makededication
\makeepigraph


%% Make the various tables of contents
\tableofcontents
%\listoffigures
%\listoftables

\acknowledgments
% Enter Acknowledgements here

\abstract

One of the most challenging questions in modern neuroscience is the mechanisms by which primate cortical circuits learn from example and perform inference based upon their past experience. The cerebral cortex is responsible for many higher-order brain functions including memory and . The search for principles of neural computation common to these phenomena is made extremely difficult by the diversity of cortical structures and their putative functions. Nevertheless, progress has been made in our understanding of the interplay between the organization of cortical neurons, network dynamics, and the resulting reorganization via synaptic plasticity during learning. Many deficiencies in our models of these processes can be attributed to our lack of a mathematical understanding of both the fundamental computational paradigm implemented by neural circuits and the roles of synaptic plasticity mechanisms observed \emph{in vivo}. These problems are further aggravated by the apparent stochastic features of the firing activity of neurons and experimentally observed trial-to-trial variability in population recordings. However, there is, in principle, a very powerful framework for stochastic computation evident in the Boltzmann machine: probabilistic inference by sampling. Here, we implement a recurrent spiking neural network (RSNN) as a variant of the Boltzmann machine and propose a precise mathematical role for a set of synaptic plasticity mechanisms in the formation of a set of synaptic weights that successfully ``embody" a Gaussian input statistic.


\mainmatter

\chapter{Introduction}

Recent decades have yielded a number of experimental and theoretical advances in our understanding of learning in biological neural networks through the introduction of as homosynaptic, heterosynaptic, and homeostatic plasticity. However, the precise mathematical relationship between the synaptic plasticity mechanisms or \emph{learning rules} implemented by neurons and the formation of probabilistic models of their inputs, remains elusive.

Neural network models date back to McCulloch and Pitts [1] followed later by the Hopfield network [2] which spawned an entire field of research applying techniques from the statistical physics of spin glasses to the description of neural activity. By simplifying a network of neurons firing action potentials to an ensemble of coupled spins $\sigma \in \{-1, +1\}$, the Hopfield model related the patterns learned by a network to the energy landscape over the discrete space of states. The storage capacity of these networks and the geometry of this energy landscape were of particular interest and rigorous mathematical treatment has been used to show limits on the basins of attraction in such energy landscapes [4,10,11]. However, recent experimental evidence has suggested that networks of neurons may follow a stochastic, rather than deterministic, computational paradigm. There are many examples in the literature of trial-to-trial variability in the response of cortical neurons to identical stimuli, suggesting that computations in the brain are inherently stochastic. The origins of this stochasticity are hypothesized to originate in noise in synaptic conductances and infidelity in processes clearing neurotransmitters from the synaptic cleft. Interestingly, an extension of the Hopfield model - the Boltzmann machine actually leverages stochastic activity of Ising spins to perform powerful computations [14]. In such a model, the set of synaptic weights $\Phi$ ``embody" the joint distribution $P_{\Phi}(X,R) = P_{\Phi}(X)P_{\Phi}(R|X)$ over network inputs and network response, respectively. Then, computations can be viewed as probabilistic inference after suitable transformations of the weights $\Delta\Phi$. Such physically inspired models have proven useful; however, the models are sufficiently abstract to make experimental comparisons difficult. More recent endeavors appear to make useful predictions on network dynamics by using Fokker-Planck equations to compute distributions of the membrane voltage as a function of time [5]. Here, we apply a similar formalism to the description of network dynamics with heterogeneous and stochastic synaptic weights to probe the distribution over network states $P_{\Phi}(R)$ and describe how this framework can be used to provide insights on how the neural networks embody the distribution $P_{\Phi}(R|X)$ in statistical inference tasks. To conclude, we discuss the effect of synaptic plasticity.

By solving the Fokker-Planck equations numerically for the time evolution of the distribution over membrane voltages $P_{\Phi}(V,t)$ simulataneously permits knowledge of the distribution $P_{\Phi}(\mathbf{z},t)$. To see this, consider thresholding a sample from the joint distribution $\mathbf{z} \sim \Theta\left(\mathbf{v}\right)$ where $\mathbf{v} \sim (P_{\Phi}(V,t))^{N}$. We might then ask how local synaptic plasticity rules permit the formation of cell assemblies which collectively encode stimuli.

\chapter{Statistical mechanics of a population of neurons}

In this section we make use of Fokker-Planck equations from statistical mechanics in the description of neural dynamics. A natural example is to describe the voltage dynamics of a single leaky integrate and fire (LIF) neuron that receives $N$ feedforward Poisson input spike trains. The formalism presented can then be generalized to a population of LIF neurons.\\

\section{Formalism}

We consider two main populations of neurons: an external population and a recurrent population. The input neurons are described only by their spiking activity and are symbolized by the binary vector $\mathbf{s}(t)\in \mathbb{F}_{2}^{n}$. Furthermore, we describe the \emph{state} variables of the recurrent population by a vector of voltages $\mathbf{v}(t)\in \mathbb{R}^{n\times 1}$ and a vector of so-called observable states denoted by $\mathbf{z}(t) = \Theta(\mathbf{v}(t))$ where $\Theta(v(t)) = H(v(t) - \theta)$ and $H$ is the Heaviside step function. Input neurons communicate to the recurrent population through excitatory connections summarized by the input weight matrix $\mathbf{X}\in \mathbb{R}^{n\times m}$. Similarly, a recurrent weight matrix $\mathbf{J}\in \mathbb{R}^{n\times n}$ specifies the recurrent network connectivity. The matrix $\mathbf{J}$ may contain both excitatory and inhibitory connections. Therefore, it will prove useful to partition $\mathbf{X}$ and $\mathbf{J}$ into four quadrants as follows

\begin{align}
\mathbf{X} = \begin{bmatrix}
    X_{E}\\
    X_{I} 
\end{bmatrix}  && \mathbf{J} = \begin{bmatrix}
    J_{EE} & J_{IE} \\
    J_{EI} & J_{II} 
\end{bmatrix} 
\end{align}

and an appropriate partitioning scheme is applied to the network state variables

\begin{align}
\mathbf{v} = \begin{bmatrix}
    v_{E}\\
    v_{I} 
\end{bmatrix}  && \mathbf{z} = \begin{bmatrix}
    z_{E}\\
    z_{I} 
\end{bmatrix} 
\end{align}

which are defined such that, for example, the inputs from the external and recurrent population to an excitatory neuron $j \in \{1, 2, ..., n'-1, n'\}$ with $n' \leq n$ is simply $\xi_{j}(t) = J_{EE}z_{E} + J_{IE}z_{I}$. More generally, the voltage state variable for an arbitrary neuron evolves according to the update equation for the voltage 

\begin{align*}
v_{j}(t+\Delta t) &= v_{j}(t)\exp\left(-\frac{\Delta t}{\tau}\right) + \xi_{j}(t)\\
&= v_{j}(t)\exp\left(-\frac{\Delta t}{\tau}\right) + \sum_{k\neq \ell}X_{k\ell}s_{k}(t) + \sum_{i\neq j} J_{ij}z_{i}(t)
\end{align*}

Letting the leakage term $\alpha = \exp\left(-\frac{\Delta t}{\tau}\right)$ this can be written for the entire population as $\mathbf{v}(t) = \alpha\mathbf{v}(t) + \mathbf{X}(t)\cdot \mathbf{s} + \mathbf{J}(t)\cdot \mathbf{z}$ where $\cdot$ denotes standard matrix multiplication. 

The input spike train $\mathbf{s}_{k}(t)$ of neuron $k$ is taken to be a heterogeneous Poisson process with firing rate $\nu_{k}(t)$. Given the nature of such a stochastic process, a fundamental description of network dynamics demands knowledge of the time evolution of the voltage distribution $P(v,t)$ over the population, which is the focus of the following sections.

\section{A single leaky integrate and fire neuron}

It is fundamentally intractable to predict the time evolution of the membrane potential in a single LIF neuron embedded in a complex network - we can only speak of probability densities over the space of membrane voltages $P(v)$. The problem is further aggravated by the fact that $P(v)$ is often not stationary and we therefore must predict how the distribution evolves in time $P(v,t)$. This situation bears many similiarities to a situation often encountered the statistical physics of systems far from equilibrium where the task at hand is to predict the time evolution of the distribution over states e.g., particle velocities, and the stationary distribution at thermal equilibrium. If the spike trains input to the LIF neuron follow an inhomogeneous Poisson process i.e. we assume every sequence of spikes has equal probability, the probability of observing $n$ spikes in a time step $\Delta t$ is given by

\begin{equation*}
P[X=n] = \frac{(\nu\Delta t)^{n}}{n!}\exp\left(-\nu\Delta t\right)
\end{equation*}

Under this assumption, the membrane voltage follows a Markov process and we can define the following transition operator

\begin{align*}
T(v[t+1]\;|\;v[t]) &= \left(1-\Delta t\sum_{k}\nu[t]\right)\delta\left(v[t]-\alpha\cdot v[t]\right)\\
&+ \Delta t\sum_{k}\nu_{k}[t]\cdot\delta\left(v[t]-\alpha \cdot v[t] + w_{k}\right)
\end{align*}

with $\alpha = \exp\left(-\frac{\Delta t}{\tau_{m}}\right)$ and a membrane time constant $\tau_{m}$. This means that the probability of arriving at a voltage $v$ is the integral

\begin{align*}
P(v,t+\Delta t) &= \int T(v[t+\Delta t]\;|\;v[t])P(v[t])dv\\
&= \int \left(1-\Delta t\sum_{k}\nu[t]\right)\delta\left(v[t]-\alpha\cdot v[t]\right)dv\\
&+ \int \Delta t\sum_{k}\nu_{k}[t]\cdot\delta\left(v[t]-\alpha \cdot v[t] + w_{k}\right)P(v[t])dv\\
&= \left(1-\Delta t\sum_{k}\nu[t]\right)\alpha^{-1}P(\alpha^{-1}v,t) + \Delta t \sum_{k}\nu[t] \alpha^{-1}P(\alpha^{-1}v-w_{k},t)
\end{align*}

The interpetation of the left term is that a fraction of the probability mass at a voltage $\alpha^{-1}v$ moves to $v$ according to the probability that no input neurons fire. The right term then adds in probability mass at points $\alpha^{-1}v -w_{k}$ depending on the firing rate of input neuron $k$, in the case of nonzero input. In practice, we consider the voltage to be discrete, with a resolution equal to $\Delta v  = \alpha$.

\section{A network of leaky integrate and fire neurons}

We would like to obtain the joint distribution over membrane voltages and its time evolution $P(v_{1}, v_{2}, ... , v_{n}, t)$. The transition operator for a neuron $j$ is given by 


\begin{align*}
T(v[t+1]\;|\;v[t]) &= \left(1-\Delta t\sum_{k}\nu[t]\right)\delta\left(v[t]-\alpha\cdot v[t]\right)\\
&+ \Delta t\sum_{k}\nu_{k}[t]\cdot\delta\left(v[t]-\alpha \cdot v[t] + w_{k}\right)\\
&+ \sum_{k} \mathrm{erfc}_{\theta}\left(P_{k}(v, t)\right)\delta\left(v[t]-\alpha \cdot v[t] + w_{k}\right) + \mathrm{erfc}_{\theta}\left(P(v,t)\right)\delta\left(0\right)
\end{align*}

The above transition operator has four terms: two for the input from an external population, one for the input from the recurrent population, and one that enforces the spiking threshold $\theta$. From this we can see that the probability mass that reaches the threshold $\theta$ for a neuron $k$ is distributed across other dimensions of the $N$ dimensional space according to the weight between those dimensions. Also, for the neuron under consideration, that probability mass passes through the threshold $\theta$ back to the resting potential, according to periodic boundary conditions. An analagous process is a particle undergoing diffusion in $N$ dimensions with periodic boundary conditions for each dimension. When the particle reaches the boundary along a dimension it receives a `kick' along a direction $\vec{\Delta v} = \sum_{k}w_{k}\hat{k}$ 


\chapter{Dynamical states of recurrent networks}
\section{Introduction}


\chapter{Stochastic computation by recurrent networks}
\section{Introduction}

\chapter{Conclusions}

% Intro to chapter one

% Format a LaTeX bibliography
\makebibliography

[1] Mculloch and Pitts. \textit{A logical calculus of the ideas imminent in nervous activity}. Journal of Mathematical Biophysics. 1943.

[2] J.J. Hopfield. \textit{Neural networks and physical systems with emergent collective computational abilities}. 1982.

[3] D.J. Amit. \textit{Spin-glass models of neural networks}. Physical Rev A. 1985.

[4] E. Gardner. \textit{The space of interactions in neural network models}. Journal of Physics A: Mathematical and General. 1988.

[5] N. Brunel. \textit{Dynamics of sparsely connected networks of excitatory and inhibitory neurons}. Journal of Computational Neuroscience. 2000. 

[6] Rosenbaum and Doiron. \textit{Balanced Networks of Spiking Neurons with Spatially Dependent Recurrent Connections}. Physical Review X. 2014.

[7] Zenke et al. \textit{Diverse synaptic plasticity mechanisms
orchestrated to form and retrieve memories
in spiking neural networks}. Nature Communications. 2015.

[8] Williams et al. \textit{Nonnegative decomposition of multivariate information}. arXiv. 2010.

[9] Amit et al. \textit{Storing infinite numbers of patterns in a spin-glass model of neural networks}. Physical Rev. Letters. 1985

[10] Nishimori. \textit{Statistical physics of spin glasses and information processing: an introduction}. Clarendon Press. 2010

[11] Buesing et al. \textit{Neural Dynamics as Sampling: A Model for Stochastic Computation in Recurrent Networks of Spiking Neurons}. PLOS Computational Biology. 2011

[12] Pecevski et al. \textit{Learning Probabilisitc Inference through Spike-Timing-Dependent Plasticitys}. eNeuro. 2016

[13] Pecevski et al. \textit{Formation and maintenance of neuronal assemblies through synaptic plasticity}. Nature Communications. 2014

[14] Pecevski et al. \textit{A learning algorithm for Boltzmann Machines}. Cognitive Science. 1985

% Figures and tables, if you decide to leave them to the end
%\input{figure}
%\input{table}

\end{document}


