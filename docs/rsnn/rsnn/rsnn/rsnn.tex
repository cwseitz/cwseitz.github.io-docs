% ---- ETD Document Class and Useful Packages ---- %
\documentclass{ucetd}
\usepackage{subfigure,epsfig,amsfonts}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage{url}


%% Use these commands to set biographic information for the title page:
\title{Stochastic computation in recurrent networks of spiking neurons}
\author{Clayton W. Seitz}
\department{Graduate Program in Biophysics}
\division{Physical and Biological Sciences}
\degree{Master of Science}
\date{Winter 2021}

%% Use these commands to set a dedication and epigraph text
%\dedication{I dedicate this thesis to my friends and family, who have pushed me to think critically and live a principled life}


\epigraph{Epigraph}

\begin{document}
%% Basic setup commands
% If you don't want a title page comment out the next line and uncomment the line after it:
\maketitle
%\omittitle

% These lines can be commented out to disable the copyright/dedication/epigraph pages
\makecopyright
%\makededication
\makeepigraph


%% Make the various tables of contents
\tableofcontents
%\listoffigures
%\listoftables

\acknowledgments
% Enter Acknowledgements here

\abstract

A central goal of modern neuroscience is the discovery of the mechanisms by which primate cortical circuits permit learning from example and performing inference from those experiences. The cerebral cortex is responsible for many higher-order brain functions including memory and learning; however, the search for principles of neural computation common to these phenomena is made extremely difficult by the diversity of cortical structures and their putative functions. Nevertheless, progress has been made in our understanding of the interplay between the organization of cortical neurons, network dynamics, and the reorganization via synaptic plasticity during learning. Many deficiencies in our models of these processes can be attributed to our lack of a mathematical understanding of both the fundamental computational paradigm implemented by neural circuits and the roles of synaptic plasticity mechanisms observed \emph{in vivo}. These problems are further aggravated by the apparent stochastic features of the firing activity of neurons and experimentally observed trial-to-trial variability in population recordings. However, there is, in principle, a very powerful framework for stochastic computation: probabilistic inference by sampling. Here, we implement a recurrent spiking neural network (RSNN) as a variant of the Boltzmann machine and propose a precise mathematical role for a set of synaptic plasticity mechanisms in the formation of a set of synaptic weights that successfully ``embody" a Gaussian input statistic.


\mainmatter

\chapter{Introduction}

Recent decades have yielded a number of experimental and theoretical advances in our understanding of learning in biological neural networks through the introduction of as homosynaptic, heterosynaptic, and homeostatic plasticity. However, the precise mathematical relationship between the synaptic plasticity mechanisms or \emph{learning rules} implemented by neurons and the formation of probabilistic models of their inputs, remains elusive.

Neural network models date back to McCulloch and Pitts [1] followed later by the Hopfield network [2] which spawned the application of techniques from the statistical physics of spin glasses to the description of neural activity. By simplifying a network of neurons firing action potentials to an ensemble of coupled spins $z \in \{-1, +1\}$, the Hopfield model related the patterns learnt by a network to an energy landscape over the discrete space of states. The storage capacity of these networks and the geometry of this energy landscape were of particular interest and rigorous mathematical treatment has been used to show limits on the density of basins of attraction in such energy landscapes [4,10,11]. However, a primary feature of these attractor networks, that is unlikely to be realized in neural circuitry, is that they are deterministic. There are many examples in the literature of trial-to-trial variability in the response of cortical neurons to identical stimuli, suggesting that computations in the brain are inherently stochastic []. Interestingly, the Boltzmann machine, which is an extension of the Hopfield model, actually leverages stochastic activity of Ising spins to perform powerful computations [14]. In such a model, the set of synaptic weights $\Phi$ ``embody" the joint distribution $P_{\Phi}(X,R) = P_{\Phi}(X)P_{\Phi}(R|X)$ over network inputs and network response, respectively. Then, computations can be viewed as probabilistic inference after suitable transformations of the weights $\Delta\Phi$. Our primary goal is to understand the relationship between a choice of the learning rule $\Delta \Phi$ and features of the learned distribution $P_{\Phi}(X,R)$.


\chapter{Stochastic Networks of Neurons: The Boltzmann Machine}
\section{Introduction}

This is the introduction to the section

\section{Glauber Dynamics}

In the simplest form of the Boltzmann machine, we consider an ensemble of recurrently connected point neurons which can be described by a binary variable $z_{i}(t) \in \{0, 1\}$ where $z_{i}(t) = 1$ if  neuron $i$ fired an action potential at time $t$ and $z_{i}(t) = 0$ otherwise. We will make use of the standard architecture of a Boltzmann machine with an input layer $X$ and a hidden layer $M$ with feedforward connections from $X$ to $M$ are allowed and 
recurrent connections from layer $M$ to itself. The probability that a neuron $i$ fires an action potenetial at a time $t + \Delta t$ is given by 

\begin{equation}
P(z_{i}(t+\Delta t) = 1) = \sigma(v_{i}(t)) = \frac{1}{1 + \exp(-v_{i}(t))}
\end{equation}

where $v_{i}(t)$ is taken to be the membrane voltage of the neuron $i$. We will begin with the assumption that neural dynamics form a Markov process we take $v_{i}(t)$ to be a sum over feedforward and recurrent inputs at a time $t$

\begin{equation}
v_{i}(t) = \sum_{j}X_{ij}z_{j}(t) + \sum_{k}M_{ik}z_{k}(t)
\end{equation}

which clearly has the Markov memoryless property: our defintion of $\mathbb{P}(z_{i}(t+\Delta t) = 1)$ in (1) dictates that the probability of a state transition $z_{i}(t) = 0 \rightarrow z_{i}(t+\Delta t) = 1$ is determined by a weighted sum over input $z_{j}(t)$ and recurrent $z_{k}(t)$ at a time $t$ only. The advantage of Glauber dynamics is that is that stationary distribution over the network state $\mathbf{z}$ can be predicted via the Boltzmann distribution

\begin{equation}
P[\mathbf{z}] = \frac{1}{Z}\exp\left(-E(\mathbf{z})\right)
\end{equation}


with $E(\mathbf{z}) = -\mathbf{X} \cdot \mathbf{z} - \frac{1}{2}\mathbf{z}\cdot \mathbf{M} \cdot \mathbf{z}$ and the partition function $Z = \sum \exp\left(-E(\mathbf{z})\right)$. This is a particularly useful property since we have expressed a direct relationship between the synaptic connectivity $\mathbf{X}$ and $\mathbf{M}$ and the probability distribution $P$ they implement. 

The input-output relationship of the network is described by the distribution $P_{\Phi}(\mathbf{z}|\mathbf{x})$ where we refer to $\Phi = \left(\mathbf{X}, \mathbf{M}\right)$ as `the model'. That is an input $x \sim P(\mathbf{x})$ has a stochastic relationship to the network response $\mathbf{z}$ implemented by the model $P_{\Phi}(\mathbf{z}|\mathbf{x})$ 


\chapter{Dynamical States of Recurrent Networks}
\section{Introduction}


\chapter{Biologically-Motivated Learning}
\section{Introduction}

\chapter{Conclusions}

% Intro to chapter one

% Format a LaTeX bibliography
\makebibliography

[1] Mculloch and Pitts. \textit{A logical calculus of the ideas imminent in nervous activity}. Journal of Mathematical Biophysics. 1943.

[2] J.J. Hopfield. \textit{Neural networks and physical systems with emergent collective computational abilities}. 1982.

[3] D.J. Amit. \textit{Spin-glass models of neural networks}. Physical Rev A. 1985.

[4] E. Gardner. \textit{The space of interactions in neural network models}. Journal of Physics A: Mathematical and General. 1988.

[5] N. Brunel. \textit{Dynamics of sparsely connected networks of excitatory and inhibitory neurons}. Journal of Computational Neuroscience. 2000. 

[6] Rosenbaum and Doiron. \textit{Balanced Networks of Spiking Neurons with Spatially Dependent Recurrent Connections}. Physical Review X. 2014.

[7] Zenke et al. \textit{Diverse synaptic plasticity mechanisms
orchestrated to form and retrieve memories
in spiking neural networks}. Nature Communications. 2015.

[8] Williams et al. \textit{Nonnegative decomposition of multivariate information}. arXiv. 2010.

[9] Amit et al. \textit{Storing infinite numbers of patterns in a spin-glass model of neural networks}. Physical Rev. Letters. 1985

[10] Nishimori. \textit{Statistical physics of spin glasses and information processing: an introduction}. Clarendon Press. 2010

[11] Buesing et al. \textit{Neural Dynamics as Sampling: A Model for Stochastic Computation in Recurrent Networks of Spiking Neurons}. PLOS Computational Biology. 2011

[12] Pecevski et al. \textit{Learning Probabilisitc Inference through Spike-Timing-Dependent Plasticitys}. eNeuro. 2016

[13] Pecevski et al. \textit{Formation and maintenance of neuronal assemblies through synaptic plasticity}. Nature Communications. 2014

[14] Pecevski et al. \textit{A learning algorithm for Boltzmann Machines}. Cognitive Science. 1985

% Figures and tables, if you decide to leave them to the end
%\input{figure}
%\input{table}

\end{document}


