% ---- ETD Document Class and Useful Packages ---- %
\documentclass{ucetd}
\usepackage{subfigure,epsfig,amsfonts}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage{url}


%% Use these commands to set biographic information for the title page:
\title{Stochastic computation in recurrent networks of spiking neurons}
\author{Clayton W. Seitz}
\department{Graduate Program in Biophysics}
\division{Physical and Biological Sciences}
\degree{Master of Science}
\date{Winter 2021}

%% Use these commands to set a dedication and epigraph text
%\dedication{I dedicate this thesis to my friends and family, who have pushed me to think critically and live a principled life}


\epigraph{

``Machines cannot think as humans do but just because something thinks differently from you, does that mean its not thinking?" - Alan Turing
}

\begin{document}
%% Basic setup commands
% If you don't want a title page comment out the next line and uncomment the line after it:
\maketitle
%\omittitle

% These lines can be commented out to disable the copyright/dedication/epigraph pages
\makecopyright
%\makededication
\makeepigraph


%% Make the various tables of contents
\tableofcontents
%\listoffigures
%\listoftables

\acknowledgments
% Enter Acknowledgements here

\abstract
One of the least understood subjects in biology is the mechanisms by which networks of neurons extract and store information from signals originating in their evironment and perform rather complex, yet efficient, computations on those signals. Fundamental questions still pervade the neuroscience community such as how networks of neurons compute and what they compute - questions that simply cannot be fully answered without a rigorous mathematical theory of computation in neural circuits. However, it is widely accepted in the neuroscience community that the connectivity of networks of neurons and its modification via synaptic plasticity are fundamental mechanisms by which information is extracted and stored by neural circuits. It is natural to then infer, then, that the storage of information and the process of computation are simultaneous and their delicate balance dictates the function of the circuit. At the same time, various dynamical regimes can be observed in regions of the brain thought to have varying functions, bringing the relationship between network dynamics and computation into question. The problem is even further exacerbated by the library of plasticity mechanisms neurons undergo and the shear complexity of their connectivity \emph{in vivo}. Therefore, it appears that an approach starting from first principles, such as that provided by information theory and statistics, is necessary - adding in the complex biological details as we move forward. Here, we introduce a model of recurrent spiking neural networks (RSNNs), illustrating the impact of network dynamics on its topology and interpret the computational character of these states using Shannon's information theory. It will be shown that computation in these networks is unavoidably stochastic but, still, the brain computes.




\mainmatter

\chapter{Introduction}

Fundamental questions in theoretical neuroscience pertain to how circuits in cortex store and process information. Recent decades have yielded a number of experimental and theoretical advances in our understanding of the mechanisms information storage, such as homosynaptic, heterosynaptic, and homeostatic plasticity. However, the precise mathematical relationship between the synaptic weights and the stimuli or `patterns' to which the network has been exposed, remains elusive. 

It is generally accepted that neural network models date back to McCulloch and Pitts [1] followed later by the Hopfield network [2] which spawned a concrete application of statistical physics to the description of neural circuits. At the time, the storage of memories in the Hopfield model was of particular interest and was well-characterized by Gardner [4], Amit [10], and Nishimori [11]. Such models draw direct comparisons between simplified neural networks and magnetic systems, thereby reducing their statistical properties to those previously known to statistical physics. More specifically, such comparisons potentially allow one to write down the Hamiltonian for the system analytically, and in turn the probability density over the phase space of the network. However, it is not a simple task to probe the geometry of the free energy landscape and the problem is further aggravated by the replacement of spins with even minimally biologically realistic units. 

The state space of a network model is made even more diverse with the addition of biological realism e.g. variable cell types as in the Brunel network [5]. Even in this case, statistical physics has inspired great progress towards an understanding of the dynamics of coupled excitatory and inhibitory populations, in spite of the common assumption that weights between different populations are homogeneous. The Brunel network has been used to initialize so-called balanced networks which maintain asynchronous irregular firing. Such balanced networks have been used to initialize networks to be later trained via a family of orchestrated slow and fast plasticity mechanisms [7]. After training, these networks have been shown to exhibit attractor dynamics towards states imprinted on the network weights by previous stimuli, all in an arguably more brain-like fashion than the original Hopfield model. Many questions remain, such as the range of spiking dynamics observed for weight matrices that range from fully connected to fully random as in a Erdos-Renyi directed binomial graph and the storage capacity of such networks and their compresson and error correction capabilities.


\chapter{Network Connectivity and Dynamics}
\section{Introduction}
How a circuit computes, physically. How connectivity shapes dynamics and the computational description of different dynamical regimes according to information theory

\chapter{Information Theory of Spiking Neurons}
\section{Introduction}
Does a neural circuit compute? The theoretical toolkit for understanding the computational capabilities of a neural circuit. Proving the processing of information.

\chapter{Stochastic Computation in Recurrent Networks}
\section{Introduction}
If the network processed information, what does it do with that information. How does it build a model with that information

\chapter{Conclusions}

% Intro to chapter one

% Format a LaTeX bibliography
\makebibliography

[1] Mculloch and Pitts. \textit{A logical calculus of the ideas imminent in nervous activity}. Journal of Mathematical Biophysics. 1943.

[2] J.J. Hopfield. \textit{Neural networks and physical systems with emergent collective computational abilities}. 1982.

[3] D.J. Amit. \textit{Spin-glass models of neural networks}. Physical Rev A. 1985.

[4] E. Gardner. \textit{The space of interactions in neural network models}. Journal of Physics A: Mathematical and General. 1988.

[5] N. Brunel. \textit{Dynamics of sparsely connected networks of excitatory and inhibitory neurons}. Journal of Computational Neuroscience. 2000. 

[6] Rosenbaum and Doiron. \textit{Balanced Networks of Spiking Neurons with Spatially Dependent Recurrent Connections}. Physical Review X. 2014.

[7] Zenke et al. \textit{Diverse synaptic plasticity mechanisms
orchestrated to form and retrieve memories
in spiking neural networks}. Nature Communications. 2015.

[8] Williams et al. \textit{Nonnegative decomposition of multivariate information}. arXiv. 2010.

[9] Amit et al. \textit{Storing infinite numbers of patterns in a spin-glass model of neural networks}. Physical Rev. Letters. 1985

[10] Nishimori. \textit{Statistical physics of spin glasses and information processing: an introduction}. Clarendon Press. 2010

[11] Buesing et al. \textit{Neural Dynamics as Sampling: A Model for Stochastic Computation in Recurrent Networks of Spiking Neurons}. PLOS Computational Biology. 2011

[12] Pecevski et al. \textit{Learning Probabilisitc Inference through Spike-Timing-Dependent Plasticitys}. eNeuro. 2016

[13] Pecevski et al. \textit{Formation and maintenance of neuronal assemblies through synaptic plasticity}. Nature Communications. 2014

% Figures and tables, if you decide to leave them to the end
%\input{figure}
%\input{table}

\end{document}


