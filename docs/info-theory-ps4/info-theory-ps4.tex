\documentclass[12pt]{article}
\usepackage{amsmath} % AMS Math Package
\usepackage{amsthm} % Theorem Formatting
\usepackage{amssymb}    % Math symbols such as \mathbb
\usepackage{graphicx} % Allows for eps images
\usepackage[dvips,letterpaper,margin=1in,bottom=0.7in]{geometry}
\usepackage{amsmath}


\newtheorem{p}{Problem}[section]
\usepackage{cancel}
\newtheorem*{lem}{Lemma}
\theoremstyle{definition}
\newtheorem*{dfn}{Definition}
 \newenvironment{s}{%\small%
        \begin{trivlist} \item \textbf{Solution}. }{%
            \hspace*{\fill} $\blacksquare$\end{trivlist}}%


\begin{document}

{\noindent\Huge\bf  \\[0.5\baselineskip] {\fontfamily{cmr}\selectfont  Problem Set 4}         }\\[2\baselineskip] % Title
{ {\bf \fontfamily{cmr}\selectfont Information and Coding Theory}\\ {\textit{\fontfamily{cmr}\selectfont     March 14, 2021}}}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    {\large \textsc{Clayton Seitz}
\\[1.4\baselineskip] 

\begin{p}
More on linear codes
\end{p}

A linear code is a subspace $C \subseteq \mathbb{F}_{2}^{n}$. The distance of such a code $\Delta(C)$ can be written as a minimization of the weight over all $x \in C$. 

\begin{s}
\begin{align*}
\Delta(C) &= \min_{x_{1},x_{2}\in C} \Delta(x_{1},x_{2})\\
&= \min_{x_{1},x_{2}\in C} \Delta(0,x_{2}-x_{1})\\
&= \min_{x\in C\; \backslash \{0^{n}\}} \text{wt}(x)
\end{align*}

Since we have required that $C$ is linear i.e. $\forall x_{1},x_{2} \in C$ we have that $x_{2}-x_{1} \in C$ ($C$ is closed under addition). 
\\
\\
Now, we consider the general Hamming code $C:  \mathbb{F}_{2}^{k} \rightarrow  \mathbb{F}_{2}^{n}$ which maps messages $w$ of length $k$ to  $\mathbb{F}_{2}^{n}$ via the generator matrix $G\in \mathbb{F}_{2}^{n\times k}$. We also define the parity check matrix $H \in \mathbb{F}_{2}^{r\times n}$ which has the property that for any encoded message $x$ we have $Hx = 0$. Note this also means that the columns of $G$ form a basis for the null space of $H$.
\\
\\
Since we are defining our code to be the null space of $H \in \mathbb{F}_{2}^{r\times n}$, the dimension of the code is given by

\begin{align*}
\text{dim}(\text{null}(H)) &= n - \text{rank}(H)\\
&=n - r\\
&= 2^{r} - 1 - r
\end{align*}
\\
\\

Also, since $H$ has $r$ columns, the block length of a general Hamming code is $2^{r}-1$. Finally, we can show that the distance of such a code is 3 by writing the Hamming bound and assuming we can correct $t=1$ errors:

\begin{align*}
|C| &\leq \frac{2^{n}}{|B(0,t)|}\\
&= \frac{2^{n}}{|B(0,1)|}\\
&= 2^{n-r}
\end{align*}


where $2^{n-r}$ is indeed the size of the code according to $\text{dim}(\text{null}(H))$ calculated above.  
\\
\\
Finally, we will define the dual code $C^{\perp}$ to be the code with generator matrix $H^{T}$ and  parity check matrix $G^{T}$. As stated above, the generator matrix $G$ is a matrix with columns equal to the basis vectors of the null space of $H$ i.e. $HG = 0$. This is equivalent to saying that the columns of $H^{T}$ form the basis of the null space of $G^{T}$:

\begin{equation*}
HG = 0 \iff G^{T}H^{T}=0
\end{equation*}

Therefore $H^{T}$ can be viewed as the generator matrix and $G^{T}$ the parity check matrix for the dual code $C^{\perp}$. We can repeat our calculations of dimension, block length, and minimum distance for the dual code $C^{\perp}$:

\begin{align*}
\text{dim}(\text{null}(G^{T})) &= n - \text{rank}(G^{T})\\
&=n - k\\
&= 2^{r} - 1 - k
\end{align*}
\\
\\
and has block length $2^{n}-1$ and minimum distance 3 by the same packing argument made above except the size of the dual code $|C^{\perp}| = 2^{n-k}$.

\end{s}

\begin{p}
Good distance codes from linear compression
\end{p}

We would like to prove that an arbitrary compression algorithm $\text{Com}:  \mathbb{F}_{2}^{n} \rightarrow  \mathbb{F}_{2}^{m}$ implemented by the matrix $H \sim \mathbb{F}_{2}^{m\times n}$ yields a good compression scheme for a sequence $Z \sim (\text{Bern}(p))^{n}$ which means that there exists a decompression algorithm $\text{Decom}:  \mathbb{F}_{2}^{m} \rightarrow  \mathbb{F}_{2}^{n}$ with the following error bound

\begin{equation*}
\underset{Z\sim (\text{Bern}(p))^{n}}{\mathbf{Pr}}\left[\text{Decom}(HZ)\neq Z\right] \leq 2^{-t}
\end{equation*}

\begin{s}

To prove such a bound, we can first rewrite the LHS of the above. Notice that since $m < n$ (compression) we cannot have a $\text{Decom}:  \mathbb{F}_{2}^{m} \rightarrow  \mathbb{F}_{2}^{n}$ that is a bijection. Thus, we have to define $\text{Decom}$ in such a way that we minimize the probability that the wrong $Z$ is recovered after decompression while accounting for the distribution over the input: $Z \sim (\text{Bern}(p))^{n}$. 
\\
\\
Given that $\text{im}(H) = \mathbb{F}_{2}^{m}$, we need to capture this ``lack of bijectivity" of our compression scheme. To do this, we consider the probability that our $\text{Decom}$ decompresses $w \in \mathbb{F}_{2}^{m}$ to a draw from the input distribution.

\begin{equation*}
\underset{Z\sim (\text{Bern}(p))^{n}}{\mathbf{Pr}}\left[\text{Decom}(HZ)\neq Z\right] = 1 - \underset{Z\sim (\text{Bern}(p))^{n}}{\mathbf{Pr}}\left[\text{Decom}(HZ)= Z\right]
\end{equation*} 

where we can write the term on the right hand side as 

\begin{align*}
\underset{Z\sim (\text{Bern}(p))^{n}}{\mathbf{Pr}}\left[\text{Decom}(HZ)= Z\right]
&= \sum_{w\in \mathbb{F}_{q}^{m}}\underset{Z\sim (\text{Bern}(p))^{n}}{\mathbf{Pr}}\left[\text{Decom}(w)= Z\right]
\end{align*}

Now, let's say there is a set of elements in $\mathbb{F}_{2}^{n}$ that map to the same element in $\mathbb{F}_{2}^{m}$: $S = \left\{z\in \mathbb{F}_{q}^{n}|Hz=w\right\}$. Since we have said that $p < \frac{1}{2}$, the decompression algorithm that picks the $z$ with minimal weight i.e.

\begin{equation*}
\text{Decom}(w) := \underset{x\;\in\; S}{\text{argmin}}\left\{\text{wt}(x)\right\}
\end{equation*}

maximizes $\underset{Z\sim (\text{Bern}(p))^{n}}{\mathbf{Pr}}\left[\text{Decom}(w)= Z\right]$. This can be seen if we let $z^{*} \in S$ be the element with minimal weight

\begin{equation*}
\underset{z\;\in\; S}{\mathbf{Pr}}\left[\text{wt}(z)\geq\text{wt}(z^{*})\right] = \sum_{t > \text{wt}(z^{*})}{n \choose k}\cdot p^{k}(1-p)^{n-k} \rightarrow 0 
\end{equation*}

as $p\rightarrow 0$. Finally, we can show that a code $C\subseteq\mathbb{F}_{q}^{n}$ which is defined as 

\begin{equation*}
C = \left\{x \in  \mathbb{F}_{q}^{n} | Hx=0\right\}
\end{equation*}

has distance that satisfies

\begin{equation*}
\Delta(C) \geq \frac{t}{\log(1/p)}
\end{equation*}

To see why this inequality is true, consider $z \in \mathbb{F}_{q}^{n}$ and $z' \in \mathbb{F}_{q}^{n}$ where $Hz = Hz'$ i.e. both $z$ and $z'$ map to the same codeword $x \in C$. Now if 

\begin{align*}
\text{dim}(C) &= n - \text{rank}(H)\\
&= n - m 
\end{align*}

and $z$ and $z'$ each define a linear combination of $n-m$ column vectors of $H$. Such a linear combination may result in the same $x \in C$. In the worst case, such a collision can occur by flipping a single bit which occurs with probability $p$ since $Z \sim (\text{Bern}(p))^{n}$. The number of ways we can perform such a flip is $2^{\delta}$ so we have

\begin{align*}
\underset{Z\sim (\text{Bern}(p))^{n}}{\mathbf{Pr}}\left[\text{Decom}(HZ)\neq Z\right] &= 2^{\delta} \cdot p \leq 2^{-t}
\end{align*}

Therefore, $\delta \geq \frac{t}{\log(1/p)}$.

\end{s}

\begin{p}
Mixing polynomials
\end{p}

\begin{s}
\\
\\
We are given two sequences of values $b_{n}$ and $c_{n}$ which are the result of evaluating polynomials $f_{1}(x)$ and $f_{2}(x)$ at points $a_{n}$. However, for some values of $n$, $b_{n}$ and $c_{n}$ are swapped during transmission.
\\
\\
For any particular $a_{n}$, the sum $f_{1}(a_{n}) + f_{2}(a_{n}) = b_{n} + c_{n}$ and the product $f_{1}(a_{n})\cdot f_{2}(a_{n}) = b_{n}\cdot c_{n}$ do not change upon swapping $b_{n}$ and $c_{n}$. Let $y$ be the sequence of values received, then we can write a bi-variate polynomial

\begin{align*}
h(x,y) &= y^{2} - y(f_{1}(x)+f_{2}(x)) + f_{1}(x)\cdot f_{2}(x)\\
&= (y-f_{1}(x))(y-f_{2}(x))
\end{align*}

If we can perform such a factorization of $h(x,y)$ then we can descramble $f_{1}(x)$ and $f_{2}(x)$. 
\\
\\
In the second case, we are given a value $\beta_{n}$ at each point in the domain $a_{n}$ but we don't know whether $\beta_{n}$ came from $f_{1}(x)$ or $f_{2}(x)$. However, we are given the guarantee that the number of points coming from $f_{1}(x)$ satisfies $\frac{n}{3} \leq n_{1} \leq \frac{2n}{3}$ and the points coming from $f_{2}(x)$ satisfies $\frac{n}{3} \leq n_{2} \leq \frac{2n}{3}$ where $n = n_{1} + n_{2}$. 
\\
\\
We can recast this problem by thinking of the points that came from one polynomial, say $f_{1}(x)$, as ``errors" and define an error polynomial $e$ that is zero when $y \neq f_{1}(x)$. Then, we can use the general Reed-Solomon decoding scheme to solve for $f_{2}(x)$. Once we have $f_{2}(x)$, finding $f_{1}(x)$ is straightforward: use Lagrange interpolation again on the difference between $f_{2}(x)$ and $y$. 

Recall that Lagrange interpolation requires that $k\leq n-t$ where $n$ is the total number of points, $t$ the number of errors, and $k$ is the degree of the polynomial to interpolate. We can still apply Lagrange interpolation here since $k \leq \frac{n}{3}$ because $t=\frac{2n}{3}$ is the maximum number of "errors" which must be true since we are certain that $k < \frac{n}{6}$.


\end{s}


\end{document}