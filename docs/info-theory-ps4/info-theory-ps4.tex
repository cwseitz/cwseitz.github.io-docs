\documentclass[12pt]{article}
\usepackage{amsmath} % AMS Math Package
\usepackage{amsthm} % Theorem Formatting
\usepackage{amssymb}    % Math symbols such as \mathbb
\usepackage{graphicx} % Allows for eps images
\usepackage[dvips,letterpaper,margin=1in,bottom=0.7in]{geometry}
\usepackage{amsmath}


\newtheorem{p}{Problem}[section]
\usepackage{cancel}
\newtheorem*{lem}{Lemma}
\theoremstyle{definition}
\newtheorem*{dfn}{Definition}
 \newenvironment{s}{%\small%
        \begin{trivlist} \item \textbf{Solution}. }{%
            \hspace*{\fill} $\blacksquare$\end{trivlist}}%


\begin{document}

{\noindent\Huge\bf  \\[0.5\baselineskip] {\fontfamily{cmr}\selectfont  Problem Set 4}         }\\[2\baselineskip] % Title
{ {\bf \fontfamily{cmr}\selectfont Information and Coding Theory}\\ {\textit{\fontfamily{cmr}\selectfont     March 14, 2021}}}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    {\large \textsc{Clayton Seitz}
\\[1.4\baselineskip] 

\begin{p}
This is the first problem
\end{p}

\begin{s}
\begin{align*}
\Delta(C) &= \min_{x_{1},x_{2}\in C} \Delta(x_{1},x_{2})\\
&= \min_{x_{1},x_{2}\in C} \Delta(0,x_{2}-x_{1})\\
&= \min_{x\in C} \mathbf{wt}(x)
\end{align*}

Since the code is linear, $x_{2}-x_{1} \in C$. Now, we consider the parity check matrix $H \in \mathbb{F}_{2}^{r\times n}$ where $n = 2^{r} - 1$. We will find the dimension, block length, and distance for such a code. First, the dimension of the code $\dim(C)$ is $r+1$ since the rank of $H$ is $r$. The block length is then $2^{r+1}$ and the distance is 3. Now, consider the Hamming code $C: \mathbb{F}_{2}^{k} \rightarrow \mathbb{F}_{2}^{n}$ which is formally defined as the set of $x$ in the null space in of the parity check matrix:

\begin{equation*}
C = \left\{x\in \mathbb{F}_{2}^{n} | Hx = 0\right\}
\end{equation*}

where $H \sim \mathbb{F}_{2}^{k\times n}$ is the parity check matrix. We can also define the dual code $C^{\perp}$ to be the code with generator matrix $H^{T}$ and  parity check matrix $G^{T}$. 

To see why this is possible, we will use the fact that we have defined our code $C$ to be the vectors $x$ that lie in the null space of the parity matrix $H$. Now, the definition of our code requires that $H(x)=H(G(w))=0$ which means that the generator matrix $G$ is a matrix with columns equal to the basis vectors of the null space of $H$ i.e. $HG = 0$. This is equivalent to saying that the columns of $H^{T}$ form the basis of the null space of $G^{T}$:

\begin{equation*}
HG = 0 \iff G^{T}H^{T}=0
\end{equation*}

Therefore $H^{T}$ can be viewed as the generator matrix and $G^{T}$ the parity check matrix for the dual code $C^{\perp}$.

\end{s}

\begin{p}
Good distance codes from linear compression
\end{p}

We would like to prove that an arbitrary compression algorithm $\text{Com}:  \mathbb{F}_{2}^{n} \rightarrow  \mathbb{F}_{2}^{m}$ implemented by the matrix $H \sim \mathbb{F}_{2}^{m\times n}$ yields a good compression scheme for a sequence $Z \sim (\text{Bern}(p))^{n}$ which means that there exists a decompression algorithm $\text{Decom}:  \mathbb{F}_{2}^{m} \rightarrow  \mathbb{F}_{2}^{n}$ with the following error bound

\begin{equation*}
\underset{Z\sim (\text{Bern}(p))^{n}}{\mathbf{Pr}}\left[\text{Decom}(HZ)\neq Z\right] \leq 2^{-t}
\end{equation*}

\begin{s}

To prove such a bound, we can first rewrite the LHS of the above. Notice that since $m < n$ whether or not we have a $\text{Decom}:  \mathbb{F}_{2}^{m} \rightarrow  \mathbb{F}_{2}^{n}$ that comes close to a bijection depends on the distribution over the input: $Z \sim (\text{Bern}(p))^{n}$. 
\\
\\
Assuming that $\text{im}(H) = \mathbb{F}_{2}^{m}$, if we consider all possible elements $w \in  \mathbb{F}_{2}^{m}$, the "lack of bijectivity" is captured by the expected size of a set of inputs that map to a particular output. However, it is simpler to think of the inverse problem where a draw from the input distribution is equal to the decompression of $w$: $\underset{Z\sim (\text{Bern}(p))^{n}}{\mathbf{Pr}}\left[\text{Decom}(w)= Z\right]$. Therefore,

\begin{align*}
\underset{Z\sim (\text{Bern}(p))^{n}}{\mathbf{Pr}}\left[\text{Decom}(HZ)\neq Z\right] &= 1 - \underset{Z\sim (\text{Bern}(p))^{n}}{\mathbf{Pr}}\left[\text{Decom}(HZ)= Z\right]\\
&= 1 - \sum_{w\in \mathbb{F}_{q}^{m}}\underset{Z\sim (\text{Bern}(p))^{n}}{\mathbf{Pr}}\left[\text{Decom}(w)= Z\right]
\end{align*} 

Now, let's say there is a set of inputs that map to the same output $S = \left\{z\in \mathbb{F}_{q}^{n}|Hz=w\right\}$. Since we have said that $p < \frac{1}{2}$, the decompression algorithm that picks the $z$ with minimal weight i.e.

\begin{equation*}
\text{Decom}(w) := \underset{z\;\in\; S}{\text{argmin}}\left\{S\right\}
\end{equation*}

minimizes $\underset{Z\sim (\text{Bern}(p))^{n}}{\mathbf{Pr}}\left[\text{Decom}(w)= Z\right]$. This can be seen if we let $z^{*} \in S$ be the element with minimal weight

\begin{equation*}
\underset{Z\;\in\; S}{\mathbf{Pr}}\left[\text{wt}(z)\geq\text{wt}(z^{*})\right] = \sum_{t > \text{wt}(z^{*})}{n \choose t}\cdot p^{t}(1-p)^{n-t} \rightarrow 0 
\end{equation*}

as $p\rightarrow 0$.

\end{s}

\begin{p}
Mixing polynomials
\end{p}

\begin{s}

We are given two sequences of values $(b_{1}, \;...\;, b_{n})$ and $(c_{1}, \;...\;, c_{n})$ which are the result of evaluating polynomials $f_{1}$ and $f_{2}$ at points $a_{i}$, respectively. Notice that for any particular $a_{i}$ we have that the sum $f_{1}(a_{i}) + f_{2}(a_{i}) = b_{i} + c_{i}$ and the product $f_{1}(a_{i})\cdot f_{2}(a_{i}) = b_{i}\cdot c_{i}$ which of course do not change upon swapping $b_{i}$ and $c_{i}$. If $y$ is the sequence of values received, then we can write a bivariate polynomial

\begin{align*}
h(x,y) &= y^{2} - y(f_{1}(x)+f_{2}(x)) + f_{1}(x)\cdot f_{2}(x)\\
&= (y-f_{1}(x))(y-f_{2}(x))
\end{align*}

If we can perform such a factorization of $h(x,y)$ then we can descramble $f_{1}(x)$ and $f_{2}(x)$. 

In the second case, we are given a value $\beta_{i}$ at each point in the domain $a_{i}$ but we don't know whether $\beta_{i}$ came from $f_{1}(x)$ or $f_{2}(x)$. However, we are given the guarantee that the number of points coming from $f_{1}(x)$ satisfies $\frac{n}{3} \leq n_{1} \leq \frac{2n}{3}$ and the points coming from $f_{2}(x)$ satisfies $\frac{n}{3} \leq n_{2} \leq \frac{2n}{3}$ where $n = n_{1} + n_{2}$. 

We can recast this problem by thinking of the points that came from one polynomial, say $f_{1}(x)$, as "errors" and define an error polynomial $e$ that is zero when $y \neq f_{1}(x)$. Then, we can use the Reed-Solomon decoding scheme to solve for $f_{2}(x)$. Once we have $f_{2}(x)$, finding $f_{1}(x)$ is straightforward: use Lagrange interpolation again on the difference between $f_{2}(x)$ and $y$. 

Recall that Lagrange interpolation requires that $k\leq n-t$ where $n$ is the total number of points, $t$ the number of errors, and $k$ is the degree of the polynomial to interpolate. We can still apply Lagrange interpolation here since $k \leq \frac{n}{3}$ because $t=\frac{2n}{3}$ is the maximum number of "errors" which must be true since we have already been told $k < \frac{n}{6}$.


\end{s}


\end{document}