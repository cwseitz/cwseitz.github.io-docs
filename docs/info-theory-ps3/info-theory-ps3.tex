\documentclass[12pt]{article}
\usepackage{amsmath} % AMS Math Package
\usepackage{amsthm} % Theorem Formatting
\usepackage{amssymb}    % Math symbols such as \mathbb
\usepackage{graphicx} % Allows for eps images
\usepackage[dvips,letterpaper,margin=1in,bottom=0.7in]{geometry}
\usepackage{amsmath}


\newtheorem{p}{Problem}[section]
\usepackage{cancel}
\newtheorem*{lem}{Lemma}
\theoremstyle{definition}
\newtheorem*{dfn}{Definition}
 \newenvironment{s}{%\small%
        \begin{trivlist} \item \textbf{Solution}. }{%
            \hspace*{\fill} $\blacksquare$\end{trivlist}}%


\begin{document}

{\noindent\Huge\bf  \\[0.5\baselineskip] {\fontfamily{cmr}\selectfont  Problem Set 3}         }\\[2\baselineskip] % Title
{ {\bf \fontfamily{cmr}\selectfont Information and Coding Theory}\\ {\textit{\fontfamily{cmr}\selectfont     February 26, 2021}}}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    {\large \textsc{Clayton Seitz}
\\[1.4\baselineskip] 

\begin{p}
A single dice is rolled and we gain a dollar if the outcome is 2,3,4,5 and lose a dollar if the outcome is 1 or 6. Find the expected gain and the maximum entropy distribution over the possible outcomes of a roll.
\end{p}

\begin{s}

Let $P$ be the uniform distribution over the dice universe $\chi$ where an outcome of a roll is $x \in \chi$. Furthermore, let $\phi(x)$ be the gain given the outcome of a roll $x$ according the problem definition

\[\phi = \begin{cases} 
      1 & \; 2,3,4,5 \\
      -1 & \; 1,6 \\
   \end{cases}
\]

and $\bar{x}\sim P^{n}$ be a draw of a sequence of $n$ rolls from the product distribution $P^{n}$. We can then calculate the expected gain over $n$ rolls as

\begin{align*}
\underset{\bar{x} \sim P^{n}}{\mathbf{E}}\left[\phi(\bar{x})\right] &= \sum_{n}\left(\sum_{i} \phi(x_{n})\cdot p(x_{n})\right)\\
&=\sum_{n}\left(\frac{1}{6}\sum_{i}\phi(x_{n})\right)\\
&= \frac{n}{3}
\end{align*}

Now, we would like to find the maximum entropy distribution $P^{*}$ over $\chi$ in the set of distributions $\Pi$ such that

\begin{equation}
\underset{\bar{x} \sim (P^{*})^{n}}{\mathbf E}\left[\phi(\bar{x})\right] > \frac{n}{3}
\end{equation}

We can find such a distribution $P^{*}$ by defining the linear family of distributions that satisfy this constraint on the expected gain

\begin{equation*}
\mathcal{L} = \left\{ P : \underset{\bar{x} \sim P^{n}}{\mathbf E}\left[\phi(\bar{x})\right] = \sum_{x\in \chi} p(x)\cdot \phi(x) > \alpha \right\}
\end{equation*}

We would like to find the distribution $P^{*}$ such that $P^{*} = \mathbf{Proj}_{\mathcal{L}}(Q)$ and we now compute this projection by using the Lagrangian

\begin{equation}
\mathbf{\Lambda}(P, \lambda_{0}, \lambda_{1}) = D(P||Q) + \lambda_{0}\left(\sum p(x) - 1\right) + \lambda_{1}\xi_{\alpha}(x)
\end{equation}

where 

\[\xi_{\alpha} = \begin{cases} 
      -x & x < \alpha \\
      0 & x \geq \alpha \\
   \end{cases}
\]

We find a solution by setting the derivative of this Lagrangian to zero

\begin{equation*}
\nabla \Lambda = \log\left(\frac{p^{*}(x)}{q(x)}\right) + \frac{1}{2\ln 2} + \lambda_{0} + \nabla \xi_{\alpha}
\end{equation*}

\[\nabla \xi_{\alpha} = \begin{cases} 
      -\lambda_{1} & x < \alpha \\
      0 & x > \alpha \\
   \end{cases}
\]

Ultimately, we have the solution

\begin{equation*}
p^{*}(x) = q(x) \cdot 2^{\lambda_{0} -\lambda_{1}\cdot \phi(x)}
\end{equation*}


\end{s}

\begin{p}
Exponential families and maximum entropy
\end{p}

\begin{s}


\begin{align*}
H(Q) &= -\sum_{x\sim \chi} Q(x)\log\exp\left\{\lambda_{0} + \sum_{i\sim [k]} \lambda_{i}f_{i}(x)\right\}\\ 
&= -\frac{1}{\ln 2}\sum_{x\sim \chi} Q(x)\left\{\lambda_{0} + \sum_{i\sim [k]} \lambda_{i}f_{i}(x)\right\}\\
&= -\frac{1}{\ln 2}\left(\lambda_{0} + \sum_{x\sim \chi} Q(x)\left\{\sum_{i\sim [k]} \lambda_{i}f_{i}(x)\right\}\right)\\
&= -\frac{1}{\ln 2}\left(\lambda_{0} + \sum_{i\sim [k]} \lambda_{i}\alpha_{i}\right)
\end{align*}

Now we will show that the KL-Divergence is the difference of entropies

\begin{align*}
D(P||Q) &= \sum_{x\sim \chi} p(x)\log\frac{p(x)}{q(x)}\\
&= -\frac{1}{\ln 2}\sum_{x\sim \chi} p(x)\left\{\lambda_{0} + \sum_{i\sim [k]} \lambda_{i}f_{i}(x)\right\} - H(P)\\
&= -\frac{1}{\ln 2}\left(\lambda_{0} + \sum_{i\sim [k]} \lambda_{i}\alpha_{i}\right) - H(P)\\
&= H(Q) - H(P)
\end{align*}

Finally, we can show that $Q$ is the maximum entropy distribution in the family $\mathcal{L}$

\begin{align*}
D(P||Q) &=  H(Q) - H(P) \geq 0
\end{align*}

which requires that $H(Q) \geq H(P)$.

\end{s}


\begin{p}
Minimax rates for denoising
\end{p}

\begin{s}

This can be shown by using the chain rule for KL-Divergence

\begin{align*}
D(P(X,Y)||Q(X,Y)) &= D(P(X)||Q(X)) + D(P(Y|X)||Q(Y|X))\\
&= D(P(Y|X)||Q(Y|X))\\
&= D(\mathcal{N}(f(x),\sigma^{2})||\mathcal{N}(g(x),\sigma^{2}))\\
\end{align*}

which we now compute

\begin{align*}
D(\mathcal{N}(f(x),\sigma^{2})||\mathcal{N}(g(x),\sigma^{2})) &= 
\frac{1}{\ln 2}\int_{0}^{1} \exp\left(-(x-f(x))^{2}/2\sigma\right)\\ &\cdot \ln \left(\frac{\exp\left(-(x-f(x))^{2}/2\sigma\right)}{\exp\left(-(x-g(x))^{2}/2\sigma\right)}\right)dx\\
&= \frac{1}{2\ln 2\cdot \sigma}\int_{0}^{1} \exp\left(-(x-f(x))^{2}\right)\\ &\cdot \left((x-g(x))^{2}-(x-f(x))^{2}\right)dx\\
&= \frac{1}{2\ln 2\cdot \sigma^{2}}\int_{0}^{1} |f(x)-g(x)|^{2}dx\\
&= \frac{1}{2\ln 2\cdot \sigma^{2}}\cdot ||f(x)-g(x)||_{2}^{2}\\
\end{align*}

Next we will prove a lower bound on the minimax loss for $n$ samples when we have a collection of $S$ functions. First, we can manipulate the result from above and show that

\begin{align*}
D(P_{f}||P_{g}) &= \frac{1}{2\ln 2\sigma^{2}}\cdot ||f(x)-g(x)||_{2}^{2} \\
&\leq \frac{32\delta^{2}}{\ln 2\cdot \sigma^{2}}
\end{align*}

since we have assumed $||f-g||_{2}^{2} \leq 8\delta$. At the same time, we can use what we already know about lower bounds for minimax rates via multiple hypotheses. Let's say that the data $\bar{x}$ is drawn from the distribution $P_{s}$. The probability of error by a classifier $T(\bar{x})$ that outputs the distribution $s\in S$ the data was drawn from is lower bounded by

\begin{equation*}
\mathbf{Pr}\left[T(\bar{x}) \neq s]\right] \geq 1 - \frac{n\cdot \underset{s_{1},s_{2}\in S}{\mathbf{E}}\left[D(P_{s_{1}}||P_{s_{2}})\right] + 1}{\log |S|}
\end{equation*}

Also, we define the loss function to be $\ell = ||f-g||_{2}^{2}$

\begin{align*}
M_{n}(\Pi, \ell) &\geq \ell(\delta)\cdot \underset{T}{\inf}\left\{\mathbf{Pr}\left[T(\bar{x}) \neq s]\right]\right\}\\
&= \delta^{2}\cdot\left(1 - \frac{n\cdot \underset{s_{1},s_{2}\in S}{\mathbf{E}}\left[D(P_{s_{1}}||P_{s_{2}})\right] + 1}{\log |S|}\right)\\
&= \delta^{2}\cdot\left(1 - \frac{n\cdot \left(32\delta^{2}/\sigma^{2}\ln 2\right) + 1}{\log |S|}\right)
\end{align*}

since the expectation over all possible pairs of functions can be at most the largest distance between two functions. Now, if we define the family to be the bump functions, which are $L$-Lipschitz since

\begin{align*}
|B_{\epsilon}(x_{1}) - B_{\epsilon}(x_{2})| &= L\cdot ||x_{2}|-|x_{1}||\\
&\leq L\cdot |x_{2}-x_{1}|
\end{align*}

and assuming $\epsilon < 1$

\begin{align*}
\int_{-1}^{1}(B_{\epsilon}(x))^{2}dx &= \int_{-\epsilon}^{\epsilon}(B_{\epsilon}(x))^{2}dx\\
&= 2\int_{0}^{\epsilon} (L\cdot (\epsilon-|x|)^{2}dx\\
&= 2L^{2}\int_{0}^{\epsilon} (x-\epsilon)^{2}dx\\
&= \frac{2L^{2}\epsilon^{3}}{3}
\end{align*}

Next, we will show something

\begin{align*}
||f_{a}-f_{b}||_{2}^{2} &= \int_{-1}^{1} |f_{a}(x)-f_{b}(x)|^{2}dx\\
&= \int_{-1}^{1} |\sum_{i=1}^{m}(a_{i}-b_{i})B_{\epsilon}(x-z_{i})|^{2}dx\\
&= \int_{-1}^{1} |\sum_{a_{i}\neq b_{i}}B_{\epsilon}(x-z_{i})|^{2}dx\\
&= \sum_{a_{i}\neq b_{i}}\int_{-1}^{1} \left(B_{\epsilon}(x-z_{i})\right)^{2}dx\\
&= \frac{2L^{2}\epsilon^{3}}{3}d_{H}(a,b)
\end{align*}

\end{s}

\end{document}