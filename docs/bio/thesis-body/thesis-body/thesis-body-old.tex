
\documentclass{article} % For LaTeX2e
\usepackage{iclr2020_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{siunitx}
\usepackage{url}
\usepackage{graphicx}


\title{Dynamics of plastic networks of spiking neurons}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}


\maketitle

\begin{abstract}

Fundamental questions in theoretical neuroscience pertain to how neural networks store and process information. Recent decades have yielded a number of advances in our understanding of the mechanisms information storage, such as homosynaptic, heterosynaptic, and homeostatic plasticity. However, the precise mathematical relationship between synaptic weights and the stimuli or `patterns' to which the network is exposed, remains elusive. At the same time, statistical physics and the theory of dynamical systems have provided substantial insight on network dynamics during processing. There remain questions on the optimum dynamical state or \emph{phase} of the network during information processing and how that state is simultaneous with a particular class of synaptic connectivity. Here, we review recent work on the above questions.


\end{abstract}


\section{Introduction}

\subsection{Information storage}

It is thought that neural networks that lie at a critical point between order and chaos maximize basic processing properties such as sensitivity, dynamic range, correlation length, information transfer, and susceptibility [1]. Multiple signatures of criticality have been proposed in the literature, one being power-law distributions of neuronal avalanches following a branching process similar to that seen in physical theories of avalanches [2]. However, it has recently been demonstrated that criticality is not a universal determinant of computational power; rather, critical networks are only well-suited for more complex tasks such as those that require integration of stimulus information over long timescales [1]. 

Although various sources have analyzed the relationship between network topology and the dynamical state of the network, the relationship remains unclear. Here, we seek to accomplish two objectives (i) develop a mathematical framework to analyze the dynamics of coupled excitatory and inhibitory populations of leaky integrate and fire neurons (ii) use the framework in (i) to tune the spiking dynamics of the ensemble from asynchronous and chaotic to bursty and critical. Criticality of networks will be assessed according to established measures e.g., the branching parameter, autocorrelation time, and avalanche size distributions. 

We conclude by analyzing the sensitivity of the network dynamical state to perturbations in the weights induced by spike-timing dependent plasticity (STDP).

\section{Methods}


\subsection{Excitatory-Inhibitory network model}

We make the initial assumption that subthreshold activity does not contribute to the firing of a neuron. Also, we consider a current-based model where each unit performs a weighted integration of spikes of its presynaptic partners. The voltage of a generic excitatory or inhibitory LIF neuron evolves according to

\begin{align*}
\frac{dV}{dt} &= -\frac{V-\theta}{\tau_{l}} + \sum_{i \neq j}W_{ij}z_{j} + \sum_{k \neq j} X_{kj}z_{k}\\
&= -\frac{V_{0}-\theta}{\tau_{l}} + W_{ab}n_{a}(t) + X_{a}n_{x}(t)\\
\end{align*}

where matrices $W$ and $X$ refer to recurrent and input connectivity matrices, respectively. The second equality comes from the assumption that the all synaptic connections between populations $a$ and $b$ have the same weight and we can pull the weight matrices out of the sum. The variable $z_{j} = \sum_{i} \delta_{j} (t-t_{i})$ is the spike train for reservoir neuron $j$ while $z_{k} = \sum_{i} \delta_{k} (t-t_{i})$ is the spike train for input neuron $k$. We often refer to $z_{j}^{t}$ as the \emph{observable state} of neuron $j$ at time $t$. Its value is computed at a time $t$ by passing the voltage $v_{j}^{t}$ through a form of activation function called the spike function $H(v_{j}^{t} - v_{th}) \rightarrow \{0,1\}$ where $H$ is the Heaviside step function.

\subsection{Parameteric control of spiking dynamics}

In complex networks, it becomes difficult to predict the trajectory of the voltage of any particular neuron $v_{j}^{t}$ and therefore its spiking activity. As a workaround, many have turned to mean field theories which have been successful in estimating macroscopic dynamics of the population firing rate. While this approch has proven valuable, it ignores the spiking mechanism of individual neurons and therefore their contribution to that activity. It is then difficult to discern whether such a theory is simply a mathematical object or can indeed be implemented by a spiking network. Recently, another mean field theory has been successful in making predictions on the dynamics of the mean population voltage rather than the firing rate, uncovering the role of microscopic variables in the population spiking dynamics. 

Let the variable $m_{\ell}(t) = \langle z_{\ell}(t) \rangle$ denote the population average of population $\ell$ as a function of time. Then the mean spiking activity can be written as

\begin{equation*}
m_{\ell}(t) = \langle H(V_{j}^{\ell}(t) - \theta)\rangle_{j}
\end{equation*}

which relates the fraction of active neurons in population $\ell$ to the average voltage in the population. Assuming that the voltage $V_{j}^{\ell}(t)$ is Gaussian distributed and has a time dependent mean $\mu_{\ell}$ and time independent variance $\sigma_{\ell}^{2}$, the above expression can be written more explicitly

\begin{align*}
m_{\ell}(t) &= \int_{\theta}^{\infty} P(V_{\ell})dV\\
&= \int_{\theta}^{\infty} \frac{1}{\sqrt{\pi}}\exp\left(-\frac{1}{2}\frac{\left(V_{\ell}-\mu_{\ell}\right)^{2}}{\sigma_{\ell}^{2}}\right)dV_{\ell}\\
&= \frac{1}{2}\cdot \mathrm{erfc}\left(\frac{V_{\ell}-\mu_{\ell}}{\sqrt{2}\sigma_{\ell}}\right)
\end{align*}


where $\mathrm{erfc}(z) = \frac{2}{\sqrt{\pi}}\int_{z}^{\infty} \exp\left(-t^{2}\right)dt$. The temporal dynamics of $m_{\ell}(t)$ are then

\begin{align}
\tau_{\ell}'\dot{m_{\ell}} &= -m_{\ell} + \frac{N_{\ell}}{2}\cdot \mathrm{erfc}\left(\frac{\theta-\mu_{\ell}}{\sqrt{2}\sigma_{\ell}}\right)
\end{align}

At this point we can rewrite eq. (1) in its mean-field form

\begin{equation}
\frac{d\mu_{\ell}}{dt} = -\frac{\mu_{\ell}-\theta}{\tau_{l}} + \sum_{b} W_{ab}n_{b}m_{b} + X_{0}\cdot\left(n_{0}m_{0} + \sqrt{\frac{n_{0}Q_{0}}{N_{\ell}}\xi_{\ell}(t)}\right)
\end{equation}

where we have replaced the Poisson input spike train with a term for Gaussian white noise as in [1]. At this point, the dynamics of the network are completely specified by equation (1) and (2) and we are left to determine their stability. It will be shown that, by tuning particular connectivity parameters, the system can transition from chaotic asychronous spiking to bursty critical dynamics. We have four state variables $\left(V_{\mathrm{E}}, V_{\mathrm{I}}, m_{\mathrm{E}}, m_{\mathrm{I}}\right)$. The Jacobian matrix for such a system is given by 

\begin{equation*}
J = \begin{bmatrix}
-\frac{1}{\tau_{E}} & 0 & W_{\mathrm{EE}}n_{\mathrm{I}} & W_{\mathrm{IE}}n_{\mathrm{E}} \\
0 &  -\frac{1}{\tau_{I}} & W_{\mathrm{EI}}n_{\mathrm{E}} & W_{\mathrm{II}}n_{\mathrm{I}} \\
-\frac{1}{\tau_{E}} & 0 & W_{\mathrm{IE}}n_{\mathrm{I}} & W_{\mathrm{EI}}n_{\mathrm{E}} \\
-\frac{1}{\tau_{E}} & 0 & W_{\mathrm{IE}}n_{\mathrm{I}} & W_{\mathrm{EI}}n_{\mathrm{E}} \\
\end{bmatrix}
\end{equation*}



\end{document}
