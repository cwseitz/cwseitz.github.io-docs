%
% This is a borrowed LaTeX template file for lecture notes for CS267,
% Applications of Parallel Computing, UCBerkeley EECS Department.
% Now being used for CMU's 10725 Fall 2012 Optimization course
% taught by Geoff Gordon and Ryan Tibshirani.  When preparing 
% LaTeX notes for this class, please use this template.
%
% To familiarize yourself with this template, the body contains
% some examples of its use.  Look them over.  Then you can
% run LaTeX on this file.  After you have LaTeXed this file then
% you can look over the result either by printing it out with
% dvips or using xdvi. "pdflatex template.tex" should also work.
%

\documentclass[twoside]{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

%
% ADD PACKAGES here:
%

\usepackage{amsmath,amsfonts,graphicx}

%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}

%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf NURB-31800: Cellular Neurobiology
	\hfill Winter 2021} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Lecturer: #3 \hfill Scribes: #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}
   \vspace*{4mm}
}
%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
% Also commands that create a suitable format for the reference list.
\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}%
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
\newcommand{\fig}[3]{
			\vspace{#2}
			\begin{center}
			Figure \thelecnum.#1:~#3
			\end{center}
	}
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

\newcommand\E{\mathbb{E}}

\begin{document}
\lecture{1}{January 12}{Madhur Tulsiani}{Clayton Seitz}

\section{Basic structure of the course}
Information theory is a mathematical framework for quantifying uncertainty. 
Codes are a way of providing certainty. Basic concepts such as entropy, KL-divergence, 
and mutual information and applications of each of them. This will be followed 
by some statistics and error correcting codes and applications in theoretical computer science. 
We will use lowercase symbols like $x,y,z$ to define random variables and sets of values as
$\mathcal{X}, \mathcal{Y}, \mathcal{Z}$ and distributions as $P,Q$. All logarithms
$log(x) = log_{2}(x)$.

\section{Background information}

We will first consider a discrete probability space $\Omega$ and we assign a probability
to each element $x$ of that space $P(x)$ where we define $x$ to be a random variable. 
We can define the entropy of such a variable as $H(x)$. 

\begin{equation}
\mathbf{E}(x) = \sum_{w \Omega} M(w)\mathcal{X}(w)
\end{equation}

\subsection{Convexity}

A convex set is set where if two points are in the set then all points on the line
connecting them is also in the set. A definition of a convex function is that
the average value of the function at two points is greater than or equal the 
average value of the function. Conversely, the function is concave if we flip
the inequality. 

\subsection{Jensen's inequality}

If a function $f$ takes a set $S$ onto real numbers and $f$ is convex, 
then the $\mathbf{E}(f(x))$ is geq the $f(\mathbf{E}(f(x))$. 

\section{Basic information theory}

\subsection{Shannon Entropy}

Intuitively, entropy is a measure of uncertainty of a random variable. 
or how much *information* it contains. To specify an outcome where 
there are $N$ possible outcomes, we need to specify $log(N)$ bits of information. 

The actual outcomes or values are not important, it is their distributions which 
determine the entropy. Importantly, the "bucketing" property, the information 
content when you have a set of choices where some choices are only available 
according to the sum of their probabilities. 

\begin{equation}
H(x) = -\sum P(x)\log P(x) \geq 0
\end{equation}

which has the property that $H(x) = \log N$ for a uniform distribution. We can 
prove that the uniform distribution maximizes the Shannon entropy using Jensen's 
inequality. 

\subsection{Source coding}

A code $c$ maps a set of $\mathcal{X}$ to a finite alphabet $Sigma$. It must be 
prefix-free in order to ensure that there is no ambiguity of where an element 
of the alphabet ends and another begins. Each encoding is referred to as a 
codword. This could be done by ensuring the code has a fixed blocked length. 
Let $\Sigma = {0,1}$, how could we ensure that the expected number of bits 
we need to communicate according to the probability distribution $P$. Now 
we sketch a proof of Kraft's inequality. 

In a prefix-free code we have that no code is a prefix for another code. 
Each codeword is a node in a binary search tree and if the code is prefix-free, 
we have to delete all children of a particular codeword. 


\end{document}







