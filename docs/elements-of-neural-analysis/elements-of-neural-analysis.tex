%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% LaTeX book template 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[a4paper,11pt]{book}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\newtheorem{definition}{Definition}

\newenvironment{dedication}
{
   \cleardoublepage
   \thispagestyle{empty}
   \vspace*{\stretch{1}}
   \hfill\begin{minipage}[t]{0.66\textwidth}
   \raggedright
}
{
   \end{minipage}
   \vspace*{\stretch{3}}
   \clearpage
}

\makeatletter
\renewcommand{\@chapapp}{}% Not necessary...
\newenvironment{chapquote}[2][2em]
  {\setlength{\@tempdima}{#1}%
   \def\chapquote@author{#2}%
   \parshape 1 \@tempdima \dimexpr\textwidth-2\@tempdima\relax%
   \itshape}
  {\par\normalfont\hfill--\ \chapquote@author\hspace*{\@tempdima}\par\bigskip}
\makeatother

\title{\Huge \textbf{Neural dynamics of vision}  \\ \huge A computational perspective}

\author{\textsc{Clayton Seitz}\thanks{\url{cwseitz.github.io}}}

\begin{document}

\frontmatter
\maketitle

\begin{dedication}
Dedicated to Calvin and Hobbes.
\end{dedication}

\tableofcontents
\mainmatter

\chapter*{Preface}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis risus ante, auctor et pulvinar non, posuere ac lacus. Praesent egestas nisi id metus rhoncus ac lobortis sem hendrerit. Etiam et sapien eget lectus interdum posuere sit amet ac urna.

\section*{Un-numbered sample section}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis risus ante, auctor et pulvinar non, posuere ac lacus. Praesent egestas nisi id metus rhoncus ac lobortis sem hendrerit. Etiam et sapien eget lectus interdum posuere sit amet ac urna. Aliquam pellentesque imperdiet erat, eget consectetur felis malesuada quis. Pellentesque sollicitudin, odio sed dapibus eleifend, magna sem luctus turpis.

\section*{Another sample section}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis risus ante, auctor et pulvinar non, posuere ac lacus. Praesent egestas nisi id metus rhoncus ac lobortis sem hendrerit. Etiam et sapien eget lectus interdum posuere sit amet ac urna. Aliquam pellentesque imperdiet erat, eget consectetur felis malesuada quis. Pellentesque sollicitudin, odio sed dapibus eleifend, magna sem luctus turpis, id aliquam felis dolor eu diam. Etiam ullamcorper, nunc a accumsan adipiscing, turpis odio bibendum erat, id convallis magna eros nec metus.

\section*{Structure of book}
% You might want to add short description about each chapter in this book.
Each unit will focus on <SOMETHING>.

\section*{About the companion website}
The website\footnote{\url{https://github.com/amberj/latex-book-template}} for this file contains:
\begin{itemize}
  \item A link to (freely downlodable) latest version of this document.
  \item Link to download LaTeX source for this document.
  \item Miscellaneous material (e.g. suggested readings etc).
\end{itemize}

\section*{Acknowledgements}
\begin{itemize}
\item A special word of thanks goes to Professor Don Knuth\footnote{\url{http://www-cs-faculty.stanford.edu/~uno/}} (for \TeX{}) and Leslie Lamport\footnote{\url{http://www.lamport.org/}} (for \LaTeX{}).
\item I'll also like to thank Gummi\footnote{\url{http://gummi.midnightcoding.org/}} developers and LaTeXila\footnote{\url{http://projects.gnome.org/latexila/}} development team for their awesome \LaTeX{} editors.
\item I'm deeply indebted my parents, colleagues and friends for their support and encouragement.
\end{itemize}
\mbox{}\\
%\mbox{}\\
\noindent Amber Jain \\
\noindent \url{http://amberj.devio.us/}

\chapter{The Neural Code}

\begin{chapquote}{Author's name, \textit{Source of this quote}}
``This is a quote and I don't know who said this.''
\end{chapquote}

\section{Section heading}

\chapter{Learning Theory}

\begin{chapquote}{Author's name, \textit{Source of this quote}}
``This is a quote and I don't know who said this.''
\end{chapquote}

\section{Section heading}


\chapter{Biologically-Inspired Computer Vision}

\begin{chapquote}{Author's name, \textit{Source of this quote}}
``This is a quote and I don't know who said this.''
\end{chapquote}

\section{Natural Image Statistics}
\section{Gabor Analysis}

\chapter{Semantic Coding}

\begin{chapquote}{Author's name, \textit{Source of this quote}}
``This is a quote and I don't know who said this.''
\end{chapquote}

\section{Section heading}

\chapter{Information and Coding Theory}

\begin{chapquote}{Claude Shannon}
``We may have knowledge of the past but cannot control it; we may control the future but have no knowledge of it''
\end{chapquote}

\section{Introduction}

Information theory is a framework first introduced by Claude Shannon's seminal paper $\textit{A mathematical theory of communication}$  published in 1948. At it's core, information theory makes the intuitive concept of $\textit{information}$ mathematically rigorous and forms the foundation of many modern communication systems. Neural circuits in the visual system are an especially interesting example of such a communication system. Therefore, in this section, the information theoretic concepts necessary for studying neural circuits are introduced. 

\section{Entropy}

The concept of entropy is not exclusive to information theory; rather, it is used widely in disciplines such as physics and mathematical statistics. In fact, entropy was originally defined in statistical physics when Ludwig Boltzmann gave a statistical description of a thermodynamic system of particles. Since this is arguably the more intuitive path as opposed an entirely mathematical description, I will follow a similar line of reasoning in the following paragraphs.

In every application, the entropy $\mathbf{H}$ is a measure of uncertainty or how much information is contained in a random variable $x$. In information theory, the entropy is a property of a probability distribution of a random variable $P(x)$ where $x$ can take on continuous or discrete values. For the discrete case, we can express the entropy in bits 

\begin{equation}
\textbf{H} = \sum_{x\in S} P(x)\log\frac{1}{P(x)}
\end{equation}

where the set $S$ spans the entire space of possible discrete values of $x$. We can go on to derive upper and lower bounds for the entropy. Notice that $\mathbf{H} \geq 0$ since $P(x) \leq 1$ and therefore $\log P(x) \leq 0$  for all $x$. At the same time, if we define a variable $Y = \frac{1}{\log x}$, we can write

\begin{eqnarray*}
\textbf{H} &=& \mathbf{E}[\log Y]\\
&\leq & \log \mathbf{E}[Y]\\
&=& \log \sum_{y}P(x) \frac{1}{P(x)}\\
&=& \log |S|
\end{eqnarray*}

which is just the entropy of a uniform distribution. 

\section{Joint Entropy}

\begin{eqnarray*}
\textbf{H}(X,Y) &=& \sum_{x,y} P(x,y)\log \frac{1}{P(x,y)} \\
&=& \sum_{x,y} P(x)P(y|x)\log \frac{1}{P(x)P(y|x)}\\
&=& \sum_{x,y}  P(x)P(y|x)\log \frac{1}{P(x)} + \sum_{x,y}  P(x)P(y|x)\log\frac{1}{P(y|x)} \\
&=& \sum_{x,y}  P(x)P(y|x)\log \frac{1}{P(x)} + \sum_{x}  P(x)\sum_{y}P(y|x)\log\frac{1}{P(y|x)} \\
&=& H(X) + H(Y|X)
\end{eqnarray*}

This result defines the $\mathbf{chain \; rule}$ for entropy.

\section{Source Coding}

\begin{definition}
A code of a set $S$ that uses an alphabet $\Omega$ is a map $C: S\rightarrow \Omega$ that assigns each element of $S$ a finite string over the alphabet $\Omega$. We say that the mapping $C$ is $\mathbf{prefix \; free}$ if for all pairs $x,y \in S$ where $x \neq y$, $C(x)$ is not a prefix of $C(y)$.
\end{definition}

Most of the time the alphabet $\Omega$ we use is the set $0,1$.

\subsection{Kraft's Inequality}

\begin{definition}
For a binary code, there exists a prefix free code $C$ with codeword lengths $l_{i}$ if and only if
\end{definition}

\begin{equation}
\sum_{i} 2^{-l_{i}} \leq 1
\end{equation}

At this point we would like to apply the concept of entropy to source coding. Indeed, it is true that if we have a random variable $X$ over the set $S$, the minimum number of bits it will take us to communicate the value of $X$ on average is the entropy $H(X)$.

\begin{proof}
  The expected number of bits to communicate $X$ is given by $\sum_{x}p(x)|C(x)|$
  \begin{align*}
    H(X) - \sum_{x}P(x)|C(x)| &= \sum P(x)[\log\frac{1}{P(x)} - |C(x)|] \\
    &= \sum P(x)\log\frac{1}{P(x)2^{|C(x)|}} \\
    &\geq \log\sum P(x)\frac{1}{P(x)2^{|C(x)|}}  \\
    &= \log\sum \frac{1}{2^{|C(x)|}}  \\
    &\leq 0 &&\qedhere
  \end{align*}
  by Kraft's inequality for prefix-free codes.
\end{proof}


\subsection{Jensen's Inequality}

Jensen's inequality is a statement about convexity. Consider a binary variable $x$ that takes the value 0 with probability $\alpha$ and value 1 with probability $1-\alpha$.

\[x= \begin{cases} 
      0 & \alpha \\
      1 & 1-\alpha \\
   \end{cases}
\]

A function $f$ of the variable $x$ is said to be $\textit{convex}$ if the following inequality holds

\begin{equation*}
\alpha f(x) + (1-\alpha)f(y) \leq f(\alpha x + (1-\alpha)y)
\end{equation*}

which when generalized for an arbitrary random variable $x$ forms Jensen's inequality

\begin{equation}
\mathbf{E}[f(x)] \leq f(\mathbf{E}[x])
\end{equation}

\subsection{The Data-Processing Inequality}

The data-processing inequality states that for any function $f$ s.t. $y = f(X)$, we have that $H(Y) \geq H(X)$. In plain english, that means that the process of transforming $X$ can never decrease it's entropy. Recall from our analysis of joint entropy above that

\begin{eqnarray}
H(X,Y) &=& H(X) + H(Y|X)\\
&=& H(Y) + H(X|Y)
\end{eqnarray}

If we can show that $H(y|x) = 0$ then the data-processing inequality holds. This is more obvious if you notice that if $y = f(x)$ then the distribution $P(x,y) = P(x)$ or $P(y|x) = 0$. 

In terms of our venn diagram above, $H(y)$ is a *subset* of $H(x)$ which also implies the following for the mutual information: $I(x,y) = H(y)$. 

\subsection{Example 1: Applying Jensen's Inequality}

Let's consider a function $f: \mathbb{R} \rightarrow \mathbb{R}$. Using Jensen's inequality, we can prove that $f=x^{2}$ or $f=x\log x$ are convex functions. Let's begin by applying it to $x^{2}$ for a general normalized probability distribution $p(x)$.

\begin{eqnarray*}
\int p(x)f(x)dx & = & \int x^{2}p(x)dx\\
&=& x^{2} - 2\int xdx\\
&=& 0 \leq x^{2} \; \forall x
\end{eqnarray*}

We have a similar proof for $f(x) = x\log x$

\begin{eqnarray*}
\int p(x)f(x)dx & = & \int x\log x\; p(x)dx\\
&=& x\log x - \int \frac{d}{dx}x\log x\;dx\\
&=& 0 \leq \mu \log \mu
\end{eqnarray*}

where $\mu = \mathbf{E}[x] \geq 0$ since $f$ is only defined on $[0, \infty]$.

\subsection{Example 2: Proving Cauchy-Schwarz}

A common form of the Cauchy-Schwarz inequality states that for two vectors $u$ and $v$, we have

\newcommand\norm[1]{\left\lVert#1\right\rVert}
\begin{eqnarray*}
u \cdot v  \leq \norm{u}\norm{v}
\end{eqnarray*}

\section{Error Correcting Codes}

\chapter{Microscopy and Image Analysis}

\begin{chapquote}{Author's name, \textit{Source of this quote}}
``This is a quote and I don't know who said this.''
\end{chapquote}

\section{Section heading}

\end{document}
