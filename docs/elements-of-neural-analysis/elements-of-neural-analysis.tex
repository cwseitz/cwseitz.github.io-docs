%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% LaTeX book template 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[a4paper,11pt]{book}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}

\newenvironment{dedication}
{
   \cleardoublepage
   \thispagestyle{empty}
   \vspace*{\stretch{1}}
   \hfill\begin{minipage}[t]{0.66\textwidth}
   \raggedright
}
{
   \end{minipage}
   \vspace*{\stretch{3}}
   \clearpage
}

\makeatletter
\renewcommand{\@chapapp}{}% Not necessary...
\newenvironment{chapquote}[2][2em]
  {\setlength{\@tempdima}{#1}%
   \def\chapquote@author{#2}%
   \parshape 1 \@tempdima \dimexpr\textwidth-2\@tempdima\relax%
   \itshape}
  {\par\normalfont\hfill--\ \chapquote@author\hspace*{\@tempdima}\par\bigskip}
\makeatother

\title{\Huge \textbf{Neural dynamics of vision}  \\ \huge A computational perspective}

\author{\textsc{Clayton Seitz}\thanks{\url{cwseitz.github.io}}}

\begin{document}

\frontmatter
\maketitle

\begin{dedication}
Dedicated to Calvin and Hobbes.
\end{dedication}

\tableofcontents
\mainmatter

\chapter*{Preface}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis risus ante, auctor et pulvinar non, posuere ac lacus. Praesent egestas nisi id metus rhoncus ac lobortis sem hendrerit. Etiam et sapien eget lectus interdum posuere sit amet ac urna.

\section*{Un-numbered sample section}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis risus ante, auctor et pulvinar non, posuere ac lacus. Praesent egestas nisi id metus rhoncus ac lobortis sem hendrerit. Etiam et sapien eget lectus interdum posuere sit amet ac urna. Aliquam pellentesque imperdiet erat, eget consectetur felis malesuada quis. Pellentesque sollicitudin, odio sed dapibus eleifend, magna sem luctus turpis.

\section*{Another sample section}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis risus ante, auctor et pulvinar non, posuere ac lacus. Praesent egestas nisi id metus rhoncus ac lobortis sem hendrerit. Etiam et sapien eget lectus interdum posuere sit amet ac urna. Aliquam pellentesque imperdiet erat, eget consectetur felis malesuada quis. Pellentesque sollicitudin, odio sed dapibus eleifend, magna sem luctus turpis, id aliquam felis dolor eu diam. Etiam ullamcorper, nunc a accumsan adipiscing, turpis odio bibendum erat, id convallis magna eros nec metus.

\section*{Structure of book}
% You might want to add short description about each chapter in this book.
Each unit will focus on <SOMETHING>.

\section*{About the companion website}
The website\footnote{\url{https://github.com/amberj/latex-book-template}} for this file contains:
\begin{itemize}
  \item A link to (freely downlodable) latest version of this document.
  \item Link to download LaTeX source for this document.
  \item Miscellaneous material (e.g. suggested readings etc).
\end{itemize}

\section*{Acknowledgements}
\begin{itemize}
\item A special word of thanks goes to Professor Don Knuth\footnote{\url{http://www-cs-faculty.stanford.edu/~uno/}} (for \TeX{}) and Leslie Lamport\footnote{\url{http://www.lamport.org/}} (for \LaTeX{}).
\item I'll also like to thank Gummi\footnote{\url{http://gummi.midnightcoding.org/}} developers and LaTeXila\footnote{\url{http://projects.gnome.org/latexila/}} development team for their awesome \LaTeX{} editors.
\item I'm deeply indebted my parents, colleagues and friends for their support and encouragement.
\end{itemize}
\mbox{}\\
%\mbox{}\\
\noindent Amber Jain \\
\noindent \url{http://amberj.devio.us/}

\chapter{The Neural Code}

\begin{chapquote}{Author's name, \textit{Source of this quote}}
``This is a quote and I don't know who said this.''
\end{chapquote}

\section{Section heading}

\chapter{Learning Theory}

\begin{chapquote}{Author's name, \textit{Source of this quote}}
``This is a quote and I don't know who said this.''
\end{chapquote}

\section{Section heading}


\chapter{Biologically-Inspired Computer Vision}

\begin{chapquote}{Author's name, \textit{Source of this quote}}
``This is a quote and I don't know who said this.''
\end{chapquote}

\section{Natural Image Statistics}
\section{Gabor Analysis}

\chapter{Semantic Coding}

\begin{chapquote}{Author's name, \textit{Source of this quote}}
``This is a quote and I don't know who said this.''
\end{chapquote}

\section{Section heading}

\chapter{Information and Coding Theory}

\begin{chapquote}{Claude Shannon}
``We may have knowledge of the past but cannot control it; we may control the future but have no knowledge of it''
\end{chapquote}

\section{Introduction}

Information theory is a framework first introduced by Claude Shannon's seminal paper $\textit{A mathematical theory of communication}$  published in 1948. At it's core, information theory makes the intuitive concept of $\textit{information}$ mathematically rigorous and forms the foundation of many modern communication systems. Neural circuits in the visual system are an especially interesting example of such a communication system. Therefore, in this section, the information theoretic concepts necessary for studying neural circuits are introduced. 

\section{Entropy}

The concept of entropy is not exclusive to information theory; rather, it is used widely in disciplines such as physics and mathematical statistics. In fact, entropy was originally defined in statistical physics when Ludwig Boltzmann gave a statistical description of a thermodynamic system of particles. Since this is arguably the more intuitive path as opposed an entirely mathematical description, I will follow a similar line of reasoning in the following paragraphs.

In every application, the entropy $\mathbf{H}$ is a measure of uncertainty or how much information is contained in a random variable $x$. In information theory, the entropy is a property of a probability distribution of a random variable $P(x)$ where $x$ can take on continuous or discrete values. For the discrete case, we can express the entropy in bits 

\begin{equation}
\textbf{H} = \sum_{x\in S} P(x)\log\frac{1}{P(x)}
\end{equation}

where the set $S$ spans the entire space of possible discrete values of $x$. We can go on to derive upper and lower bounds for the entropy. Notice that $\mathbf{H} \geq 0$ since $P(x) \leq 1$ and therefore $\log P(x) \leq 0$  for all $x$. At the same time, if we define a variable $Y = \frac{1}{\log x}$, we can write

\begin{eqnarray*}
\textbf{H} &=& \mathbf{E}[\log Y]\\
&\leq & \log \mathbf{E}[Y]\\
&=& \log \sum_{y}P(x) \frac{1}{P(x)}\\
&=& \log |S|
\end{eqnarray*}

which is just the entropy of a uniform distribution. 

\subsection{Joint and Conditional Entropy}

In this section, we discuss joint and conditional entropy which are really just two sides of the same coin

\begin{eqnarray*}
\textbf{H}(X,Y) &=& \sum_{x,y} P(x,y)\log \frac{1}{P(x,y)} \\
&=& \sum_{x,y} P(x)P(y|x)\log \frac{1}{P(x)P(y|x)}\\
&=& \sum_{x,y}  P(x)P(y|x)\log \frac{1}{P(x)} + \sum_{x,y}  P(x)P(y|x)\log\frac{1}{P(y|x)} \\
&=& \sum_{x,y}  P(x)P(y|x)\log \frac{1}{P(x)} + \sum_{x}  P(x)\sum_{y}P(y|x)\log\frac{1}{P(y|x)} \\
&=& H(X) + H(Y|X)
\end{eqnarray*}

This result defines the $\mathbf{chain \; rule}$ for entropy. We typically refer to the term $H(Y|X)$ as the $\mathbf{conditional \; entropy}$. It can be calculated independently using the following definition

\begin{eqnarray*}
H(X|Y) &=& \mathbf{E}_{y} H(X|Y=y)\\
&=& \mathbf{E}_{y} \sum_{x} P(X|Y=y)\log \frac{1}{P(X|Y=y)}\\
\end{eqnarray*}


Furthermore, it can be shown that the chain rule derived above applies to a tuple of random variables longer than two. 

\begin{equation*}
H(X_{1}, ...\;, X_{m}) = H(X_{1}) + H(X_{2}|X_{1}) + H(X_{3}|X_{2},X_{1}) \;...\; H(X_{m}|X_{1} ... X_{m-1}) 
\end{equation*}

Recalling that conditioning reduces entropy or does nothing at all, we can write

\begin{equation*}
H(X_{1}, ...\;, X_{m}) \leq H(X_{1}) + H(X_{2}) +  \;...\; + H(X_{m}) 
\end{equation*}

which is referred to as the $\mathbf{subadditivity}$ property of entropy. We should also address what to do when we need to compute the entropy of a joint distribution $(X,Y)$ conditioned on a variable $Z$ or when $Z$ itself is conditioned on a joint distribution. These two things are related by using the chain rule for joint entropy

\begin{eqnarray*}
H(X,Y|Z) &=& H(X,Y) + H(Z|X,Y)\\
\end{eqnarray*}


Now we will prove that conditioning the distribution of a random variable $X$ on another variable $Y$ i.e. can reduce the entropy of $X$. What we need to show is that $H(X|Y) - H(X) \leq 0$.


\section{KL-Divergence and Mutual Information}

The Kullbeck-Leiber distance or $\mathbf{KL\;Divergence}$ is a measure of the distance between two distributions over a random variable $X$. Assume we have two distributions $P,Q$ on a random variable $X$ where $P$ is the correct distribution on $X$ and $Q$ is an incorrect distribution. By definition, the KL-Divergence $D_{KL}(P||Q)$ is the extra information (bits) it takes to communicate $X$ when using the incorrect distribution $Q$. To be precise, $H(Q) = H(P) + D_{KL}(P||Q)$.

\begin{definition}
The KL-Divergence is 
\begin{equation*}
D_{KL}(P||Q) = \sum_{X} P(X) \log \frac{P(X)}{Q(X)}
\end{equation*}
\end{definition}

Furthermore, an indispensable tool in information theory is the idea of $\mathbf{mutual\;information}$ which, as the name suggests, measures the amount of overlapping information in a pair of random variables. More formally, it is the KL-Divergence between the joint distribution of the pair of variables and the product of their marginal distributions (which implies they are independent)

\begin{definition}
The mutual information is 
\begin{eqnarray*}
I(X;Y) &=& D_{KL}(P(X,Y)||P(X)P(Y)) \\
&=& \sum_{x}\sum_{y} P(X,Y) \log \frac{P(X,Y)}{P(X)P(Y)}
\end{eqnarray*}
\end{definition}

A very useful property of the mutual information is that it is strongly related to conditional entropy and statistical independence. Conditional entropy tells us how much information is contained in a variable $X$ which its distribution is conditioned on $Y$. We might expect that this conditioning doesn't really have an effect if $X$ and $Y$ are completely independent. Indeed,

\begin{eqnarray*}
I(X;Y) &=& D_{KL}(P(X,Y)||P(X)P(Y)) \\
&=& \sum P(x,y) \log \frac{P(x,y)}{P(x)P(y)}\\
&=& \sum P(x)\log \frac{1}{P(x)} + \sum P(x,y)\log \frac{P(x,y)}{P(y)} \\
 &=& H(X) - H(X|Y)
\end{eqnarray*}

Note that this result implies that $I(X;Y) = I(Y;X)$. We will next address the mutual information between a distribution on $X$ and a joint distribution $(Y,Z)$ making use of the relationship derived above. 

\begin{eqnarray*}
I(X;(Y,Z)) &=& H(X) - H(X|Y,Z)\\
&=& H(X) + H(Y,Z|X) - H(Y,Z)\\
\end{eqnarray*}

Finally, we look at the mutual information between a distribution on $X$ and a conditional distribution $Y|Z$.

\subsection{The Data-Processing Inequality}

The data-processing inequality states that if a function operates on a random variable $X$ it can only decrease its entropy. That is, for any function $f$ s.t. $Y= f(X)$, we have that $H(Y) \geq H(X)$. We can prove that this is true using the mutual information $I(X;Y)$.

\section{Source Coding}

\begin{definition}
A code of a set $S$ that uses an alphabet $\Omega$ is a map $C: S\rightarrow \Omega$ that assigns each element of $S$ a finite string over the alphabet $\Omega$. We say that the mapping $C$ is $\mathbf{prefix \; free}$ if for all pairs $x,y \in S$ where $x \neq y$, $C(x)$ is not a prefix of $C(y)$.
\end{definition}

Most of the time the alphabet $\Omega$ we use is the set $0,1$.

\subsection{Kraft's Inequality}

\begin{definition}
For a binary code, there exists a prefix free code $C$ with codeword lengths $l_{i}$ if and only if
\end{definition}

\begin{equation}
\sum_{i} 2^{-l_{i}} \leq 1
\end{equation}

At this point we would like to apply the concept of entropy to source coding. Indeed, it is true that if we have a random variable $X$ over the set $S$, the minimum number of bits it will take us to communicate the value of $X$ on average is the entropy $H(X)$.

\begin{proof}
  The expected number of bits to communicate $X$ is given by $\sum_{x}p(x)|C(x)|$
  \begin{align*}
    H(X) - \sum_{x}P(x)|C(x)| &= \sum P(x)[\log\frac{1}{P(x)} - |C(x)|] \\
    &= \sum P(x)\log\frac{1}{P(x)2^{|C(x)|}} \\
    &\geq \log\sum P(x)\frac{1}{P(x)2^{|C(x)|}}  \\
    &= \log\sum \frac{1}{2^{|C(x)|}}  \\
    &\leq 0 &&\qedhere
  \end{align*}
  by Kraft's inequality for prefix-free codes.
\end{proof}

\subsection{Source Coding Theorem}

So far we have seen how to construct a prefix-free code and that the absolute lower bound on the number of bits it takes to encode a random variable is its entropy. Next, we would like to answer the following question: how do we actually design a code to communicate a random variable $X$ so that it approaches this lower bound?  The answer is addressed by the $\it{fundamental \; source \; coding \; theorem}$ 

\begin{theorem}
For all $epsilon > 0$ there is a $n_{0} \leq n$ such that given $n$ instances of a variable $X$ it is possible to communicate $X$ with $H(X) + \epsilon$ bits on average.
\end{theorem}

This means that we can approach the entropy by increasing $n$.


\subsection{Jensen's Inequality}

Jensen's inequality is a statement about convexity. Consider a binary variable $x$ that takes the value 0 with probability $\alpha$ and value 1 with probability $1-\alpha$.

\[x= \begin{cases} 
      0 & \alpha \\
      1 & 1-\alpha \\
   \end{cases}
\]

A function $f$ of the variable $x$ is said to be $\textit{convex}$ if the following inequality holds

\begin{equation*}
\alpha f(x) + (1-\alpha)f(y) \leq f(\alpha x + (1-\alpha)y)
\end{equation*}

which when generalized for an arbitrary random variable $x$ forms Jensen's inequality

\begin{equation}
\mathbf{E}[f(x)] \leq f(\mathbf{E}[x])
\end{equation}


\subsection{Example 1: Applying Jensen's Inequality}

Let's consider a function $f: \mathbb{R} \rightarrow \mathbb{R}$. Using Jensen's inequality, we can prove that $f=x^{2}$ or $f=x\log x$ are convex functions. Let's begin by applying it to $x^{2}$ for a general normalized probability distribution $p(x)$.

\begin{eqnarray*}
\int p(x)f(x)dx & = & \int x^{2}p(x)dx\\
&=& x^{2} - 2\int xdx\\
&=& 0 \leq x^{2} \; \forall x
\end{eqnarray*}

We have a similar proof for $f(x) = x\log x$

\begin{eqnarray*}
\int p(x)f(x)dx & = & \int x\log x\; p(x)dx\\
&=& x\log x - \int \frac{d}{dx}x\log x\;dx\\
&=& 0 \leq \mu \log \mu
\end{eqnarray*}

where $\mu = \mathbf{E}[x] \geq 0$ since $f$ is only defined on $[0, \infty]$.

\subsection{Example 2: Proving Cauchy-Schwarz}

A common form of the Cauchy-Schwarz inequality states that for two vectors $u$ and $v$, we have

\newcommand\norm[1]{\left\lVert#1\right\rVert}
\begin{eqnarray*}
u \cdot v  \leq \norm{u}\norm{v}
\end{eqnarray*}

\section{Error Correcting Codes}

\chapter{Microscopy and Image Analysis}

\begin{chapquote}{Author's name, \textit{Source of this quote}}
``This is a quote and I don't know who said this.''
\end{chapquote}

\section{Section heading}

\end{document}
