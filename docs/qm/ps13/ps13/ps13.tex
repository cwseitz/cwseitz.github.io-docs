\documentclass[12pt]{article}
\usepackage{amsmath} % AMS Math Package
\usepackage{bm}
\usepackage{amsthm} % Theorem Formatting
\usepackage{amssymb}    % Math symbols such as \mathbb
\usepackage{graphicx} % Allows for eps images
\usepackage[dvips,letterpaper,margin=1in,bottom=0.7in]{geometry}
\usepackage{tensor}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{physics}
\usepackage{amsmath, amssymb, graphics, setspace}

\newcommand{\mathsym}[1]{{}}
\newcommand{\unicode}[1]{{}}

\newcounter{mathematicapage}

\newtheorem{p}{Problem}
\usepackage{cancel}
\newtheorem*{lem}{Lemma}
\theoremstyle{definition}
\newtheorem*{dfn}{Definition}
 \newenvironment{s}{%\small%
        \begin{trivlist} \item \textbf{Solution}. }{%
            \hspace*{\fill} $\blacksquare$\end{trivlist}}%


\begin{document}

 {\noindent\Huge\bf  \\[0.5\baselineskip] {\fontfamily{cmr}\selectfont  Homework 3}         }\\[2\baselineskip] % Title
{ {\bf \fontfamily{cmr}\selectfont Quantum Mechanics}\\ {\textit{\fontfamily{cmr}\selectfont     \today}}}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    {\large \textsc{C Seitz}
\\[1.4\baselineskip] 


\begin{p}
Problem 2.48
\end{p}

\begin{s}

The polar decomposition of a matrix is $A = UJ = KU$ where $U$ is a unitary operator and $J,K$ are positive operators that satisfy $J = \sqrt{A^{\dagger}A}$ and $K = \sqrt{AA^{\dagger}}$. If $P$ is itself a positive matrix we can immediately say that its polar decomposition is $P = IP = PI$. If the matrix to decompose is unitary then of course $UU^{\dagger} = U^{\dagger}U = I$ so its decomposition is itself. If the matrix to decompose is Hermitian, then $H = H^{\dagger}$ and 

\begin{equation*}
J = K = \sqrt{H^{2}} = \sqrt{\sum_{i}\lambda_{i}^{2}\ket{i}\bra{i}} = \sum_{i}|\lambda_{i}|\ket{i}\bra{i}
\end{equation*}

and its polar decomposition is therefore $U\sum_{i}|\lambda_{i}|\ket{i}\bra{i}$ or $\sum_{i}|\lambda_{i}|\ket{i}\bra{i}U$.

\end{s}

\begin{p}
Problem 2.49
\end{p}

\begin{s}

The polar decomposition is $A = UJ$. The spectral decomposition of $J$ is 

\begin{equation*}
J = \sqrt{\sum_{i}\lambda_{i}\lambda_{i}^{*}\ket{i}\bra{i}} = \sum_{i}|\lambda_{i}|\ket{i}\bra{i}
\end{equation*}

For the unitary matrix $U$, we have

\begin{equation*}
U = \sum_{j}\lambda_{j}\ket{j}\bra{j}
\end{equation*}

Therefore the product $UJ$ reads

\begin{align*}
UJ &= \left(\sum_{j}\lambda_{j}\ket{j}\bra{j}\right)\left(\sum_{i}|\lambda_{i}|\ket{i}\bra{i}\right)\\
&= \sum_{ij} |\lambda_{i}|\lambda_{j}\ket{i}\bra{i}\ket{j}\bra{j}
\end{align*}



\end{s}

\begin{p}
Problem 2.50
\end{p}

\begin{s}

\begin{align*}
A = \begin{pmatrix}1&0\\1&1\end{pmatrix}
\end{align*}

First, consider

\begin{equation*}
A^{\dagger}A = \begin{pmatrix}2&1\\1&1\end{pmatrix}\;\;\; AA^{\dagger} = \begin{pmatrix}1&1\\1&2\end{pmatrix}
\end{equation*}

\begin{align*}
J = \sqrt{A^{\dagger}A} = 
\end{align*}

\end{s}

\begin{p}
Problem 2.51
\end{p}

\begin{s}
The Hadamard gate $H$ is unitary if $H^{\dagger} = H^{-1}$. It is easy to see that

\begin{equation*}
H^{\dagger} = H = \frac{1}{\sqrt{2}}
\begin{pmatrix}
1 & 1\\ 1& -1
\end{pmatrix}
\end{equation*}

It's inverse is 

\begin{equation*}
H^{-1} =-\frac{1}{\sqrt{2}}
\begin{pmatrix}
-1 & -1\\ -1& 1
\end{pmatrix} = H
\end{equation*}

\end{s}

\begin{p}
Problem 2.52
\end{p}

\begin{s}
\begin{equation*}
H^{2} =  \frac{1}{\sqrt{2}}
\begin{pmatrix}
1 & 1\\ 1& -1
\end{pmatrix} \frac{1}{\sqrt{2}}
\begin{pmatrix}
1 & 1\\ 1& -1
\end{pmatrix} = 
\begin{pmatrix}
1 & 0\\ 0& 1
\end{pmatrix}
\end{equation*}
\end{s}

\begin{p}
Problem 2.53
\end{p}

\begin{s}
Writing out the characteristic equation gives that the eigenvalues are $\lambda = \pm \sqrt{2}$. 
\end{s}

\begin{p}
Problem 2.54
\end{p}

\begin{s}
Since the two operators commute, they are simultaneously diagonalizable. Consider the following spectral decompositions

\begin{equation*}
A = \sum_{n}a_{n}\ket{n}\bra{n}
\end{equation*}

\begin{equation*}
B = \sum_{n}b_{n}\ket{n}\bra{n}
\end{equation*}

Therefore, it must be true that

\begin{equation*}
A+B = \sum_{n}(a_{n}+b_{n})\ket{n}\bra{n}
\end{equation*}

Now these matrices are Hermitian so their eigenvectors are orthogonal, and the product of matrix exponentials is just

\begin{align*}
\exp(A)\exp(B) &= \left(\sum_{n}\exp(a_{n})\ket{n}\bra{n}\right)\left(\sum_{m}\exp(b_{m})\ket{m}\bra{m}\right)\\
&= \sum_{m,n} \delta_{mn}\exp(a_{n})\exp(b_{m})\ket{n}\bra{m}\\
&= \sum_{n}\exp(a_{n})\exp(b_{n})\ket{n}\bra{n}\\
&= \sum_{n}\exp(a_{n}+b_{n})\ket{n}\bra{n}\\
&= \exp(A+B)
\end{align*}


\end{s}

\begin{p}
Problem 2.55
\end{p}

\begin{s}

\begin{align*}
UU^{\dagger} &= \exp\left(\frac{-iH(t_2-t_1)}{\hbar}\right)\exp\left(\frac{iH(t_2-t_1)}{\hbar}\right)\\
&= \left(\sum_{n} \exp(\frac{-i E_{n}(t_2-t_1)}{\hbar})\ket{n}\bra{n}\right)\left(\sum_{m} \exp(\frac{i E_{m}(t_2-t_1)}{\hbar})\ket{m}\bra{m}\right)\\
&= \sum_{m,n}\delta_{mn}\ket{n}\bra{m}\\
&= \sum_{n}\ket{n}\bra{n} = I
\end{align*}

where $H$ is a Hermitian operator. 


\end{s}

\begin{p}
Problem 2.56
\end{p}

\begin{s}

$U$ is unitary so its eigenvalues $u_{n}$ have unit norm, which means

\begin{equation*}
K = -i\log(U) = -i\sum_{n}\log(u_{n})\ket{n}\bra{n} = \sum_{n}\theta \ket{n}\bra{n}
\end{equation*}

since

\begin{equation*}
\log(u_{n}) = \log(|u_{n}|e^{i\theta}) = \log(|u_{n}|) + i\theta = i\theta
\end{equation*}

Therefore, $K = K^{\dagger}$ since $\theta\in\mathbb{R}$.

\end{s}

\begin{p}
Problem 2.57
\end{p}

\begin{s}


\begin{align*}
L_{l}\ket{\alpha} = \frac{\ell\ket{l}}{|\ell|}
\end{align*}

\begin{align*}
M_{m}\frac{\ell\ket{l}}{|\ell|} = \frac{m\ell}{|m||\ell|}\ket{m}
\end{align*}

which is equivalent to

\begin{align*}
N_{m\ell}\ket{\alpha} &= M_{m}L_{\ell}\ket{\alpha}\\
&= \frac{\ket{m}\bra{m}\ket{\ell}\bra{\ell}}{|m||\ell|}\ket{\alpha}\\
&= \frac{\ell\ket{m}\bra{m}}{|m||\ell|}\ket{\ell}\\
&= \frac{m\ell}{|m||\ell|}\ket{m}
\end{align*}

\end{s}

\begin{p}
Problem 2.58
\end{p}

\begin{s}

Since the system is in an eigenstate of $M$ with eigenvalue $m$, the average will be $m$

\begin{align*}
\langle M\rangle = \bra{m}M\ket{m} = \bra{m}m\ket{m} = m
\end{align*}

The variance must then be zero

\begin{align*}
\left(\Delta M\right)^{2} &= \langle M^{2} \rangle - \langle M\rangle^{2}\\
&= m^{2} - m^{2} = 0
\end{align*}

\end{s}

\begin{p}
Problem 2.59
\end{p}

\begin{s}

\begin{align*}
\bra{0}X\ket{0} = \bra{0}\ket{1} = 0
\end{align*}

\begin{align*}
\left(\Delta X\right)^{2} &= \langle X^{2} \rangle - \langle X\rangle^{2}\\
&= \langle X^{2} \rangle \\
&= \bra{0}X^{2}\ket{0}\\
&= 1
\end{align*}

\end{s}

\begin{p}
Problem 2.60
\end{p}

\begin{s}

\begin{align*}
\vec{v}\cdot \sigma &=
\begin{pmatrix}
v_{z} & v_{x} - iv_{y}\\
v_{x}+iv_{y} & -v_{z}
\end{pmatrix}\\
 &= v_{z}\left(\ket{0}\bra{0} - \ket{1}\bra{1}\right) + \left(v_{1}-iv_{2}\right)\ket{0}\bra{1} + \left(v_{1}+iv_{2}\right)\ket{1}\bra{0}
\end{align*}

from the outer product representations of $\sigma_x,\sigma_y,\sigma_z$. The corresponding characteristic equation is

\begin{align*}
\lambda^{2} - (v_{z}^{2} + v_{y}^{2} + v_{x}^{2}) = 0
\end{align*}

If $\vec{v}$ is normalized then $\lambda = \pm 1$. We now show that the projectors onto the respective eigenspaces are $P_{\pm} = \left(I\pm \vec{v}\cdot\sigma\right)/2$. Let $\ket{\pm}$ be the eigenvectors of $\vec{v}\cdot\sigma$ with eigenvalues $\pm 1$, respectively.

\begin{align*}
P_{+} &= \ket{+}\bra{+}\\
&=\frac{\ket{+}\bra{+} + \ket{-}\bra{-} + \ket{+}\bra{+} - \ket{-}\bra{-}}{2}\\ 
&= \frac{I+\vec{v}\cdot\sigma}{2}
\end{align*}


since by spectral decomposition we know that  $\vec{v}\cdot\sigma = \ket{+}\bra{+} - \ket{-}\bra{-}$. Of course, we also have that 

\begin{align*}
P_{-} &= \ket{-}\bra{-}\\
&=\frac{\ket{+}\bra{+} + \ket{-}\bra{-} - \ket{+}\bra{+} + \ket{-}\bra{-}}{2}\\ 
&= \frac{I-\vec{v}\cdot\sigma}{2}
\end{align*}



\end{s}
\begin{p}
Problem 2.61
\end{p}

\begin{s}

Let $\ket{0}$ and $\ket{1}$ be the eigenvectors of $\sigma_{z}$.

\begin{align*}
p(+) &= \bra{0} P_{+}\ket{0}\\
&= \bra{0} \frac{I + \vec{v}\cdot\sigma}{2}\ket{0}\\
&= \frac{1}{2}\left(1 + \bra{0}\left(v_{3}\left(\ket{0}\bra{0} - \ket{1}\bra{1}\right) + \left(v_{1}-iv_{2}\right)\ket{0}\bra{1} + \left(v_{1}+iv_{2}\right)\ket{1}\bra{0}\right)\ket{0}\right)\\
&= \frac{1}{2}\left(1 + v_{3}\right)
\end{align*}

The state of the system must be then in the eigenvector $\ket{+}$ of $\vec{v}\cdot\sigma$ with eigenvalue $+1$. This can be conveniently obtained by applying the measurement operator $P_{+}$, which was obtained in the last problem: 


\begin{align*} 
P_{+}\ket{0} &= \frac{I+\vec{v}\cdot\sigma}{2}\ket{0}\\
&= \frac{1}{2}\left(\ket{0} + (v_{3} + v_{1} + iv_{2}\ket{1}\right)
\end{align*}

\end{s}


\end{document}