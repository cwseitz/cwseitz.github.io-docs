\documentclass[12pt]{article}
\usepackage{amsmath} % AMS Math Package
\usepackage{bm}
\usepackage{amsthm} % Theorem Formatting
\usepackage{amssymb}    % Math symbols such as \mathbb
\usepackage{graphicx} % Allows for eps images
\usepackage[dvips,letterpaper,margin=1in,bottom=0.7in]{geometry}
\usepackage{tensor}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{physics}
\usepackage{amsmath, amssymb, graphics, setspace}

\newcommand{\mathsym}[1]{{}}
\newcommand{\unicode}[1]{{}}

\newcounter{mathematicapage}

\newtheorem{p}{Problem}
\usepackage{cancel}
\newtheorem*{lem}{Lemma}
\theoremstyle{definition}
\newtheorem*{dfn}{Definition}
 \newenvironment{s}{%\small%
        \begin{trivlist} \item \textbf{Solution}. }{%
            \hspace*{\fill} $\blacksquare$\end{trivlist}}%


\begin{document}

 {\noindent\Huge\bf  \\[0.5\baselineskip] {\fontfamily{cmr}\selectfont  Homework 3}         }\\[2\baselineskip] % Title
{ {\bf \fontfamily{cmr}\selectfont Quantum Mechanics}\\ {\textit{\fontfamily{cmr}\selectfont     \today}}}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    {\large \textsc{C Seitz}
\\[1.4\baselineskip] 


\begin{p}
Problem 2.48
\end{p}

\begin{s}

The polar decomposition of a matrix is $A = UJ = KU$ where $U$ is a unitary operator and $J,K$ are positive operators that satisfy $J = \sqrt{A^{\dagger}A}$ and $K = \sqrt{AA^{\dagger}}$. If $P$ is itself a positive matrix we can immediately say that its polar decomposition is $P = IP = PI$. If the matrix to decompose is unitary then of course $UU^{\dagger} = U^{\dagger}U = I$ so its decomposition is itself. If the matrix to decompose is Hermitian, then $H = H^{\dagger}$ and 

\begin{equation*}
J = K = \sqrt{H^{2}} = \sqrt{\sum_{i}\lambda_{i}^{2}\ket{i}\bra{i}} = \sum_{i}|\lambda_{i}|\ket{i}\bra{i}
\end{equation*}

and its polar decomposition is therefore $U\sum_{i}|\lambda_{i}|\ket{i}\bra{i}$ or $\sum_{i}|\lambda_{i}|\ket{i}\bra{i}U$.

\vspace{0.2in}

I will give some examples which demonstrate these properties, but are not very computationally intensive. The examples for positive matrices are the 2 x 2 matrix

\begin{align*}
A = \begin{pmatrix}3& 1\\ 1& 2\end{pmatrix}\;\;\; B = A = \begin{pmatrix}5&1&1&1\end{pmatrix}
\end{align*}


An example of a 2 x 2 Hermitian matrix is the Hamiltonian

\begin{align*}
H = -\gamma\sigma_{z}B_{0}
\end{align*}

The matrix $J$ is

\begin{align*}
J = K = \gamma B_{0} \left(\ket{+}\bra{+} + \ket{-}\bra{-}\right) = \gamma B_{0}I
\end{align*}

Clearly for both right and left polar decompositions we have $U = \sigma_{z}$. A 2 x 2 unitary example can be seen from the time-evolution corresponding to this Hamiltonian 


\begin{align*}
U &= e^{-iHt} \\
&= e^{i\omega t}\ket{+}\bra{+} + e^{-i\omega t}\ket{-}\bra{-}\\
&= \begin{pmatrix}e^{i\omega t} & 0 \\ 0 & e^{-i\omega t}\end{pmatrix}
\end{align*}

where $\omega = -\gamma B_{0}$. Obviously $J = \sqrt{U^{\dagger}U} = I$.  A 4 x 4 example is:

\begin{align*}
H = \gamma \left(\sigma_{1z}\otimes\sigma_{2z}\right)
\end{align*}

Written out explicitly, it is

\begin{align*}
H = 
\gamma \begin{pmatrix}
1 & 0 & 0 & 0\\
0 & -1 & 0 & 0\\
0 & 0 & -1 & 0\\
0 & 0 & 0 & 1
\end{pmatrix}
\end{align*}

The matrix $J$ is

\begin{align*}
J = K = \gamma (\ket{++}\bra{++} + \ket{+-}\bra{+-} \\
+ \ket{-+}\bra{-+} + \ket{--}\bra{--}) = \gamma I
\end{align*}

and in this case we have that $U = \sigma_{1z}\otimes\sigma_{2z}$, which can by easily checked to be unitary. We can again give the unitary example by considering time evolution

\begin{align*}
U &= e^{-iHt}\\
 &= \begin{pmatrix}
e^{i\gamma t} & 0 & 0 & 0\\
0 & e^{-i\gamma t} & 0 & 0\\
0 & 0 & e^{-i\gamma t} & 0\\
0 & 0 & 0 & e^{i\gamma t}
\end{pmatrix}
\end{align*}

and again we can see that $\sqrt{U^{\dagger}U} = I$.

\end{s}

\begin{p}
Problem 2.49
\end{p}

\begin{s}

The polar decomposition is $A = UJ$. The spectral decomposition of $J$ is 

\begin{equation*}
J = \sqrt{\sum_{i}\lambda_{i}\lambda_{i}^{*}\ket{i}\bra{i}} = \sum_{i}|\lambda_{i}|\ket{i}\bra{i}
\end{equation*}

For the unitary matrix $U$, we have

\begin{equation*}
U = \sum_{j}\lambda_{j}\ket{j}\bra{j}
\end{equation*}

Therefore the product $UJ$ reads

\begin{align*}
UJ &= \left(\sum_{j}\lambda_{j}\ket{j}\bra{j}\right)\left(\sum_{i}|\lambda_{i}|\ket{i}\bra{i}\right)\\
&= \sum_{ij} |\lambda_{i}|\lambda_{j}\ket{i}\bra{i}\ket{j}\bra{j}
\end{align*}



\end{s}

\begin{p}
Problem 2.50
\end{p}

\begin{s}

\begin{align*}
A = \begin{pmatrix}1&0\\1&1\end{pmatrix}
\end{align*}

First, consider

\begin{equation*}
A^{\dagger}A = \begin{pmatrix}2&1\\1&1\end{pmatrix}\;\;\; AA^{\dagger} = \begin{pmatrix}1&1\\1&2\end{pmatrix}
\end{equation*}

Let

\begin{align*}
J &= \sqrt{j_{1}}\ket{j_{1}}\bra{j_{1}} + \sqrt{j_{2}}\ket{j_{2}}\bra{j_{2}}\\
\end{align*}

where, for example, $\ket{j_{1}}$ is the first eigenvector of $A^{\dagger}A$. According to Mathematica, $\ket{j_{1}} = \frac{1+\sqrt{5}}{2}\ket{0} + \ket{1}$, and $\ket{j_{2}} = \frac{1-\sqrt{5}}{2}\ket{0} + \ket{1}$, in the standard basis. The eigenvalues are $j_{1} = \frac{3+\sqrt{5}}{2}$ and $j_{2} = \frac{3-\sqrt{5}}{2}$. We can then write normalized eigenvectors as

\begin{align*}
\ket{j_{1}} = \frac{1}{\sqrt{1 + (1+\sqrt{5})^{2}}}\left((1+\sqrt{5})\ket{0} + 2\ket{1}\right)
\end{align*}

\begin{align*}
\ket{j_{2}} = \frac{1}{\sqrt{1 + (1-\sqrt{5})^{2}}}\left((1-\sqrt{5})\ket{0} + 2\ket{1}\right)
\end{align*}

Putting it all together we find that 

\begin{align*}
J = \frac{1}{\sqrt{5}}\begin{pmatrix}3&1\\1&2\end{pmatrix}
\end{align*}

The matrix $U$ is then found by solving $U = AJ^{-1}$. Mathematica says: 

\begin{align*}
U = \frac{1}{\sqrt{5}}\begin{pmatrix}2&-1\\1&2\end{pmatrix}
\end{align*}

The matrix $K$ is found in a similar fashion: 

\begin{align*}
K = AU^{-1} = \frac{1}{\sqrt{5}}\begin{pmatrix}2&1\\1&3\end{pmatrix}
\end{align*}




\end{s}

\begin{p}
Problem 2.51
\end{p}

\begin{s}
The Hadamard gate $H$ is unitary if $H^{\dagger} = H^{-1}$. It is easy to see that

\begin{equation*}
H^{\dagger} = H = \frac{1}{\sqrt{2}}
\begin{pmatrix}
1 & 1\\ 1& -1
\end{pmatrix}
\end{equation*}

It's inverse is 

\begin{equation*}
H^{-1} =-\frac{1}{\sqrt{2}}
\begin{pmatrix}
-1 & -1\\ -1& 1
\end{pmatrix} = H
\end{equation*}

\end{s}

\begin{p}
Problem 2.52
\end{p}

\begin{s}
\begin{equation*}
H^{2} =  \frac{1}{\sqrt{2}}
\begin{pmatrix}
1 & 1\\ 1& -1
\end{pmatrix} \frac{1}{\sqrt{2}}
\begin{pmatrix}
1 & 1\\ 1& -1
\end{pmatrix} = 
\begin{pmatrix}
1 & 0\\ 0& 1
\end{pmatrix}
\end{equation*}
\end{s}

\begin{p}
Problem 2.53
\end{p}

\begin{s}
Writing out the characteristic equation gives that the eigenvalues are $\lambda = \pm \sqrt{2}$. 
\end{s}

\begin{p}
Problem 2.54
\end{p}

\begin{s}
Since the two operators commute, they are simultaneously diagonalizable. Consider the following spectral decompositions

\begin{equation*}
A = \sum_{n}a_{n}\ket{n}\bra{n}
\end{equation*}

\begin{equation*}
B = \sum_{n}b_{n}\ket{n}\bra{n}
\end{equation*}

Therefore, it must be true that

\begin{equation*}
A+B = \sum_{n}(a_{n}+b_{n})\ket{n}\bra{n}
\end{equation*}

Now these matrices are Hermitian so their eigenvectors are orthogonal, and the product of matrix exponentials is just

\begin{align*}
\exp(A)\exp(B) &= \left(\sum_{n}\exp(a_{n})\ket{n}\bra{n}\right)\left(\sum_{m}\exp(b_{m})\ket{m}\bra{m}\right)\\
&= \sum_{m,n} \delta_{mn}\exp(a_{n})\exp(b_{m})\ket{n}\bra{m}\\
&= \sum_{n}\exp(a_{n})\exp(b_{n})\ket{n}\bra{n}\\
&= \sum_{n}\exp(a_{n}+b_{n})\ket{n}\bra{n}\\
&= \exp(A+B)
\end{align*}


\end{s}

\begin{p}
Problem 2.55
\end{p}

\begin{s}

\begin{align*}
UU^{\dagger} &= \exp\left(\frac{-iH(t_2-t_1)}{\hbar}\right)\exp\left(\frac{iH(t_2-t_1)}{\hbar}\right)\\
&= \left(\sum_{n} \exp(\frac{-i E_{n}(t_2-t_1)}{\hbar})\ket{n}\bra{n}\right)\left(\sum_{m} \exp(\frac{i E_{m}(t_2-t_1)}{\hbar})\ket{m}\bra{m}\right)\\
&= \sum_{m,n}\delta_{mn}\ket{n}\bra{m}\\
&= \sum_{n}\ket{n}\bra{n} = I
\end{align*}

where $H$ is a Hermitian operator. 


\end{s}

\begin{p}
Problem 2.56
\end{p}

\begin{s}

$U$ is unitary so its eigenvalues $u_{n}$ have unit norm, which means

\begin{equation*}
K = -i\log(U) = -i\sum_{n}\log(u_{n})\ket{n}\bra{n} = \sum_{n}\theta \ket{n}\bra{n}
\end{equation*}

since

\begin{equation*}
\log(u_{n}) = \log(|u_{n}|e^{i\theta}) = \log(|u_{n}|) + i\theta = i\theta
\end{equation*}

Therefore, $K = K^{\dagger}$ since $\theta\in\mathbb{R}$.

\end{s}

\begin{p}
Problem 2.57
\end{p}

\begin{s}


\begin{align*}
L_{l}\ket{\alpha} = \frac{\ell\ket{l}}{|\ell|}
\end{align*}

\begin{align*}
M_{m}\frac{\ell\ket{l}}{|\ell|} = \frac{m\ell}{|m||\ell|}\ket{m}
\end{align*}

which is equivalent to

\begin{align*}
N_{m\ell}\ket{\alpha} &= M_{m}L_{\ell}\ket{\alpha}\\
&= \frac{\ket{m}\bra{m}\ket{\ell}\bra{\ell}}{|m||\ell|}\ket{\alpha}\\
&= \frac{\ell\ket{m}\bra{m}}{|m||\ell|}\ket{\ell}\\
&= \frac{m\ell}{|m||\ell|}\ket{m}
\end{align*}

\end{s}

\begin{p}
Problem 2.58
\end{p}

\begin{s}

Since the system is in an eigenstate of $M$ with eigenvalue $m$, the average will be $m$

\begin{align*}
\langle M\rangle = \bra{m}M\ket{m} = \bra{m}m\ket{m} = m
\end{align*}

The variance must then be zero

\begin{align*}
\left(\Delta M\right)^{2} &= \langle M^{2} \rangle - \langle M\rangle^{2}\\
&= m^{2} - m^{2} = 0
\end{align*}

\end{s}

\begin{p}
Problem 2.59
\end{p}

\begin{s}

\begin{align*}
\bra{0}X\ket{0} = \bra{0}\ket{1} = 0
\end{align*}

\begin{align*}
\left(\Delta X\right)^{2} &= \langle X^{2} \rangle - \langle X\rangle^{2}\\
&= \langle X^{2} \rangle \\
&= \bra{0}X^{2}\ket{0}\\
&= 1
\end{align*}

\end{s}

\begin{p}
Problem 2.60
\end{p}

\begin{s}

\begin{align*}
\vec{v}\cdot \sigma &=
\begin{pmatrix}
v_{z} & v_{x} - iv_{y}\\
v_{x}+iv_{y} & -v_{z}
\end{pmatrix}\\
 &= v_{z}\left(\ket{0}\bra{0} - \ket{1}\bra{1}\right) + \left(v_{1}-iv_{2}\right)\ket{0}\bra{1} + \left(v_{1}+iv_{2}\right)\ket{1}\bra{0}
\end{align*}

from the outer product representations of $\sigma_x,\sigma_y,\sigma_z$. The corresponding characteristic equation is

\begin{align*}
\lambda^{2} - (v_{z}^{2} + v_{y}^{2} + v_{x}^{2}) = 0
\end{align*}

If $\vec{v}$ is normalized then $\lambda = \pm 1$. We now show that the projectors onto the respective eigenspaces are $P_{\pm} = \left(I\pm \vec{v}\cdot\sigma\right)/2$. Let $\ket{\pm}$ be the eigenvectors of $\vec{v}\cdot\sigma$ with eigenvalues $\pm 1$, respectively.

\begin{align*}
P_{+} &= \ket{+}\bra{+}\\
&=\frac{\ket{+}\bra{+} + \ket{-}\bra{-} + \ket{+}\bra{+} - \ket{-}\bra{-}}{2}\\ 
&= \frac{I+\vec{v}\cdot\sigma}{2}
\end{align*}


since by spectral decomposition we know that  $\vec{v}\cdot\sigma = \ket{+}\bra{+} - \ket{-}\bra{-}$. Of course, we also have that 

\begin{align*}
P_{-} &= \ket{-}\bra{-}\\
&=\frac{\ket{+}\bra{+} + \ket{-}\bra{-} - \ket{+}\bra{+} + \ket{-}\bra{-}}{2}\\ 
&= \frac{I-\vec{v}\cdot\sigma}{2}
\end{align*}



\end{s}
\begin{p}
Problem 2.61
\end{p}

\begin{s}

Let $\ket{0}$ and $\ket{1}$ be the eigenvectors of $\sigma_{z}$.

\begin{align*}
p(+) &= |c_{+}|^{2}\\
&= \bra{0} P_{+}\ket{0}\\
&= \bra{0} \frac{I + \vec{v}\cdot\sigma}{2}\ket{0}\\
&= \frac{1}{2}\left(1 + \bra{0}\left(v_{3}\left(\ket{0}\bra{0} - \ket{1}\bra{1}\right) + \left(v_{1}-iv_{2}\right)\ket{0}\bra{1} + \left(v_{1}+iv_{2}\right)\ket{1}\bra{0}\right)\ket{0}\right)\\
&= \frac{1}{2}\left(1 + v_{3}\right)
\end{align*}

The state of the system must be then in the eigenvector $\ket{+}$ of $\vec{v}\cdot\sigma$ with eigenvalue $+1$. This can be conveniently obtained by applying the measurement operator $P_{+}$, which was obtained in the last problem. Consider,


\begin{align*} 
P_{+}\ket{0} &= \frac{I+\vec{v}\cdot\sigma}{2}\ket{0}\\
&= \frac{1}{2}\left((1+v_{3})\ket{0} + (v_{1} + iv_{2})\ket{1}\right)
\end{align*}

Then applying the appropriate normalization, we get

\begin{align*} 
\ket{+} =  \frac{1}{2\sqrt{(1+v_{3})/2}}\left((1+v_{3})\ket{0} + (v_{1} + iv_{2})\ket{1}\right)
\end{align*}

\end{s}


\end{document}