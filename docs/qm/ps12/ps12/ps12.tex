\documentclass[12pt]{article}
\usepackage{amsmath} % AMS Math Package
\usepackage{bm}
\usepackage{amsthm} % Theorem Formatting
\usepackage{amssymb}    % Math symbols such as \mathbb
\usepackage{graphicx} % Allows for eps images
\usepackage[dvips,letterpaper,margin=1in,bottom=0.7in]{geometry}
\usepackage{tensor}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{physics}
\usepackage{amsmath, amssymb, graphics, setspace}

\newcommand{\mathsym}[1]{{}}
\newcommand{\unicode}[1]{{}}

\newcounter{mathematicapage}

\newtheorem{p}{Problem}
\usepackage{cancel}
\newtheorem*{lem}{Lemma}
\theoremstyle{definition}
\newtheorem*{dfn}{Definition}
 \newenvironment{s}{%\small%
        \begin{trivlist} \item \textbf{Solution}. }{%
            \hspace*{\fill} $\blacksquare$\end{trivlist}}%


\begin{document}

 {\noindent\Huge\bf  \\[0.5\baselineskip] {\fontfamily{cmr}\selectfont  Homework 2}         }\\[2\baselineskip] % Title
{ {\bf \fontfamily{cmr}\selectfont Quantum Mechanics}\\ {\textit{\fontfamily{cmr}\selectfont     \today}}}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    {\large \textsc{C Seitz}
\\[1.4\baselineskip] 


\begin{p}
2.2
\end{p}

\begin{s}

The matrix representation of $A$ is 

\begin{equation*}
A = \begin{pmatrix}\bra{0}A\ket{0}&\bra{0}A\ket{1}\\
\bra{1}A\ket{0}&\bra{1}A\ket{1}\end{pmatrix}= \begin{pmatrix}0&1\\1&0\end{pmatrix}
\end{equation*}

In the output basis

\begin{equation*}
A = \begin{pmatrix}\bra{0}A\ket{0}&\bra{0}A\ket{1}\\
\bra{1}A\ket{0}&\bra{1}A\ket{1}\end{pmatrix}= \begin{pmatrix}1&0\\0&1\end{pmatrix}
\end{equation*}

We can choose a different basis, say $\ket{+} = \frac{1}{\sqrt{2}}(\ket{0} + \ket{1}), \ket{-} = \frac{1}{\sqrt{2}}(\ket{0} - \ket{1})$.

\begin{equation*}
U = \frac{1}{\sqrt{2}}
\begin{pmatrix}
1 & 1\\
1 & -1
\end{pmatrix}
\end{equation*}

In this basis $A$ takes the form:

\begin{equation*}
A' = UA = \frac{1}{\sqrt{2}}
\begin{pmatrix}
1 & -1\\
1 & 1
\end{pmatrix}
\end{equation*}

\end{s}


\begin{p}
2.9
\end{p}

\begin{s}

\begin{align*}
\sigma_{z} &= \ket{1}\bra{1} - \ket{0}\bra{0}\\
\sigma_{x} &= \ket{1}\bra{0} + \ket{0}\bra{1}\\
\sigma_{y} &=  i\ket{0}\bra{1} - i\ket{1}\bra{0}
\end{align*}

\end{s}

\begin{p}
2.12
\end{p}

\begin{s}
A matrix is diagonalizable if and only if the algebraic multiplicity equals the geometric multiplicity of each eigenvalue. It is easy to show that the characteristic equation here is $(1-\lambda)^{2} = 0$ which only has one solution.


\end{s}
\begin{p}
2.17
\end{p}
\begin{s}

If $H$ is normal, it must be diagonalizable and has the eigendecomposition

\begin{equation*}
H = U\Lambda U^{\dagger}
\end{equation*}

where $U$ is some unitary matrix. The conjugate transpose is

\begin{equation*}
H^{\dagger} = U^{\dagger}\Lambda^{\dagger} U
\end{equation*}

If $H = H^{\dagger}$, and $\Lambda$ is diagonal, then

\begin{equation*}
U^{\dagger}\Lambda^{\dagger} U = U\Lambda U^{\dagger}
\end{equation*}

which means $\Lambda = \Lambda^{\dagger}$ i.e. the eigenvalues are real. Furthermore, if $\Lambda$ is diagonal and purely real, then clearly $H = H^{\dagger}$.

\end{s}
\begin{p}
2.18
\end{p}

\begin{s}
For a unitary matrix $U^{\dagger}U = I$, so for an eigenvector $\ket{\alpha}$,
\begin{equation*}
\bra{\alpha}U^{\dagger}U\ket{\alpha} = \bra{\alpha}I\ket{\alpha} = 1
\end{equation*}
and $\bra{\alpha}U^{\dagger}U\ket{\alpha} = \lambda^{*}\lambda$, so $\lambda^{*}\lambda = 1$.
\end{s}

\begin{p}
2.24
\end{p}

\begin{s}
\end{s}

\begin{p}
Grahm-Schmidt
\end{p}

\begin{s}
It suffices to show that the following matrix has nonzero determinant:

\begin{equation*}
A = 
\begin{pmatrix}
1 & 1 & 1\\
0 & 1 & 1\\
-1 & 0 & 1
\end{pmatrix}
\end{equation*}

And it is straightforward to show that

\begin{equation*}
\mathrm{det}(A) = 1
\end{equation*}

Therefore these vectors are indeed linearly independent, but not orthogonal. We can make them orthogonal using the Graham-Schmidt procedure. Let $\ket{0},\ket{1},\ket{2}$ be our non-orthogonal basis vectors.

\begin{align*}
\ket{0'} &= \ket{0}
\end{align*}

For the second basis vector we have

\begin{align*}
\ket{1'} &\propto \ket{1} - \bra{0'}\ket{1}\ket{0}\\
&= \ket{1} - \ket{0}\\
\end{align*}

and with the appropriate normalization we get 

\begin{align*}
\ket{1'} &= \frac{1}{\sqrt{2}}\begin{pmatrix}0\\1\\-1\end{pmatrix}\\
\end{align*}


For the third basis vector we have

\begin{align*}
\ket{2'} &\propto \ket{2} - \bra{0}\ket{2}\ket{0} - \bra{1'}\ket{2}\ket{1'}\\
&= \ket{2}
\end{align*}

and with the appropriate normalization we get 

\begin{align*}
\ket{2'} &= \frac{1}{\sqrt{3}}\begin{pmatrix}1\\1\\1\end{pmatrix}\\
\end{align*}

The physical interpretation of this relative to the standard basis for $\mathbb{R}^{3}$ is that we rotated the standard basis $\phi = -\pi/4$ and $\theta = \pi/4$.

\end{s}

\begin{p}
Normal matrix parameterization
\end{p}

\begin{s}



\begin{align*}
(a\cdot\sigma)(a^{*}\cdot\sigma) &= (a^{*}\cdot\sigma)(a\cdot\sigma)\\
&\Rightarrow a\cdot a^* + i (a\cross a^{*})\cdot \sigma = a^{*}\cdot a + i (a^*\cross a)\cdot \sigma\\
&\Rightarrow (a\cross a^{*})\cdot \sigma = (a^*\cross a)\cdot \sigma\\
&\Rightarrow (a\cross a^{*}) = (a^*\cross a)
\end{align*}

which occurs when $a=a^{*}$ and the vector is strictly real. If this is satistied, the operator is normal, and has a spectral decomposition:

\begin{align*}
A &= \lambda_{1}\ket{\lambda_{1}}\bra{\lambda_{1}} + \lambda_{2}\ket{\lambda_{2}}\bra{\lambda_{2}}\\
&= \lambda_{1}P_{1} + \lambda_{2}P_{2}
\end{align*}


\end{s}

\begin{p}
2.26
\end{p}

\begin{s}
Writing out $\ket{\psi}^{\otimes 2}$ explicitly, we have
\begin{align*}
\ket{\psi}^{\otimes 2} = \frac{1}{2}\left(\ket{00}+\ket{01}+\ket{10}+\ket{11}\right)
\end{align*}
or in terms of tensor products we have
\begin{align*}
\ket{\psi}^{\otimes 2} = \frac{1}{\sqrt{2}}\begin{pmatrix}1\\1\end{pmatrix}\otimes \frac{1}{\sqrt{2}}\begin{pmatrix}1\\1\end{pmatrix} = \frac{1}{2}\begin{pmatrix}1\\1\\1\\1\end{pmatrix}
\end{align*}


Writing out $\ket{\psi}^{\otimes 3}$ explicitly, we have
\begin{align*}
\ket{\psi}^{\otimes 3} = \frac{1}{2^{3/2}}\left(\ket{000}+\ket{001}+\ket{100}+\ket{010} + \ket{101} + \ket{111} + \ket{110} + \ket{011}\right)
\end{align*}
or in terms of tensor products we have
\begin{align*}
\ket{\psi}^{\otimes 3} = \frac{1}{\sqrt{2}}\begin{pmatrix}1\\1\end{pmatrix}\otimes \frac{1}{\sqrt{2}}\begin{pmatrix}1\\1\end{pmatrix}\otimes \frac{1}{\sqrt{2}}\begin{pmatrix}1\\1\end{pmatrix} = \frac{1}{2^{3/2}}\begin{pmatrix}1\\1\\1\\1\\1\\1\\1\\1\end{pmatrix}
\end{align*}
\end{s}

\begin{p}
2.27
\end{p}

\begin{s}
\begin{align*}
X\otimes Z =
\begin{pmatrix}
0&0&1&0\\
0&0&0&-1\\
1&0&0&0\\
0 &-1&0&0
\end{pmatrix}
\end{align*}
\begin{align*}
I\otimes X =
\begin{pmatrix}
0&1&0&0\\
1&0&0&0\\
0&0&0&1\\
0&0&1&0
\end{pmatrix}
\end{align*}
\begin{align*}
X\otimes I =
\begin{pmatrix}
0&0&1&0\\
0&0&0&1\\
1&0&0&0\\
0&1&0&0
\end{pmatrix}
\end{align*}

Clearly from this last result, the tensor product does not necessarily commute.

\end{s}

\begin{p}
2.33
\end{p}

\begin{s}
Consider the outer product representation of the Hadamard gate

\begin{align*}
H &= \frac{1}{\sqrt{2}}\left(\ket{0}\bra{0} + \ket{0}\bra{1} + \ket{1}\bra{0} - \ket{1}\bra{1}\right)\\
&= \frac{1}{\sqrt{2}}\sum_{x,y}(-1)^{x\cdot y} \ket{x}\bra{y}
\end{align*}

To see how this generalizes, consider:

\begin{align*}
\ket{0}\bra{0} \otimes \ket{0}\bra{0} &= (\ket{0}\otimes\ket{0})(\bra{0}\otimes\bra{0})\\
&= \ket{00}\otimes\bra{00}
\end{align*}

This is generally true of the tensor product, so multiplying sums like the one above $n$ times will give a similar expression for vectors $\bm{x}$ and $\bm{y}$: 

\begin{align*}
H^{\otimes n} &= \frac{1}{\sqrt{2}}\sum_{\bm{x},\bm{y}}(-1)^{\bm{x}\cdot \bm{y}} \ket{\bm{x}}\bra{\bm{y}}
\end{align*}

For $n=2$, we just need to evaluate this sum for all possible binary strings of length 2 and add them up according to this scheme, which gives

\begin{align*}
H^{\otimes 2} &= \frac{1}{2}
\begin{pmatrix}
1&1&1&1\\
1&-1&1&-1\\
1&1&-1&-1\\
1&-1&-1&1
\end{pmatrix}
\end{align*}




\end{s}

\begin{p}
2.34
\end{p}

\begin{s}
We can compute these functions of matrices if we have its spectral decomposition. For an arbitrary matrix $A$, with spectral decomposition, the square root of $A$ is simply

\begin{equation*}
\sqrt{A} = \sum_{n}\sqrt{\lambda_{n}}\ket{\lambda_{n}}\bra{\lambda_{n}}
\end{equation*}

and its logarithm is

\begin{equation*}
\log{A} = \sum_{n}\log{\lambda_{n}}\ket{\lambda_{n}}\bra{\lambda_{n}}
\end{equation*}

Therefore we start by finding the eigendecomposition of the matrix. The characteristic equation is 

\begin{equation*}
\lambda^{2} - 8\lambda  + 7 = 0
\end{equation*}

so the two eigenvalues are $\lambda_{1} = 1$, $\lambda_{2} = 7$. The eigenvectors are:

\begin{equation*}
\ket{\lambda_{1}} = \frac{1}{\sqrt{2}}\begin{pmatrix}1\\-1\end{pmatrix}, \ket{\lambda_{2}} = \frac{1}{\sqrt{2}}\begin{pmatrix}1\\1\end{pmatrix} 
\end{equation*}

Thus for its square root we get

\begin{align*}
\sqrt{A} &= \sum_{n}\sqrt{\lambda_{n}}\ket{\lambda_{n}}\bra{\lambda_{n}}\\
&= \ket{\lambda_{1}}\bra{\lambda_{1}} + \sqrt{7}\ket{\lambda_{2}}\bra{\lambda_{2}}\\
&= \frac{1}{2}\begin{pmatrix}1+\sqrt{7}&-1+\sqrt{7}\\-1+\sqrt{7}&1+\sqrt{7}\end{pmatrix}
\end{align*}

and the logarithm is 

\begin{align*}
\log{A} &= \log{7}\ket{\lambda_{2}}\bra{\lambda_{2}}\\
&= \frac{\log 7}{2}\begin{pmatrix}1&1\\1&1\end{pmatrix}
\end{align*}

\end{s}

\begin{p}
2.35
\end{p}

\begin{s}

First recall that $\left(\vec{a}\cdot \vec{\sigma}\right)^{k}$ is identity for even $k$ and is just $\left(\vec{a}\cdot \vec{\sigma}\right)$ for odd $k$.

\begin{align*}
\exp(i\theta\vec{a}\cdot \vec{\sigma}) &= \sum_{k=0}^{\infty}\frac{1}{k!}\left(i\theta\vec{a}\cdot \vec{\sigma}\right)^{k}\\
&=  \sum_{k=0}^{\infty}\frac{(i\theta)^{k}}{k!}\left(\vec{a}\cdot \vec{\sigma}\right)^{k}\\
&= \sum_{k \;odd}^{\infty}\frac{(-1)^{k}\theta^{k}}{k!}\left(\vec{a}\cdot \vec{\sigma}\right)^{k} + i\sum_{k \;even}^{\infty}\frac{(-1)^{k}(\theta)^{k}}{k!}\left(\vec{a}\cdot \vec{\sigma}\right)^{k}\\
&= \cos\theta I + i\sin\theta(\vec{a}\cdot\vec{\sigma})
\end{align*}
\end{s}

\begin{p}
2.39
\end{p}

\begin{s}

\end{s}

\end{document}