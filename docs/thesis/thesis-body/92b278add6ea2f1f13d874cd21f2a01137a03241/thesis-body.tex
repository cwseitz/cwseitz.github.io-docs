
\documentclass{article} % For LaTeX2e
\usepackage{iclr2020_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{siunitx}
\usepackage{url}
\usepackage{graphicx}


\title{Plastic networks of binary spiking neurons}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}


\maketitle

\begin{abstract}

Fundamental questions in theoretical neuroscience pertain to how circuits in cortex store and process information. Recent decades have yielded a number of experimental and theoretical advances in our understanding of the mechanisms information storage, such as homosynaptic, heterosynaptic, and homeostatic plasticity. However, the precise mathematical relationship between synaptic weights and the stimuli or `patterns' to which the network is exposed, remains elusive. At the same time, statistical physics and the theory of dynamical systems have provided substantial insight on network dynamics during processing. There remain questions on the optimum dynamical state or \emph{phase} of the network during information processing and how that state is simultaneous with a particular class of synaptic connectivity. Here, we review the stream of research on the above topics.


\end{abstract}


\section{Introduction}

To present a comprehensive picture of recent research on information processing in spiking cortical circuits, we classify studies on their primary focus: information storage and network topology, network dynamics, or both. It is generally accepted that neural network models date back to McCulloch and Pitts [1] followed later by the Hopfield network [2]. We now proceed to an implementation of the Hopfield network. All code for the following analysis can be found at [9].  

\section{The Hopfield Network}

We begin by reviewing the Hopfield network as originally formulated by J.J. Hopfield in [2]. The Hopfield network appears primitive these days; however, it provides a useful starting point as it set the stage for its descendant models. We will conclude this section by pointing out some of the biological inaccuracies and reflect on the computational utility of those additional ingredients.

We describe the state of the neural network as a set of spins (neurons) $\{\sigma_{i}\}$ with $1 \leq i \leq n$. Each $\sigma_{i} \in \{0,1\}$  which we will take to indicate whether or not a spike occured. The neurons $\sigma_{i}$ and $\sigma_{j}$ are coupled according to a connectivity matrix $J_{ij}$. If we let the network state $\{\sigma_{i}\}$ evolve in discrete time, it may relax to a particular state $\xi_{i}$ with $1 \leq i \leq m$, in which case we say the pattern $\xi_{i}$ is stored in the connectivity matrix $J$. We can be very general and write $\xi$ as a matrix of $m$ binary patterns i.e. $\xi \in \mathbb{F}_{2}^{n\times m}$ The mechanism to store all patterns $\xi_{i}$ in the matrix simultaneously via an outer-product learning rule

\begin{equation*}
J = \xi^{T}\xi - mI
\end{equation*}

where the second term serves to remove all $n$ self connections $J_{ii} = 0$ for $1 \leq i \leq n$. 


The Hopfield network is an attractive model because the theoretical treatment it requires is identical to that used for magnetic systems. However, when using such a system to directly describe a spiking neural network in cortex, there are notable gaps. Here are a few:

\begin{itemize}
  \item Biological learning rules are much more complex
  \item Model neurons are not stateful objects
  \item Synaptic connectivity is sparse in cortex (low connection probability)
\end{itemize}


\subsection{Storage Capacity}




\section{Phase transitions and critical phenomena}

Statistical physics proves to be an invaluable tool when examining the relationship between synaptic connectivity and network dynamics. If the voltage of individual integrate and fire neurons is described by a diffusion process, we can make estimates of the time evolution of the instantaenous firing rate $\nu$ using Langevin dynamics. It turns out that the phase space of the network can be divided into four distinct phases [5]

\begin{itemize}
  \item Asynchronous regular (AR), stationary global activity
  \item Asynchronous irregular (AI), stationary global activity but irregular firing at low rates
  \item Synchronous regular (SR), where neurons are almost fully synchronized in a few clusters
  \item Synchronous irregular (SI), oscillatory global activity but highly irregular firing at low rates
\end{itemize}

We can introduce a similiar parameterization to [5] to go beyond the Hopfield model and start to build more realistic networks. 


It is thought that neural networks that lie at a critical point between order and chaos maximize basic processing properties such as sensitivity, dynamic range, correlation length, information transfer, and susceptibility [1]. Multiple signatures of criticality have been proposed in the literature, one being power-law distributions of neuronal avalanches following a branching process similar to that seen in physical theories of avalanches [2]. However, it has recently been demonstrated that criticality is not a universal determinant of computational power; rather, critical networks are only well-suited for more complex tasks such as those that require integration of stimulus information over long timescales [1]. 

Although various sources have analyzed the relationship between network topology and the dynamical state of the network, the relationship remains unclear. Here, we seek to accomplish two objectives (i) develop a mathematical framework to analyze the dynamics of coupled excitatory and inhibitory populations of leaky integrate and fire neurons (ii) use the framework in (i) to tune the spiking dynamics of the ensemble from asynchronous and chaotic to bursty and critical. Criticality of networks will be assessed according to established measures e.g., the branching parameter, autocorrelation time, and avalanche size distributions. 

\section{Diverse synaptic plasticity mechanisms}

Previous models have been plagued with an inability to store memories efficiently. It is possible that a variety of plasticity mechanisms work in concert to form and consolidate memories over a different time scales [7].



\section*{References}

[1] Mculloch and Pitts. \textit{A logical calculus of the ideas imminent in nervous activity}. Journal of Mathematical Biophysics. 1943.

[2] J.J. Hopfield. \textit{Neural networks and physical systems with emergent collective computational abilities}. 1982.

[3] D.J. Amit. \textit{Spin-glass models of neural networks}. Physical Rev A. 1985.

[4] E. Gardner. \textit{The space of interactions in neural network models}. Journal of Physics A: Mathematical and General. 1988.

[5] N. Brunel. \textit{Dynamics of sparsely connected networks of excitatory and inhibitory neurons}. Journal of Computational Neuroscience. 2000. 

[6] Rosenbaum and Doiron. \textit{Balanced Networks of Spiking Neurons with Spatially Dependent Recurrent Connections}. Physical Review X. 2014.

[7] Zenke et al. \textit{Diverse synaptic plasticity mechanisms
orchestrated to form and retrieve memories
in spiking neural networks}. Nature Communications. 2015.

[8] Williams et al. \textit{Nonnegative decomposition of multivariate information}. arXiv. 2010.

[9] \url{https://github.com/cwseitz/hebb.git}

\end{document}
