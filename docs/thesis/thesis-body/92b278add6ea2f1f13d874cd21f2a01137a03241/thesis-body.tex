
\documentclass{article} % For LaTeX2e
\usepackage{iclr2020_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{siunitx}
\usepackage{url}
\usepackage{graphicx}


\title{Plastic networks of binary spiking neurons}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}


\maketitle

\begin{abstract}

Fundamental questions in theoretical neuroscience pertain to how circuits in cortex store and process information. Recent decades have yielded a number of experimental and theoretical advances in our understanding of the mechanisms information storage, such as homosynaptic, heterosynaptic, and homeostatic plasticity. However, the precise mathematical relationship between synaptic weights and the stimuli or `patterns' to which the network is exposed, remains elusive. At the same time, statistical physics and the theory of dynamical systems have provided substantial insight on network dynamics during processing. There remain questions on the optimum dynamical state or \emph{phase} of the network during information processing and how that state is simultaneous with a particular class of synaptic connectivity. Here, we review recent work on the above questions.


\end{abstract}


\section{Introduction}

To present a comprehensive picture of recent research on information processing in spiking cortical circuits, we classify studies on their primary focus: information storage and network topology, network dynamics, or both. It is generally accepted that neural network models date back to McCulloch and Pitts [1] followed later by the Hopfield network [2]. We now proceed to an implementation of the Hopfield network. All code for the following analysis can be found at [9].  

\section{The Hopfield Network}

We begin by reviewing the Hopfield network as originally formulated by J.J. Hopfield in [2]. The Hopfield network appears primitive these days; however, it provides a useful starting point and sets the stage for its descendant models. We will point out some of the biological inaccuracies at the end and reflect on the computational utility of those additional ingredients.

\section{Critical phenomena}

It is thought that neural networks that lie at a critical point between order and chaos maximize basic processing properties such as sensitivity, dynamic range, correlation length, information transfer, and susceptibility [1]. Multiple signatures of criticality have been proposed in the literature, one being power-law distributions of neuronal avalanches following a branching process similar to that seen in physical theories of avalanches [2]. However, it has recently been demonstrated that criticality is not a universal determinant of computational power; rather, critical networks are only well-suited for more complex tasks such as those that require integration of stimulus information over long timescales [1]. 

Although various sources have analyzed the relationship between network topology and the dynamical state of the network, the relationship remains unclear. Here, we seek to accomplish two objectives (i) develop a mathematical framework to analyze the dynamics of coupled excitatory and inhibitory populations of leaky integrate and fire neurons (ii) use the framework in (i) to tune the spiking dynamics of the ensemble from asynchronous and chaotic to bursty and critical. Criticality of networks will be assessed according to established measures e.g., the branching parameter, autocorrelation time, and avalanche size distributions. 

\section{Methods}


\subsection{Excitatory-Inhibitory network model}

We make the initial assumption that subthreshold activity does not contribute to the firing of a neuron. Also, we consider a current-based model where each unit performs a weighted integration of spikes of its presynaptic partners. The voltage of a generic excitatory or inhibitory LIF neuron evolves according to

\begin{align*}
\frac{dV}{dt} &= -\frac{V-\theta}{\tau_{l}} + \sum_{i \neq j}W_{ij}z_{j} + \sum_{k \neq j} X_{kj}z_{k}\\
&= -\frac{V_{0}-\theta}{\tau_{l}} + W_{ab}n_{a}(t) + X_{a}n_{x}(t)\\
\end{align*}

where matrices $W$ and $X$ refer to recurrent and input connectivity matrices, respectively. The second equality comes from the assumption that the all synaptic connections between populations $a$ and $b$ have the same weight and we can pull the weight matrices out of the sum. The variable $z_{j} = \sum_{i} \delta_{j} (t-t_{i})$ is the spike train for reservoir neuron $j$ while $z_{k} = \sum_{i} \delta_{k} (t-t_{i})$ is the spike train for input neuron $k$. We often refer to $z_{j}^{t}$ as the \emph{observable state} of neuron $j$ at time $t$. Its value is computed at a time $t$ by passing the voltage $v_{j}^{t}$ through a form of activation function called the spike function $H(v_{j}^{t} - v_{th}) \rightarrow \{0,1\}$ where $H$ is the Heaviside step function.

\subsection{Parameteric control of spiking dynamics}

In complex networks, it becomes difficult to predict the trajectory of the voltage of any particular neuron $v_{j}^{t}$ and therefore its spiking activity. As a workaround, many have turned to mean field theories which have been successful in estimating macroscopic dynamics of the population firing rate. While this approch has proven valuable, it ignores the spiking mechanism of individual neurons and therefore their contribution to that activity. It is then difficult to discern whether such a theory is simply a mathematical object or can indeed be implemented by a spiking network. Recently, another mean field theory has been successful in making predictions on the dynamics of the mean population voltage rather than the firing rate, uncovering the role of microscopic variables in the population spiking dynamics. 

Let the variable $m_{\ell}(t) = \langle z_{\ell}(t) \rangle$ denote the population average of population $\ell$ as a function of time. Then the mean spiking activity can be written as

\begin{equation*}
m_{\ell}(t) = \langle H(V_{j}^{\ell}(t) - \theta)\rangle_{j}
\end{equation*}

which relates the fraction of active neurons in population $\ell$ to the average voltage in the population. Assuming that the voltage $V_{j}^{\ell}(t)$ is Gaussian distributed and has a time dependent mean $\mu_{\ell}$ and time independent variance $\sigma_{\ell}^{2}$, the above expression can be written more explicitly

\begin{align*}
m_{\ell}(t) &= \int_{\theta}^{\infty} P(V_{\ell})dV\\
&= \int_{\theta}^{\infty} \frac{1}{\sqrt{\pi}}\exp\left(-\frac{1}{2}\frac{\left(V_{\ell}-\mu_{\ell}\right)^{2}}{\sigma_{\ell}^{2}}\right)dV_{\ell}\\
&= \frac{1}{2}\cdot \mathrm{erfc}\left(\frac{V_{\ell}-\mu_{\ell}}{\sqrt{2}\sigma_{\ell}}\right)
\end{align*}


where $\mathrm{erfc}(z) = \frac{2}{\sqrt{\pi}}\int_{z}^{\infty} \exp\left(-t^{2}\right)dt$. The temporal dynamics of $m_{\ell}(t)$ are then

\begin{align}
\tau_{\ell}'\dot{m_{\ell}} &= -m_{\ell} + \frac{N_{\ell}}{2}\cdot \mathrm{erfc}\left(\frac{\theta-\mu_{\ell}}{\sqrt{2}\sigma_{\ell}}\right)
\end{align}

At this point we can rewrite eq. (1) in its mean-field form

\begin{equation}
\frac{d\mu_{\ell}}{dt} = -\frac{\mu_{\ell}-\theta}{\tau_{l}} + \sum_{b} W_{ab}n_{b}m_{b} + X_{0}\cdot\left(n_{0}m_{0} + \sqrt{\frac{n_{0}Q_{0}}{N_{\ell}}\xi_{\ell}(t)}\right)
\end{equation}

where we have replaced the Poisson input spike train with a term for Gaussian white noise as in [1]. At this point, the dynamics of the network are completely specified by equation (1) and (2) and we are left to determine their stability. It will be shown that, by tuning particular connectivity parameters, the system can transition from chaotic asychronous spiking to bursty critical dynamics. We have four state variables $\left(V_{\mathrm{E}}, V_{\mathrm{I}}, m_{\mathrm{E}}, m_{\mathrm{I}}\right)$. The Jacobian matrix for such a system is given by 

\begin{equation*}
J = \begin{bmatrix}
-\frac{1}{\tau_{E}} & 0 & W_{\mathrm{EE}}n_{\mathrm{I}} & W_{\mathrm{IE}}n_{\mathrm{E}} \\
0 &  -\frac{1}{\tau_{I}} & W_{\mathrm{EI}}n_{\mathrm{E}} & W_{\mathrm{II}}n_{\mathrm{I}} \\
-\frac{1}{\tau_{E}} & 0 & W_{\mathrm{IE}}n_{\mathrm{I}} & W_{\mathrm{EI}}n_{\mathrm{E}} \\
-\frac{1}{\tau_{E}} & 0 & W_{\mathrm{IE}}n_{\mathrm{I}} & W_{\mathrm{EI}}n_{\mathrm{E}} \\
\end{bmatrix}
\end{equation*}

\section*{References}

[1] Mculloch and Pitts. \textit{A logical calculus of the ideas imminent in nervous activity}. Journal of Mathematical Biophysics. 1943.

[2] J.J. Hopfield. \textit{Neural networks and physical systems with emergent collective computational abilities}. 1982.

[3] D.J. Amit. \textit{Spin-glass models of neural networks}. Physical Rev A. 1985.

[4] E. Gardner. \textit{The space of interactions in neural network models}. Journal of Physics A: Mathematical and General. 1988.

[5] N. Brunel. \textit{Dynamics of sparsley connected networks of excitatory and inhibitory neurons}. Journal of Computational Neuroscience. 2000. 

[6] Rosenbaum and Doiron. \textit{Balanced Networks of Spiking Neurons with Spatially Dependent Recurrent Connections}. Physical Review X. 2014.

[8] Williams et al. \textit{Nonnegative decomposition of multivariate information}. arXiv. 2010.

[9] \url{https://github.com/cwseitz/hebb.git}

\end{document}
