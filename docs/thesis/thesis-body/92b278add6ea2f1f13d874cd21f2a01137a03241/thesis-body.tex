
\documentclass{article} % For LaTeX2e
\usepackage{iclr2020_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{siunitx}
\usepackage{url}
\usepackage{graphicx}


\title{Plastic networks of binary spiking neurons}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}


\maketitle

\section{Introduction}

Fundamental questions in theoretical neuroscience pertain to how circuits in cortex store and process information. Recent decades have yielded a number of experimental and theoretical advances in our understanding of the mechanisms information storage, such as homosynaptic, heterosynaptic, and homeostatic plasticity. However, the precise mathematical relationship between synaptic weights and the stimuli or `patterns' to which the network is exposed, remains elusive. At the same time, statistical physics and the theory of dynamical systems have provided substantial insight on network dynamics during processing. There remain questions on the optimum dynamical state or \emph{phase} of the network during information processing and how that state is simultaneous with a particular class of synaptic connectivity. 


It is generally accepted that neural network models date back to McCulloch and Pitts [1] followed later by the Hopfield network [2]. J.J. Hopfield pioneered a concrete application of statistical physics to the description of a simplified neural network and the model's elegance triggered an avalanche of research. Of particular interest has been the storage capacity of the network, which has been well-characterized by Gardner [4], Amit [10], and Nishimori [11]. Such models draw direct comparisons between simplified neural networks and magnetic systems, thereby reducing their statistical properties to those previously known to statistical physics. At the same time, such comparisons potentially allow one to write down the Hamiltonian for the system analytically, and therefore the probability over the phase space of the network. However, it is not a simple task to probe the geometry of the free energy landscape and therefore the probability over states and the problem is further aggravated by the replacement of spins with even minimally biologically realistic units such as leaky integrate and fire (LIF) neurons. The state space of a network is made much more diverse, which could have implications for computation, with the addition of different cell types such as excitatory and inhibitory units. 

Great progress towards an understanding of the dynamics of coupled excitatory and inhibitory populations was made by the Brunel network [5] in spite of the rather harsh assumption that weights between different populations were homogeneous. Nevertheless, these networks have been used commonly to initialize so-called balanced networks which maintain asynchronous irregular firing. Balanced networks have, for example, been used to initialize networks trained via a family of orchestrated slow and fast plasticity mechanisms [7]. After training, these networks have been shown to exhibit attractor dynamics towards states imprinted on the network weights by previous stimuli, all in an arguably more brain-like fashion than the original Hopfield model. Many questions remain, such as the range of spiking dynamics observed for weight matrices that range from fully connected to fully random as in a Erdos-Renyi directed binomial graph and the storage capacity of such networks and their compresson and error correction capabilities.

Here, we assess the ability of a network initialized in a balance state and trained using similiar plasticity mechanisms in [7] to exhibit attractor dynamics, and probe the mechanism by which `memories' are stored in the synaptic weights, using ideas from compressed sensing and information theory. We also comment on the role of individual plasticity rules during learning as they pertain to the various network phases observed, network error correction, and potential applications in neuromorphic content addressable memory devices. An understanding of the mapping between weight matrices that store maximal information and precise network states is a critical step in creating programmable neuromorphic systems.

\section{The Hopfield Network}

We begin by reviewing the Hopfield network as originally formulated by J.J. Hopfield in [2]. The Hopfield network appears primitive these days; however, it provides a useful starting point as it set the stage for its descendant models. We will conclude this section by pointing out some of the biological inaccuracies and reflect on the computational utility of those additional ingredients.

We describe the state of the neural network as a set of spins (neurons) $\{\sigma_{i}\}$ with $1 \leq i \leq n$. Each $\sigma_{i} \in \{0,1\}$  which we will take to indicate whether or not a spike occured. The neurons $\sigma_{i}$ and $\sigma_{j}$ are coupled according to a connectivity matrix $J_{ij}$. If we let the network state $\{\sigma_{i}\}$ evolve in discrete time, it may relax to a particular state $\xi_{i}$ with $1 \leq i \leq m$, in which case we say the pattern $\xi_{i}$ is stored in the connectivity matrix $J$. We can be very general and write $\xi$ as a matrix of $m$ binary patterns i.e. $\xi \in \mathbb{F}_{2}^{n\times m}$ The mechanism to store all patterns $\xi_{i}$ in the matrix simultaneously via an outer-product learning rule

\begin{equation*}
J = \xi^{T}\xi - mI
\end{equation*}

where the second term serves to remove all $n$ self connections $J_{ii} = 0$ for $1 \leq i \leq n$. 


The Hopfield network is an attractive model because the theoretical treatment it requires is identical to that used for magnetic systems. However, when using such a system to directly describe a spiking neural network in cortex, there are notable gaps. Here are a few:

\begin{itemize}
  \item Biological learning rules are much more complex
  \item Model neurons are not stateful objects
  \item Synaptic connectivity is sparse in cortex (low connection probability)
\end{itemize}


\subsection{Storage Capacity}




\section{Phase transitions and critical phenomena}

Statistical physics proves to be an invaluable tool when examining the relationship between synaptic connectivity and network dynamics. If the voltage of individual integrate and fire neurons is described by a diffusion process, we can make estimates of the time evolution of the instantaenous firing rate $\nu$ using Langevin dynamics. It turns out that the phase space of the network can be divided into four distinct phases [5]

\begin{itemize}
  \item Asynchronous regular (AR), stationary global activity
  \item Asynchronous irregular (AI), stationary global activity but irregular firing at low rates
  \item Synchronous regular (SR), where neurons are almost fully synchronized in a few clusters
  \item Synchronous irregular (SI), oscillatory global activity but highly irregular firing at low rates
\end{itemize}

We can introduce a similiar parameterization to [5] to go beyond the Hopfield model and start to build more realistic networks. 


It is thought that neural networks that lie at a critical point between order and chaos maximize basic processing properties such as sensitivity, dynamic range, correlation length, information transfer, and susceptibility [1]. Multiple signatures of criticality have been proposed in the literature, one being power-law distributions of neuronal avalanches following a branching process similar to that seen in physical theories of avalanches [2]. However, it has recently been demonstrated that criticality is not a universal determinant of computational power; rather, critical networks are only well-suited for more complex tasks such as those that require integration of stimulus information over long timescales [1]. 

Although various sources have analyzed the relationship between network topology and the dynamical state of the network, the relationship remains unclear. Here, we seek to accomplish two objectives (i) develop a mathematical framework to analyze the dynamics of coupled excitatory and inhibitory populations of leaky integrate and fire neurons (ii) use the framework in (i) to tune the spiking dynamics of the ensemble from asynchronous and chaotic to bursty and critical. Criticality of networks will be assessed according to established measures e.g., the branching parameter, autocorrelation time, and avalanche size distributions. 

\section{Diverse synaptic plasticity mechanisms}

Previous models have been plagued with an inability to store memories efficiently. It is possible that a variety of plasticity mechanisms work in concert to form and consolidate memories over a different time scales [7].



\section*{References}

[1] Mculloch and Pitts. \textit{A logical calculus of the ideas imminent in nervous activity}. Journal of Mathematical Biophysics. 1943.

[2] J.J. Hopfield. \textit{Neural networks and physical systems with emergent collective computational abilities}. 1982.

[3] D.J. Amit. \textit{Spin-glass models of neural networks}. Physical Rev A. 1985.

[4] E. Gardner. \textit{The space of interactions in neural network models}. Journal of Physics A: Mathematical and General. 1988.

[5] N. Brunel. \textit{Dynamics of sparsely connected networks of excitatory and inhibitory neurons}. Journal of Computational Neuroscience. 2000. 

[6] Rosenbaum and Doiron. \textit{Balanced Networks of Spiking Neurons with Spatially Dependent Recurrent Connections}. Physical Review X. 2014.

[7] Zenke et al. \textit{Diverse synaptic plasticity mechanisms
orchestrated to form and retrieve memories
in spiking neural networks}. Nature Communications. 2015.

[8] Williams et al. \textit{Nonnegative decomposition of multivariate information}. arXiv. 2010.

[9] \url{https://github.com/cwseitz/hebb.git}

[10] Amit et al. \textit{Storing infinite numbers of patterns in a spin-glass model of neural networks}. Physical Rev. Letters. 1985

[11] Nishimori. \textit{Statistical physics of spin glasses and information processing: an introduction}. Clarendon Press. 2010

\end{document}
