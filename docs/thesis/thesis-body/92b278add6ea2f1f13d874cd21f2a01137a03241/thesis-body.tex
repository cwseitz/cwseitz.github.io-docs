
\documentclass{article} % For LaTeX2e
\usepackage{iclr2020_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{siunitx}
\usepackage{url}
\usepackage{graphicx}


\title{Plastic networks of binary spiking neurons}

\author{Clayton W. Seitz \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Electrical and Computer Engineering\\
Cranberry-Lemon University\\
Indianapolis, IN 46203, USA \\
\texttt{cwseitz@uchicago.edu} \\
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy
\begin{document}


\maketitle

\begin{abstract}
The human brain is capable of storing a multitude of memories over the lifetime of an individual and performing rather complex computations using less power than a lightbulb. It is thought that the origins of this astounding efficiency are due to the massive parallelism and organization of neural circuits into forms substantially different from the von-Neumann architecture. It is widely accepted in neuroscience that synaptic plasticity is the mechanism by which memory is stored and that variable network connectivity results in variable spiking dynamics i.e. computations. It is natural to then infer, then, that memory retrieval and computation overlap if their conventional definition is to be applied in this context. Of course, there are many classes of computations that the brain performs; however, memory retrieval appears to be of fundamental importance to understanding how the brain computes. Here, we train a balanced Brunel network on an orchestrated set of plasticity mechanisms and compare the storage capacity of the network to the predictions made by the rate-interpretation of the Hopfield network.
\end{abstract}

\section{Introduction}

Fundamental questions in theoretical neuroscience pertain to how circuits in cortex store and process information. Recent decades have yielded a number of experimental and theoretical advances in our understanding of the mechanisms information storage, such as homosynaptic, heterosynaptic, and homeostatic plasticity. However, the precise mathematical relationship between the synaptic weights and the stimuli or `patterns' to which the network has been exposed, remains elusive. 

It is generally accepted that neural network models date back to McCulloch and Pitts [1] followed later by the Hopfield network [2] when J.J. Hopfield pioneered a concrete application of statistical physics to the description of a simplified neural network. Of particular interest has been the storage capacity of that network, which has been well-characterized by Gardner [4], Amit [10], and Nishimori [11]. Such models draw direct comparisons between simplified neural networks and magnetic systems, thereby reducing their statistical properties to those previously known to statistical physics. More specifically, such comparisons potentially allow one to write down the Hamiltonian for the system analytically, and in turn the probability density over the phase space of the network. However, it is not a simple task to probe the geometry of the free energy landscape and the problem is further aggravated by the replacement of spins with even minimally biologically realistic units. 

The state space of a network model is made even more diverse with the addition of biological realism e.g. variable cell types as in the Brunel network [5]. Even in this case, statistical physics has inspired great progress towards an understanding of the dynamics of coupled excitatory and inhibitory populations, in spite of the common assumption that weights between different populations are homogeneous. The Brunel network has been used to initialize so-called balanced networks which maintain asynchronous irregular firing. Such balanced networks have been used to initialize networks to be later trained via a family of orchestrated slow and fast plasticity mechanisms [7]. After training, these networks have been shown to exhibit attractor dynamics towards states imprinted on the network weights by previous stimuli, all in an arguably more brain-like fashion than the original Hopfield model. Many questions remain, such as the range of spiking dynamics observed for weight matrices that range from fully connected to fully random as in a Erdos-Renyi directed binomial graph and the storage capacity of such networks and their compresson and error correction capabilities.

Here, we assess the ability of a network initialized in a balance state and trained using similiar plasticity mechanisms in [7] to exhibit attractor dynamics, and probe the mechanism by which `memories' are stored in the synaptic weights, using ideas from compressed sensing and information theory. We also comment on the role of individual plasticity rules during learning as they pertain to the various network phases observed, network error correction, and potential applications in neuromorphic content addressable memory devices. An understanding of the mapping between weight matrices that store maximal information and precise network states is a critical step in creating programmable neuromorphic systems.

\section{Formalism}

Perhaps we are aware of a rectangular matrix $S(t) \in \mathbb{F}_{2}^{n\times t}$ which represents the stimulation of a network of $x$ input neurons over a time interval $t$. Ideally, in a noise-free scenario, we would like to be able to predict the evolution of the synaptic weights $J_{ij}(t)$ analytically throughout the duration of the stimulus. At the same time, we would like to be able to predict the spiking dynamics of the network $\mathbf{z} \in \mathbb{F}_{2}^{n}$ for a system of $n$ neurons amidst changing synaptic connectivity. The vector $\mathbf{z}$ traces out a trajectory over the field $\mathbb{F}_{2}^{n}$ in a time $t$.

In principle, the time evolution of the synaptic weights and the spiking dynamics are coupled. That is, $\dot{J} = F(\mathbf{z}(t))$ where the function $F$ represents an arbitrary plasticity function, and $\mathbf{z}(t) = \Theta(\mathbf{v}(t)-v_{0})$ were $\mathbf{v}(t)$ represents a vector of voltages and $\Theta$ is the Heaviside step function. It is hard problem to predict $\mathbf{v}(t)$, although it can be examined statistically, if we take the voltage of a neuron to be a kind of diffusion process as in [5]. 

Furthermore, while the Hopfield model lacks many features thought to be characteristic of real neural networks, it has an alternative interpretation where the spins are taken to be indicators of whether that neuron is firing at high or low rates. This is only possible if we take a `memory' to be a set of neurons firing at rates but it is a useful interpretation considering our deep understanding of the storage capacity of these networks from [3,4]. We argue that this defines a lower bound on the storage capacity of these networks. we extend [7] by assessing the storage capacity of the network and with matrix decomposition arguments to move towards a well-defined relationship between stimuli and learned synaptic weights. Free energy minima in the Ising model define the landscape of these networks.



\section*{References}

[1] Mculloch and Pitts. \textit{A logical calculus of the ideas imminent in nervous activity}. Journal of Mathematical Biophysics. 1943.

[2] J.J. Hopfield. \textit{Neural networks and physical systems with emergent collective computational abilities}. 1982.

[3] D.J. Amit. \textit{Spin-glass models of neural networks}. Physical Rev A. 1985.

[4] E. Gardner. \textit{The space of interactions in neural network models}. Journal of Physics A: Mathematical and General. 1988.

[5] N. Brunel. \textit{Dynamics of sparsely connected networks of excitatory and inhibitory neurons}. Journal of Computational Neuroscience. 2000. 

[6] Rosenbaum and Doiron. \textit{Balanced Networks of Spiking Neurons with Spatially Dependent Recurrent Connections}. Physical Review X. 2014.

[7] Zenke et al. \textit{Diverse synaptic plasticity mechanisms
orchestrated to form and retrieve memories
in spiking neural networks}. Nature Communications. 2015.

[8] Williams et al. \textit{Nonnegative decomposition of multivariate information}. arXiv. 2010.

[9] \url{https://github.com/cwseitz/hebb.git}

[10] Amit et al. \textit{Storing infinite numbers of patterns in a spin-glass model of neural networks}. Physical Rev. Letters. 1985

[11] Nishimori. \textit{Statistical physics of spin glasses and information processing: an introduction}. Clarendon Press. 2010

\end{document}
