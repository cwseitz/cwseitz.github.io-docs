---
layout: post
title: "Learning-theory"
date: 2020-11-17
categories: [deep-learning]
---


### Free lunch

A prominent question in deep learning is whether or not a universal learning algorithm exists. Many have argued for the no free lunch theorem which argues that there must be some innate system in biological brains that facilitates interpretation of natural language. In other words there must be some constraint on the space of functions of the input.


On the other hand, the free lunch theorem says that there need not be such a bias or constraint on the space of possible functions. Rather, the function is completely random

### Frequentist vs. Bayesian

It will be important to distinguish the frequentist and bayesian points of view. Frequentist guarantees don't make any assumptions about the prior distribution over the models while bayesian methods do assume a prior distribution.

### The Occam guarantee

Consider a classifier $f$ written in a programming language of your choice and let $|f|$ be the number of bits needed to represent $f$. We have the following usual expressions for the test and training loss

\begin{eqnarray*}
{\cal L}(f)  & = &  E_{(x,y)\sim \mathrm{Pop}}\;{\cal L}(f,x,y) \\
\hat{{\cal L}}(f) & = & E_{(x,y)\sim \mathrm{Train}}\;{\cal L}(f,x,y)
\end{eqnarray*}

where ${\cal L}(f)$ is the expectation of the loss over the entire population and $\hat{{\cal L}}(f)$ is the expectation over the training subset. The Occam guarantee tells us that there is an upper bound on the expected loss computed over the entire population. That upper bound is a function of the expected loss on the training subset. This has implications for generalization after training on only a subset of the population.

$${\cal L}(f) \leq \frac{10}{9}\left(\hat{{\cal L}}(f) + \frac{5L_{max}}{N_\mathrm{Train}}\left((\ln 2)|f| +\ln\frac{1}{\delta}\right)\right)$$

Notice the penalty term which is a function of the number of bits to encode our classifier (thanks Occam). Further, we can write this guarantee in a probabilistic form where we replace the number of bits used to represent the classifier in a particular language with a prior probability over the space of possible functions

\begin{eqnarray*}
{\cal L}(f) \leq \frac{10}{9}{\hat{\cal L}(f) + \frac{5 L_\mathrm{max}}{N_\mathrm{Train}}{- \ln \delta P(f)}}
\end{eqnarray*}

so we find the optimum $h$ by driving the training loss + penalty towards this lower bound

\begin{eqnarray*}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
f^* & = & {\argmin_f \hat{\cal L}(f) + \frac{5 L_\mathrm{max}}{N_\mathrm{Train}}{- \ln P(f)}}
\end{eqnarray*}

Importantly, the Occam guarantee doesn't make any assumptions about the prior $P(f)$ as we did in the Bayesian approach for regularization. That is, it doesn't assume that the sample was generated by drawing a function from the prior and then generating the sample from the function.

### PAC-Bayes guarantee

A major problem with the Occam guarantee is that it assumes we can write down the number of bits it takes to construct a model i.e. the space of models is discrete. For continuous models, we can no longer specify how many bits it takes to encode a model $f$.

For a posterior (informed) distribution over the space of models $q$ from which we can draw a model $f$

For the entire population,

\begin{eqnarray*}
  L(q) & =  &E_{f \sim q}{L(f)} \\
\end{eqnarray*}

and for the training set

\begin{eqnarray*}
  \hat{L}(q) & =  &E_{f \sim q}{\hat{L}(f)}
\end{eqnarray*}

and we ultimately want to modify the prior distribution to approximate the posterior with our training data. The PAC-Bayes guarantee tells us that

$$L(q) \leq \frac{1}{1-\frac{1}{2\lambda}}({\hat{L}(q) + \frac{\lambda L_{max}}{N_\mathrm{Train}}){KL(q,p) + \ln \frac{1}{\delta}}}$$

This is the bound that is compatible with deep learning because we are always learning continuous parameters.

### Example

Let us take the case where the prior is a Gaussian centered around the initialization point $\Phi_{init}$ and the posterior is also a Gaussian centered around some other point in the space of models.

### Implicit regularization

### Double descent


```code

```
