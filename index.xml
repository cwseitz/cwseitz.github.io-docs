<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <link>/</link>
    <description>Recent content on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
    <lastBuildDate>Tue, 17 Nov 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Analog impulse response</title>
      <link>/posts/analog-impulse-response/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/analog-impulse-response/</guid>
      <description>When designing electrical circuits that process input signals, we are always concerned with obtaining an output that suits the purpose of the circuit. In the time domain, we are usually interested in voltage at the output as a function of time. In the frequency domain, we are concerned with the frequency response or transfer function of the circuit.
There are cases where you can write down differential equations for the circuit in the time domain and solve those equations to find the output of your circuit.</description>
    </item>
    
    <item>
      <title>Attention is all you need</title>
      <link>/posts/attention-is-all-you-need/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/attention-is-all-you-need/</guid>
      <description>Sequence to sequence learning One method developed by Sutskever et al in 2015 is a sequence to sequence (seq2seq) method for machine translation. As will all seq2seq translation tasks, we want to estimate distribution over the output sequence given the input sequence
\begin{eqnarray} P_{\Phi}(w_{1},..,w_{T_{in}}|w_{1},..,w_{T_{in}}) \end{eqnarray}
We usually estimate this probability distribution by using autoregression which is realized with a RNN
\begin{eqnarray} P_{\Phi}(w_{1},..,w_{T_{in}}|w_{1},..,w_{T_{in}}) = \Pi_{t=0}^{T} P_{\Phi}(w_{t}|\overset\leftarrow{h_{in}}[1,J],w_{0}&amp;hellip;w_{t-1}) \end{eqnarray}
Notice there is an unfamiliar term $\overset\leftarrow{h_{in}}[1,J]$ we are conditioning on.</description>
    </item>
    
    <item>
      <title>Backpropagation</title>
      <link>/posts/deep-learning-backprop/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/deep-learning-backprop/</guid>
      <description>A deep learning framework is a high-level interface that allows the user to define a model $\Phi$. That model produces an output for a given input according to it&amp;rsquo;s mathematical definition. It is the job of the framework to to optimize the model by minimizing a particular loss function $\mathcal{L}$.
\begin{equation*} \DeclareMathOperator*{\argmin}{argmin} \Phi^{*} = \underset{\Phi}{\argmin} E_{(x,y)}\mathcal{L(x,y)} \end{equation*}
The are many deep learning frameworks in use today such as Tensorflow (Google), PyTorch (Facebook, Academics), Microsoft Cognitive Toolkit, Chainer, etc.</description>
    </item>
    
    <item>
      <title>Basic information theory</title>
      <link>/posts/information-theory-basics/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/information-theory-basics/</guid>
      <description>Information theory Information entropy is an information theoretic concept introduced by Claude Shannon in a paper titled A mathematical theory of communication published in 1948. At it&amp;rsquo;s core, information entropy tells us how much information is contained in the distribution of a variable. Bits are chosen as the unit of measure because information theory was originally devised to describe the novel communication systems of the mid 20th century: digital systems.</description>
    </item>
    
    <item>
      <title>Basic neuroelectronics</title>
      <link>/posts/neuroelectronics/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/neuroelectronics/</guid>
      <description>In the resting state, the cytosol of a neuron is negatively charged relative to the extracellular fluid with a potential of about -70$\mathrm{mV}$ which is maintained by ion pumps. There is a nonzero potential because 3-4 nm thick lipid bilayer blocks ions. Clearly the membrane is behaving like a capacitor.
At the same time, the membrane has a finite resistance (net conductance) as it contains highly specific ion channels which allow the passage of ions.</description>
    </item>
    
    <item>
      <title>Bubble sort in C</title>
      <link>/posts/c-vs-python-bubble-sort-performance/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/c-vs-python-bubble-sort-performance/</guid>
      <description>C vs Python Bubble Sort In this post I will implement the bubble sort algorithm in both C and Python languages. The purpose of this is to illustrate the performance differences between a compiled language like C and an interpreted language like Python. Importantly, running these two programs will require the C kernel for jupyter which can be installed by following the instructions here. Be sure to change to the appropriate kernel depending on which program you run.</description>
    </item>
    
    <item>
      <title>CCD and CMOS camera sensors</title>
      <link>/posts/characterizing-digital-ccdcmos-light-sensors/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/characterizing-digital-ccdcmos-light-sensors/</guid>
      <description>Introduction This post is dedicated to covering the theoretical characterization of digital light sensors and their impact on scientific experiments. Sensors operate by exposing a pixel array to an incoming light signal during the exposure time and accumulating charge units at each pixel proportional to the total photo irradiance during exposure. Charge units are then converted into a voltage, amplified, and ultimately converted to a digital signal by an analog to digital converter (ADC).</description>
    </item>
    
    <item>
      <title>Cheat sheet</title>
      <link>/posts/cheat-sheet/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/cheat-sheet/</guid>
      <description>RClone #Mounting a remote as a local directory !sudo mkdir /mnt/tmp !rclone mount remote:/path/to/files /mnt/tmp !fusermount -u /mnt/tmp #Syncing a remote with a local directory !rclone sync source:path dest:path [flags] #Example !rclone sync /path/to/files ucbox:/ --create-empty-src-dirs Docker #Listing docker images !docker images #List docker containers !docker ps -a #Removing all docker images !docker rmi $(docker images) #Removing all docker containers !docker rm $(docker ps -a) #Running a docker container !docker run -i -t NAME /bin/bash #Exporting a docker container to .</description>
    </item>
    
    <item>
      <title>Electrodynamics of isotropic media</title>
      <link>/posts/electrodynamics-of-isotropic-media/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/electrodynamics-of-isotropic-media/</guid>
      <description>Maxwell&amp;rsquo;s equations in vacuum \begin{equation*} \nabla \cdot \vec{E} = \frac{\rho}{\epsilon_{0}} \end{equation*}
\begin{equation*} \nabla \cdot \vec{B} = 0 \end{equation*}
\begin{equation*} \nabla \times \vec{E} = -\frac{\partial B}{\partial t} \end{equation*}
\begin{equation*} \nabla \times \vec{B} = \frac{1}{c^{2}}\frac{\partial E}{\partial t} + \mu_{0}J \end{equation*}</description>
    </item>
    
    <item>
      <title>EM fields in a dielectric</title>
      <link>/posts/electric-and-magnetic-fields-in-a-dielectric-medium/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/electric-and-magnetic-fields-in-a-dielectric-medium/</guid>
      <description>Electric permittivity We know that the electric field is a vector field resulting from the summation of electric fields generated by all charge. Here, I would like two introduce two additional fields: the polarization and displacement fields. The polarization field is another vector field that expresses the density of permanent or induced dipole moments in dielectric material. This field
\begin{equation*} P = \epsilon_{0}\chi_{E} E \end{equation*}
where $\chi$ is the electric suseptibility of the medium and is related to the permittivity by:</description>
    </item>
    
    <item>
      <title>Estimating diffusion coefficients</title>
      <link>/posts/estimating-diffusion-coefficients-in-the-presence-of-localization-error/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/estimating-diffusion-coefficients-in-the-presence-of-localization-error/</guid>
      <description>Introduction Single particle tracking has become a popular tool in the biophysical sciences as it has the potential to describe molecular processes while concurrently providing a visual representation of those processes. Various studies have been done to illustrate the precision limits of single particle tracking; however, it remains an open question in a number of applications. Here, I will try to outline the sources of error in single particle tracking and how those errors propagate to commonly extracted dynamic quantities such as the Mean Squared Displacement (MSD).</description>
    </item>
    
    <item>
      <title>Fourier image analysis</title>
      <link>/posts/fourier-analysis-of-digital-images/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/fourier-analysis-of-digital-images/</guid>
      <description>import matplotlib.pyplot as plt import numpy as np from skimage.data import camera from skimage.draw import circle from skimage.filters import gaussian from dig.fft import * raw = camera() Introduction In this post, we will show how you can analyze digital images using Fourier analysis. I&amp;rsquo;ve previously covered how to use fourier analysis with functions of time so we will be extending those ideas to higher dimensions. Anyway, the central idea of fourier analysis is that you can expression any function as a sum of infinitely many sinusoidal basis functions.</description>
    </item>
    
    <item>
      <title>Fundamentals of deep learning</title>
      <link>/posts/fundamentals-of-deep-learning/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/fundamentals-of-deep-learning/</guid>
      <description>What is a deep network? The human brain contains (~$10^{11}$) neurons that form an intricate network to form an interface between our bodies and the world around us. The job of neuroscientists is to determine the nature of that network and how it allows for interactions with the external world such as perception, prediction, and action. The ultimate goal of deep learning is to be able to harness what biological neural networks can do in artificial models.</description>
    </item>
    
    <item>
      <title>Gaussian beams</title>
      <link>/posts/gaussian-beams/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/gaussian-beams/</guid>
      <description>The Paraxial Approximation A paraxial beam is one in which the direction of propagation is limited to directions within a small angle to a cartesian axis. If we assume the beam is propagating along the z-direction and is polarized along the x-direction, we can write
\begin{equation*} E(r,t) = \psi(r)e^{-i(kz-\omega t)}\hat{x} \end{equation*}
where $\psi(r)$ is an unknown spatial function that defines the shape of the beam. By plugging in our hypothetical solution to the Helmholtz equation, we discover the paraxial Helmholtz equation</description>
    </item>
    
    <item>
      <title>Installing the C kernel in jupyter</title>
      <link>/posts/how-to-install-c-kernel-in-jupyter/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/how-to-install-c-kernel-in-jupyter/</guid>
      <description>First, we need to install the C kernel into the version of jupyter notebook that shipped with anaconda. To do that, we activate the environment, install jupyter-c-kernel via pip, change directory permissions so that it is installed within our virtual environment, then issue install_c_kernel.
conda activate XXX pip install jupyter-c-kernel sudo mkdir /usr/local/share/jupyter chmod -R 777 /usr/local/share/jupyter install_c_kernel </description>
    </item>
    
    <item>
      <title>Intro to C programming</title>
      <link>/posts/getting-started-with-c/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/getting-started-with-c/</guid>
      <description>Introduction The C programming language was first developed in the 1970s by Dennis Ritchie a Bell Labs. The language was inspired by its predecessor B and eventually would be standardized by the American National Standards Institute (ANSI) in the 1980s. The C programming language is a relatively low-level programming language which means that, as the programmer, you are required to do most of the leg work that is done automatically by higher level languages like Python.</description>
    </item>
    
    <item>
      <title>Intro to FFT</title>
      <link>/posts/introduction-to-fast-fourier-transforms-fft/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/introduction-to-fast-fourier-transforms-fft/</guid>
      <description>import sys import numpy as np import matplotlib import scipy from IPython.display import Image from IPython.core.display import HTML print(&#39;Python version:\n{}\n&#39;.format(sys.version)) print(&#39;Numpy version:\t\t{}&#39;.format(np.__version__)) print(&#39;matplotlib version:\t{}&#39;.format(matplotlib.__version__)) print(&#39;Scipy version:\t\t{}&#39;.format(scipy.__version__)) Python version: 3.7.3 (default, Mar 27 2019, 22:11:17) [GCC 7.3.0] Numpy version:	1.16.2 matplotlib version:	3.0.3 Scipy version:	1.2.1  Digital Signals Before we dive into the details of Fourier transformations, we need to review some fundamental concepts in signal processing. Recall that when we measure an analog signal e.</description>
    </item>
    
    <item>
      <title>Langevin dynamics</title>
      <link>/posts/brownian-motion-the-langevin-equation/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/brownian-motion-the-langevin-equation/</guid>
      <description>Introduction In this post I would like to model the interaction of a particle with its environment. In principle, if we could specify all the forces on a particle by its environment in a deterministic way, we could just add up all the components of the forces and write Newton&amp;rsquo;s law of motion and solve. In the real world, we have no way of predicted those forces and have to use a stochastic variant of Newton&amp;rsquo;s law of motion called the Langevin equation.</description>
    </item>
    
    <item>
      <title>Learning-theory</title>
      <link>/posts/learning-theory/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/learning-theory/</guid>
      <description>Free lunch A prominent question in deep learning is whether or not a universal learning algorithm exists. Many have argued for the no free lunch theorem which argues that there must be some innate system in biological brains that facilitates interpretation of natural language. In other words there must be some constraint on the space of functions of the input.
On the other hand, the free lunch theorem says that there need not be such a bias or constraint on the space of possible functions.</description>
    </item>
    
    <item>
      <title>Matplotlib colormaps</title>
      <link>/posts/matplotlib-colormaps/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/matplotlib-colormaps/</guid>
      <description>Choosing a colormap in Matplotlib   Sequential: change in lightness and often saturation of color incrementally, often using a single hue; should be used for representing information that has ordering.
  Diverging: change in lightness and possibly saturation of two different colors that meet in the middle at an unsaturated color; should be used when the information being plotted has a critical middle value, such as topography or when the data deviates around zero</description>
    </item>
    
    <item>
      <title>MSD in different scenarios</title>
      <link>/posts/mean-squared-displacement-msd-in-different-scenarios/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/mean-squared-displacement-msd-in-different-scenarios/</guid>
      <description>The Diffusion Equation In order to adequately describe a system of diffusing particles, we need to specify the density of particles at each point in space and how that density changes with time. In more mathematical terms, we must specify the surface $\rho(x,t)$ which $x$ is an arbitrary dimension of space. This surface can be found by solving the diffusion equation, a partial differential equation that relates the time derivative of the density of particles to the second spatial derivative of the density:</description>
    </item>
    
    <item>
      <title>Network programming in C</title>
      <link>/posts/network-programming-with-c-and-pcap/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/network-programming-with-c-and-pcap/</guid>
      <description>#include &amp;lt;pcap.h&amp;gt;#include &amp;lt;stdio.h&amp;gt;#include &amp;lt;stdlib.h&amp;gt;#include &amp;lt;errno.h&amp;gt;#include &amp;lt;sys/socket.h&amp;gt;#include &amp;lt;netinet/in.h&amp;gt;#include &amp;lt;arpa/inet.h&amp;gt;#include &amp;lt;netinet/if_ether.h&amp;gt;#include &amp;lt;net/ethernet.h&amp;gt;#include &amp;lt;netinet/ether.h&amp;gt; /* * workhorse function, we will be modifying this function */ void my_callback(u_char *args,const struct pcap_pkthdr* pkthdr,const u_char* packet) { } int main(int argc,char **argv) { char *dev; char errbuf[PCAP_ERRBUF_SIZE]; pcap_t* descr; struct bpf_program fp; /* hold compiled program */ bpf_u_int32 maskp; /* subnet mask */ bpf_u_int32 netp; /* ip */ u_char* args = NULL; /* Options must be passed in as a string because I am lazy */ if(argc &amp;lt; 2){ fprintf(stdout,&amp;#34;Usage: %s numpackets \&amp;#34;options\&amp;#34;\n&amp;#34;,argv[0]); return 0; } /* grab a device to peak into.</description>
    </item>
    
    <item>
      <title>Optical instrumentation</title>
      <link>/posts/optical-instrumentation/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/optical-instrumentation/</guid>
      <description>Optical Instrumentation This post is dedicated to the explanation of the core physical properties of commonly used optical devices such as the camera, microscope, and telescope. Each device is specially designed to achieve a particular purpose and we will show how that is achieved using ray optics.
The Camera Modern DSLR cameras come with a wide selection of lenses of varying focal lengths, each having a specific purpose. Typically, lenses with larger focal lengths are used for imaging objects far way and shorter focal lengths are used for close-ups.</description>
    </item>
    
    <item>
      <title>Photon statistics</title>
      <link>/posts/photon-statistics/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/photon-statistics/</guid>
      <description>import sys import numpy as np import matplotlib import scipy from IPython.display import Image from IPython.core.display import HTML print(&#39;Python version:\n{}\n&#39;.format(sys.version)) print(&#39;Numpy version:\t\t{}&#39;.format(np.__version__)) print(&#39;matplotlib version:\t{}&#39;.format(matplotlib.__version__)) print(&#39;Scipy version:\t\t{}&#39;.format(scipy.__version__)) Python version: 3.7.3 (default, Mar 27 2019, 22:11:17) [GCC 7.3.0] Numpy version:	1.16.2 matplotlib version:	3.0.3 Scipy version:	1.2.1  Quantum nature of light Perhaps the most interesting of the three noise sources mentioned above is shot noise as it is not really noise at all.</description>
    </item>
    
    <item>
      <title>Python/C API</title>
      <link>/posts/optimizing-python-execution-with-the-pythonc-api/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/optimizing-python-execution-with-the-pythonc-api/</guid>
      <description>The Python/C API This post will cover one method for improving the performance of native Python code. Python is an interpreted language and is therefore not compiled directly. Naturally, the solution is to make use of direclty compiled code rather than interpreted code to achieve performance gains. In the python world, that is commonly achieved by making use of the Python/C API.
The Python/C API provides an interface between the Python interpreter and the C standard libarary as well as custom modules.</description>
    </item>
    
    <item>
      <title>RC frequency response</title>
      <link>/posts/frequency-response-of-an-rc-circuit/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/frequency-response-of-an-rc-circuit/</guid>
      <description>Frequency response of an RC circuit The RC circuit can behave in different ways, depending on how we probe it. To achieve a low-pass configuration, we can probe the voltage across the capacitor. In that configuration, high frequencies are bled to ground through the cap. If we decide to swap the components like the figure the right, we have a high-pass filter, where low frequencies are bled to ground instead.</description>
    </item>
    
    <item>
      <title>Recurrent neural networks</title>
      <link>/posts/recurrent-neural-networks/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/recurrent-neural-networks/</guid>
      <description>Time as Depth Recurrent neural networks differ from typical feedforward networks (like CNNs) in that a single set of parameters $A$ is used repeatedly rather than multiple sets of parameters stacked in space as in a MLP. This particularly useful for applications like language modeling where each word in a sentence comes at a particular time.
Below it can be seen that the same transformation $A$ is used in sequence and that transformation can be itself an MLP or whatever architecture is necessary.</description>
    </item>
    
    <item>
      <title>Regularization</title>
      <link>/posts/regularization/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/regularization/</guid>
      <description>Error rate vs. Loss I need to make an important distinct before proceeding. The loss function, which we typically take to be the cross-entropy loss, is fundamentally distinct from the error-rate. The error-rate is the fraction of testing, validation, or testing samples that we correctly predict. This is an important metric for assessing the performance of a model and also things such as overfitting.
Regularization A major downfall of deep networks is that they often fail to generalize.</description>
    </item>
    
    <item>
      <title>Simulating a transformer in LT-spice</title>
      <link>/posts/simulating-a-transformer-in-ltspice/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/simulating-a-transformer-in-ltspice/</guid>
      <description>Simulating a transformer in LTSpice In LTSpice, there is no transformer component per say; however, a transformer is really just a pair of inductors. This means that we can simulate a transformer in spice by creating two or more inductors and specifying their mutual inductance via a spice directive. Before we do that, recall that the voltage ratio of the primary and secondary windings of a transformer is given by:</description>
    </item>
    
    <item>
      <title>Solving an LRC circuit</title>
      <link>/posts/solving-an-lrc-circuit/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/solving-an-lrc-circuit/</guid>
      <description>The LRC Circuit Kirchoff&amp;rsquo;s voltage rule written for an LRC circuit driven at angular frequency $\omega$:
\begin{align} L\frac{dI}{dt} + IR + \frac{1}{C}\int I dt = Ve^{i\omega t} \end{align}
which can be differentiated to give:
\begin{align} LI&amp;rsquo;&amp;rsquo; + RI&amp;rsquo; + \frac{1}{C}I = Ve^{i\omega t} \end{align}
which is second-order differential equation with constant coefficient driven by a complex exponential. The general solution for an equation of this form is:
\begin{align} I(t) = We^{i\omega t} \end{align}</description>
    </item>
    
    <item>
      <title>Stochastic gradient descent</title>
      <link>/posts/stochastic-gradient-descent/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/stochastic-gradient-descent/</guid>
      <description>Stochastic gradient descent (SGD) The purpose of stochastic gradient descent is to update the model parameters of a neural network so as to reduce a loss function. We do that by computing the gradient of that loss function, averaging over a batch, and tweaking model parameters proportional to that average gradient
$$\Delta\Phi = - \eta \hat{g}$$
where $\hat{g}$ is defined to be the average gradient of the loss over the batch.</description>
    </item>
    
  </channel>
</rss>